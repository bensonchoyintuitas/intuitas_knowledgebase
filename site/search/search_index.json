{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Enterprise Data Intelligence Blueprint This blueprint consists of a collection of resources that describe Intuitas' approach to designing and delivering Data and AI solutions. It is representative of large multi-domain enterprise deployments, however concepts should be adapted to each organisation as appropriate. Copyright This knowledge base and its contents are \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. Licence Permission is granted for free use, reproduction, and adaptation of this material, provided prior consent is obtained and appropriate attribution is given to the original author. Referenced third-party content is subject to the copyright terms of their respective owners. Table of Contents Level 0 - Enterprise-level context Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures Level 1 - Enterprise-level architecture Key concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Enterprise Security Enterprise Data Governance Audit Enterprise Billing Level 2 - Domain-level (solution) architecture Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data and information models Domain glossary Domain data and warehouse models Data Architecture Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Naming standards and conventions Naming standards and conventions","title":"Home"},{"location":"#enterprise-data-intelligence-blueprint","text":"This blueprint consists of a collection of resources that describe Intuitas' approach to designing and delivering Data and AI solutions. It is representative of large multi-domain enterprise deployments, however concepts should be adapted to each organisation as appropriate.","title":"Enterprise Data Intelligence Blueprint"},{"location":"#copyright","text":"This knowledge base and its contents are \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials.","title":"Copyright"},{"location":"#licence","text":"Permission is granted for free use, reproduction, and adaptation of this material, provided prior consent is obtained and appropriate attribution is given to the original author. Referenced third-party content is subject to the copyright terms of their respective owners.","title":"Licence"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#level-0-enterprise-level-context","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures","title":"Level 0 - Enterprise-level context"},{"location":"#level-1-enterprise-level-architecture","text":"Key concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Enterprise Security Enterprise Data Governance Audit Enterprise Billing","title":"Level 1 - Enterprise-level architecture"},{"location":"#level-2-domain-level-solution-architecture","text":"Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data and information models Domain glossary Domain data and warehouse models Data Architecture Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit","title":"Level 2 - Domain-level (solution) architecture"},{"location":"#naming-standards-and-conventions","text":"Naming standards and conventions","title":"Naming standards and conventions"},{"location":"about/","text":"Intuitas provides expertise in data architecture, governance, and real-time analytics , helping organisations build scalable and well-structured data systems. Our focus is on designing practical solutions that support efficient data management and decision-making. Areas of Expertise Enterprise Data Architecture & Governance Cloud & Hybrid Data Solutions Real-Time Data & Streaming Analytics AI-Driven Data Insights We take a pragmatic and structured approach to solving complex data challenges, ensuring systems are adaptable and aligned with business needs. For more information, contact us at office@intuitas.com.","title":"About"},{"location":"about/#areas-of-expertise","text":"Enterprise Data Architecture & Governance Cloud & Hybrid Data Solutions Real-Time Data & Streaming Analytics AI-Driven Data Insights We take a pragmatic and structured approach to solving complex data challenges, ensuring systems are adaptable and aligned with business needs. For more information, contact us at office@intuitas.com.","title":"Areas of Expertise"},{"location":"data_model/","text":"Sample Mermaid Diagram in Markdown Here is a class diagram: classDiagram class Patient { +String patientId +String firstName +String lastName +Date birthDate +String gender } class Encounter { +String encounterId +Date encounterDate +String encounterType +String patientId } class Condition { +String conditionId +String conditionName +Date onsetDate +String encounterId } class Procedure { +String procedureId +String procedureName +Date procedureDate +String encounterId } Patient \"1\" --> \"0..*\" Encounter : has Encounter \"1\" --> \"0..*\" Condition : includes Encounter \"1\" --> \"0..*\" Procedure : includes","title":"Sample Mermaid Diagram in Markdown"},{"location":"data_model/#sample-mermaid-diagram-in-markdown","text":"Here is a class diagram: classDiagram class Patient { +String patientId +String firstName +String lastName +Date birthDate +String gender } class Encounter { +String encounterId +Date encounterDate +String encounterType +String patientId } class Condition { +String conditionId +String conditionName +Date onsetDate +String encounterId } class Procedure { +String procedureId +String procedureName +Date procedureDate +String encounterId } Patient \"1\" --> \"0..*\" Encounter : has Encounter \"1\" --> \"0..*\" Condition : includes Encounter \"1\" --> \"0..*\" Procedure : includes","title":"Sample Mermaid Diagram in Markdown"},{"location":"level_0/","text":"Level 0 - Enterprise-level context Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made. Why It Matters Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals. Table of Contents Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures Org + Domain Definition Understanding the structure of the organisation and its domains is foundational to defining governance and metadata boundaries. Recommended: Description of the organisational boundary and any external organisations that are within the scope of concern. Description of domains and subdomains within the organisation. Strategies and Objectives This section outlines the strategic context in which data, technology, and governance initiatives operate. Recommended: Description of the organisation's strategies and objectives at the whole-of-enterprise level: Business strategies, objectives, plans, and initiatives Technology strategies, objectives, plans, and initiatives Data and AI strategies, objectives, plans, and initiatives Key Systems and Data Assets Identifying the systems and data assets in scope supports effective governance, architecture, and planning. Recommended: Description of the organisation's key systems within the scope of concern (e.g., CMDB). Information Asset Register (IAR) Team Capabilities and Roles Clarity on the responsibilities and capabilities of key teams is essential for executing data strategy and maintaining accountability. Recommended: Description of in-scope key teams/parties associated with: Strategy and portfolio / prioritisation of data initiatives Data creation within the business domains Data management Data governance (quality, access) Data consumption Data engineering and integration Data analysis and reporting Data application development Data ops support Data security Data and information architecture Infrastructure and platform provisioning and management RACI matrix for key teams/parties Current and target operating and service management model Maturity and skill assessment of key teams/parties Governance Structures Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. Recommended: Description of the organisation's governance frameworks, policies, standards, processes, and bodies that are relevant to the scope of concern. Billing Structures Understanding how billing is structured enables transparency, cost accountability, and alignment with financial governance. Recommended: Description of the organisation's structures in terms of billing, and how cost centres may relate to billing reports, delegations, and associated observability requirements.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#level-0-enterprise-level-context","text":"Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#why-it-matters","text":"Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals.","title":"Why It Matters"},{"location":"level_0/#table-of-contents","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures","title":"Table of Contents"},{"location":"level_0/#org-domain-definition","text":"Understanding the structure of the organisation and its domains is foundational to defining governance and metadata boundaries. Recommended: Description of the organisational boundary and any external organisations that are within the scope of concern. Description of domains and subdomains within the organisation.","title":"Org + Domain Definition"},{"location":"level_0/#strategies-and-objectives","text":"This section outlines the strategic context in which data, technology, and governance initiatives operate. Recommended: Description of the organisation's strategies and objectives at the whole-of-enterprise level: Business strategies, objectives, plans, and initiatives Technology strategies, objectives, plans, and initiatives Data and AI strategies, objectives, plans, and initiatives","title":"Strategies and Objectives"},{"location":"level_0/#key-systems-and-data-assets","text":"Identifying the systems and data assets in scope supports effective governance, architecture, and planning. Recommended: Description of the organisation's key systems within the scope of concern (e.g., CMDB). Information Asset Register (IAR)","title":"Key Systems and Data Assets"},{"location":"level_0/#team-capabilities-and-roles","text":"Clarity on the responsibilities and capabilities of key teams is essential for executing data strategy and maintaining accountability. Recommended: Description of in-scope key teams/parties associated with: Strategy and portfolio / prioritisation of data initiatives Data creation within the business domains Data management Data governance (quality, access) Data consumption Data engineering and integration Data analysis and reporting Data application development Data ops support Data security Data and information architecture Infrastructure and platform provisioning and management RACI matrix for key teams/parties Current and target operating and service management model Maturity and skill assessment of key teams/parties","title":"Team Capabilities and Roles"},{"location":"level_0/#governance-structures","text":"Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. Recommended: Description of the organisation's governance frameworks, policies, standards, processes, and bodies that are relevant to the scope of concern.","title":"Governance Structures"},{"location":"level_0/#billing-structures","text":"Understanding how billing is structured enables transparency, cost accountability, and alignment with financial governance. Recommended: Description of the organisation's structures in terms of billing, and how cost centres may relate to billing reports, delegations, and associated observability requirements.","title":"Billing Structures"},{"location":"level_1/","text":"Level 1 - Enterprise-level architecture Return to home This section describes enterprise-wide and cross-domain data and data platform architecture concepts. Table of Contents Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Lakehouse Metadata - dbt Docs Lakehouse Metadata - Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Key concepts The following key concepts are used throughout this knowledgebase. Domain Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. see Domain driven design Subdomain A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Domain-Centric Design Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Data Mesh A data mesh is a decentralized approach to data management that empowers domain teams to own their data and build data products. The then shares data as products with other domains. It emphasizes autonomy, flexibility, and interoperability. This approach is not necessarily appropriate for all organisations and organisations will embody its principles with varying degrees of maturity and success. see Data Mesh: Principles Domain Topology A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. see Data Mesh: Topologies and domain granularity Data Fabric A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles. Data Mesh vs Fabric A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric. Reference topologies Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Source: Data Mesh: Topologies and domain granularity Hybrid federated mesh topology This blueprint depicts a Hybrid Federated Mesh Topology, common in large enterprises. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organization. Distributed Local engineering Maintaining a local bronze layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access gold data from other domains and, where appropriate, silver or bronze, facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability. Enterprise Data Platform Reference Architecture Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example: Enterprise (Logical) Data Warehouse Reference Architecture Logical Data Warehouse topology Reflects the domain topology. Provides unified access to data warehouse products from across domains via a common catalog. Example: Enterprise Information and Data Architecture Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models Enterprise Metadata Architecture Metadata is an umbrella term for many types of data which play a critical roles across various domains, supporting the description, governance, and operational use of data and technology assets. Metadata can be both actively curated, as well as a byproduct. It is essential for enabling: Data and Information Governance and Architecture - specifically semantic lineage, data lineage and privacy/access/security controls Data Engineering and Analytics devlopment Business interpretation and understanding of information and analytics Data quality and integrity Technical and Platform Administration Integration, data sharing and interoperability Recommendations To support enterprise-wide consistency and governance, it is recommended to define and maintain: Applicable metadata standards An enterprise metadata architecture and metamodel Metadata governance framework, including roles (e.g., stewards) and operational processes Intuitas Metadata Architecture Principles The following principles describe Intuitas' design philosophy to ensure sustainable and effective metadata capability Accessible : Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic : Automate collection and updates to keep metadata fresh and reduce manual work. Contextual : Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated : Metadata exists in an ecosystem across tools to support diverse workflows. Consistency : Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure : Protect metadata as it may contain sensitive details. Accountability : Clearly define roles for ownership and stewardship. Agnostic : Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability. Enterprise Metadata Logical Architecture and Metamodel In the context of a Modern Data Intelligence Capability semantic lineage, data lineage The diagram below shows illustrates the concepts of semantic and data lineage/ Enterprise Metadata Logical Architecture and Metamodel The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. The diagram below shows how key metadata flows across the ecosystem. It highlights where synchronisation matters most\u2014and how a unified metastore complements existing in-context metadata serving. Data Architecture and Governance Metadata Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability. Datahub DataHub serves as a unifying layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, providing a comprehensive view of data flows and relationships throughout the ecosystem. Enterprise-wide summary of assets: Browse by business domain and filters: Metadata search by term: User-driven mapping of glossary terms to measures: User-driven tagging of PII: dbt Docs dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to naming standards and conventions . Databricks Unity Catalog Metastore Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout. Enterprise Security Recommended: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs Enterprise Data Governance Recommended: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management Audit Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions. Example questions and associated queries As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Enterprise Billing Out of the box Databricks usage dashboard to be tested at workspace level Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform (i.e. Databricks) must to align. Recommendations here align with the following Domain topology: Databricks features for usage tracking Metadata and tags In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags Cluster policies Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions. System tables System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools Usage reports Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request) Domain / Workspace Administrator Role Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts Tagging convention All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id **As an Enterprise Admin** 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation **As a Domain (workspace) Admin** 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries References to recommended practices Top 10 Queries to use with System Tables Unlocking Cost Optimization Insights with Databricks System Tables","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#level-1-enterprise-level-architecture","text":"Return to home This section describes enterprise-wide and cross-domain data and data platform architecture concepts.","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#table-of-contents","text":"Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Lakehouse Metadata - dbt Docs Lakehouse Metadata - Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing","title":"Table of Contents"},{"location":"level_1/#key-concepts","text":"The following key concepts are used throughout this knowledgebase.","title":"Key concepts"},{"location":"level_1/#domain","text":"Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. see Domain driven design","title":"Domain"},{"location":"level_1/#subdomain","text":"A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area.","title":"Subdomain"},{"location":"level_1/#domain-centric-design","text":"Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy.","title":"Domain-Centric Design"},{"location":"level_1/#data-mesh","text":"A data mesh is a decentralized approach to data management that empowers domain teams to own their data and build data products. The then shares data as products with other domains. It emphasizes autonomy, flexibility, and interoperability. This approach is not necessarily appropriate for all organisations and organisations will embody its principles with varying degrees of maturity and success. see Data Mesh: Principles","title":"Data Mesh"},{"location":"level_1/#domain-topology","text":"A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. see Data Mesh: Topologies and domain granularity","title":"Domain Topology"},{"location":"level_1/#data-fabric","text":"A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles.","title":"Data Fabric"},{"location":"level_1/#data-mesh-vs-fabric","text":"A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric.","title":"Data Mesh vs Fabric"},{"location":"level_1/#reference-topologies","text":"Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Source: Data Mesh: Topologies and domain granularity","title":"Reference topologies"},{"location":"level_1/#hybrid-federated-mesh-topology","text":"This blueprint depicts a Hybrid Federated Mesh Topology, common in large enterprises. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organization. Distributed Local engineering Maintaining a local bronze layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access gold data from other domains and, where appropriate, silver or bronze, facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability.","title":"Hybrid federated mesh topology"},{"location":"level_1/#enterprise-data-platform-reference-architecture","text":"Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example:","title":"Enterprise Data Platform Reference Architecture"},{"location":"level_1/#enterprise-logical-data-warehouse-reference-architecture","text":"Logical Data Warehouse topology Reflects the domain topology. Provides unified access to data warehouse products from across domains via a common catalog. Example:","title":"Enterprise (Logical) Data Warehouse Reference Architecture"},{"location":"level_1/#enterprise-information-and-data-architecture","text":"Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models","title":"Enterprise Information and Data Architecture"},{"location":"level_1/#enterprise-metadata-architecture","text":"Metadata is an umbrella term for many types of data which play a critical roles across various domains, supporting the description, governance, and operational use of data and technology assets. Metadata can be both actively curated, as well as a byproduct. It is essential for enabling: Data and Information Governance and Architecture - specifically semantic lineage, data lineage and privacy/access/security controls Data Engineering and Analytics devlopment Business interpretation and understanding of information and analytics Data quality and integrity Technical and Platform Administration Integration, data sharing and interoperability Recommendations To support enterprise-wide consistency and governance, it is recommended to define and maintain: Applicable metadata standards An enterprise metadata architecture and metamodel Metadata governance framework, including roles (e.g., stewards) and operational processes","title":"Enterprise Metadata Architecture"},{"location":"level_1/#intuitas-metadata-architecture-principles","text":"The following principles describe Intuitas' design philosophy to ensure sustainable and effective metadata capability Accessible : Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic : Automate collection and updates to keep metadata fresh and reduce manual work. Contextual : Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated : Metadata exists in an ecosystem across tools to support diverse workflows. Consistency : Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure : Protect metadata as it may contain sensitive details. Accountability : Clearly define roles for ownership and stewardship. Agnostic : Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability.","title":"Intuitas Metadata Architecture Principles"},{"location":"level_1/#enterprise-metadata-logical-architecture-and-metamodel","text":"In the context of a Modern Data Intelligence Capability semantic lineage, data lineage The diagram below shows illustrates the concepts of semantic and data lineage/","title":"Enterprise Metadata Logical Architecture and Metamodel"},{"location":"level_1/#enterprise-metadata-logical-architecture-and-metamodel_1","text":"The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. The diagram below shows how key metadata flows across the ecosystem. It highlights where synchronisation matters most\u2014and how a unified metastore complements existing in-context metadata serving.","title":"Enterprise Metadata Logical Architecture and Metamodel"},{"location":"level_1/#data-architecture-and-governance-metadata","text":"Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability.","title":"Data Architecture and Governance Metadata"},{"location":"level_1/#datahub","text":"DataHub serves as a unifying layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, providing a comprehensive view of data flows and relationships throughout the ecosystem. Enterprise-wide summary of assets: Browse by business domain and filters: Metadata search by term: User-driven mapping of glossary terms to measures: User-driven tagging of PII:","title":"Datahub"},{"location":"level_1/#dbt-docs","text":"dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to naming standards and conventions .","title":"dbt Docs"},{"location":"level_1/#databricks-unity-catalog-metastore","text":"Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout.","title":"Databricks Unity Catalog Metastore"},{"location":"level_1/#enterprise-security","text":"Recommended: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs","title":"Enterprise Security"},{"location":"level_1/#enterprise-data-governance","text":"Recommended: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management","title":"Enterprise Data Governance"},{"location":"level_1/#audit","text":"Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions.","title":"Audit"},{"location":"level_1/#example-questions-and-associated-queries","text":"As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_1/#enterprise-billing","text":"Out of the box Databricks usage dashboard to be tested at workspace level Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform (i.e. Databricks) must to align. Recommendations here align with the following Domain topology:","title":"Enterprise Billing"},{"location":"level_1/#databricks-features-for-usage-tracking","text":"","title":"Databricks features for usage tracking"},{"location":"level_1/#metadata-and-tags","text":"In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags","title":"Metadata and tags"},{"location":"level_1/#cluster-policies","text":"Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions.","title":"Cluster policies"},{"location":"level_1/#system-tables","text":"System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools","title":"System tables"},{"location":"level_1/#usage-reports","text":"Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request)","title":"Usage reports"},{"location":"level_1/#domain-workspace-administrator-role","text":"Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts","title":"Domain / Workspace Administrator Role"},{"location":"level_1/#tagging-convention","text":"All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id **As an Enterprise Admin** 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation **As a Domain (workspace) Admin** 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Tagging convention"},{"location":"level_1/#references-to-recommended-practices","text":"Top 10 Queries to use with System Tables Unlocking Cost Optimization Insights with Databricks System Tables","title":"References to recommended practices"},{"location":"level_2/","text":"Level 2 - Domain-Level (Solution) Architecture and Patterns Return to home This section describes Domain-level instantiations of the enterprise-level reference architecture. i.e. solutions (See Enterprise Data Platform Reference Architecture ) Table of Contents Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Billing Example reference architecture: Business architecture Business processes Business processes are the activities and tasks that are performed to achieve the goals of the business. Understanding them is necessary to understand: - the context in which data is captured and used - concepts and entities that are relevant to the domain - the relationships between different processes and data Business glossary A business glossary is a list of terms and definitions that are relevant to the business. see Domain Glossary. Business metrics Metrics are the measurements of the performance of the business processes. They should be documented according to a defined template that captures, at a minimum, the following: - name - definition - formula (with reference to data elements and definitions in the business glossary) - dimensions - source(s) - metric owner - frequency Infrastructure This section is a work in progress Environments, Workspaces and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage. Secrets This section is a work in progress - Management - Areas of use - Handling practices Storage Lakehouse storage Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod CICD and Repository This section is a work in progress - Description of git workflows for CICD in terms of: - Infrastructure - Data engineering - Analytics engineering - Data science / AIML - BI, Reports and other products Tools Github Azure Devops Databricks Asset Bundles Repositories Infrastructure dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles Observability Various tools can be used to provide insight into different aspects of the architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring dbt observability - Elementary Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention. Networking By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies are enabled. Orchestration Tools Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt) Security Tools Azure Entra Azure Key Vault Unity Catalog System access reports Data Architecture Data Architecture refers to how data is physically structured, stored, and accessed within an organization. It encompasses the design and management of data storage systems, data models, data integration processes, and data governance practices. Data and information models Domain-level data and information models are typically closer aligned to real-world business semantics and business rules, which may not necessarily align with the broader enterprise or other domains. See Bounded context Domain glossary Expand on the enterprise glossary and add domain specific terms and definitions. In cases where domain definitions are synonymous with enterprise definitions, the enterprise glossary should be referenced. In cases where definitions are conflicting, governance should be applied to resolve the conflict. Domain data and warehouse models Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities. Data layers and stages Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Top level layers follow the Medallion architecture . Within each layer, data is transformed through a series of stages. Bronze: Data according to source. Silver: Data according to business. Gold: Data according to requirements. Metadata Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Bronze The Bronze layer stores raw, immutable data as it is ingested from source systems. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing Silver The Silver layer is source centric and focuses on transforming raw data into cleaned, enriched, and validated datasets. Base Models Representation of source data with no changes. Used as a foundation for staging models as well as data quality checks. Staging Models Source-system and object centric transformations that are core to all downstream consumption. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Enrichment Models Still source-centric, however: - more complex business logic and transformations are applied e.g. common calculations and derivations. - may combine multiple staging objects from the same source By separating enrichment from core staging, we can schedule these processes independently. This allows for flexibility in updating or refreshing only the parts of the data pipeline that need it, reducing unnecessary computation and improving efficiency. It also allows for change and versioning of those business rules with minimal impact on core staging objects. Source Reference Data For convenience, reference data specific to the source can be segregated here and aligned to standards and downstream needs. Raw Vault Data vault 2.0 aligned raw data warehouse. Gold The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. Business Vault Data vault 2.0 aligned business data warehouse where business rules and transformations are applied. Intermediate Models These act as building blocks for marts, transforming and aggregating data further. Then be thought of as mart staging https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business-specific transformations such as: Pivoting Aggregation Joining Funnel creation Conformance Desensitization Enterprise Reference Data Reference data, independent of source can be aggregated here for broad consumption. Marts - Facts and dimensions Kimball style marts that represent business entities and processes. They may serve foundational or narrow requirements. be scoped to specific systems or conformed across the enterprise Marts - Denormalised Single table / view objects that combine data from multiple objects (e.g. facts and dimensions) Lakehouse Catalog to Storage Mapping Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Example: Data Engineering Ingestion Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: - Scalable, Resilient, Maintainable, Governed - Pattern-based, automated and Metadata-driven where possible - Batch and stream-based Example batch ingestion options: Ingestion patterns and notes: Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks) Transformation This section is a work in progress Batch and Micro-batch SQL transformation dbt see dbt standards Streaming SQL transformation This section is a work in progress Non SQL transformation This section is a work in progress Data sharing and delivery patterns Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below. Row Level Security see Row Level Security Pull / direct access Databricks Delta sharing practices Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing ADLSGen2 Access to Data Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing DuckDB Access to Data (via Unity Catalog) Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported SQL Access SQL Access is provided by the Databricks SQL (serverless) endpoint. API Access This section is a work in progress - The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html Snowflake Access This section is a work in progress Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial Microsoft Fabric Access The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to get data from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption Push This section is a work in progress For consideration: adf databricks lakeflow Visualisation This section is a work in progress For consideration: Powerbi Databricks dashboards Apps Open source visual options AI/ML This section is a work in progress For consideration: MLOps Training Databricks Azure ML Data governance This section describes how Enterprise-level governance will be implemented through solutions at the domain level. Data lifecycle and asset management This section is a work in progress For consideration: data contracts and policy data asset tagging Data access management This section is a work in progress For consideration: data access request management data contracts access audit activity audit Data quality This section is a work in progress For consideration: data quality checking and reporting data standards and quality business rule management Data understandability This section is a work in progress For consideration: data lineage data object metadata Privacy Preservation This section is a work in progress For consideration: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names. Row Level Security This section is a work in progress For consideration: dynamic views precomputed views Audit This section is a work in progress For consideration: audit log queries Example questions and associated queries As a Domain (workspace) Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Billing For more context and concepts, refer to the Enterprise Billing section in Level 1.","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#level-2-domain-level-solution-architecture-and-patterns","text":"Return to home This section describes Domain-level instantiations of the enterprise-level reference architecture. i.e. solutions (See Enterprise Data Platform Reference Architecture )","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#table-of-contents","text":"Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Billing Example reference architecture:","title":"Table of Contents"},{"location":"level_2/#business-architecture","text":"","title":"Business architecture"},{"location":"level_2/#business-processes","text":"Business processes are the activities and tasks that are performed to achieve the goals of the business. Understanding them is necessary to understand: - the context in which data is captured and used - concepts and entities that are relevant to the domain - the relationships between different processes and data","title":"Business processes"},{"location":"level_2/#business-glossary","text":"A business glossary is a list of terms and definitions that are relevant to the business. see Domain Glossary.","title":"Business glossary"},{"location":"level_2/#business-metrics","text":"Metrics are the measurements of the performance of the business processes. They should be documented according to a defined template that captures, at a minimum, the following: - name - definition - formula (with reference to data elements and definitions in the business glossary) - dimensions - source(s) - metric owner - frequency","title":"Business metrics"},{"location":"level_2/#infrastructure","text":"This section is a work in progress","title":"Infrastructure"},{"location":"level_2/#environments-workspaces-and-storage","text":"This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage.","title":"Environments, Workspaces and Storage"},{"location":"level_2/#secrets","text":"This section is a work in progress - Management - Areas of use - Handling practices","title":"Secrets"},{"location":"level_2/#storage","text":"","title":"Storage"},{"location":"level_2/#lakehouse-storage","text":"Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Lakehouse storage"},{"location":"level_2/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"level_2/#cicd-and-repository","text":"This section is a work in progress - Description of git workflows for CICD in terms of: - Infrastructure - Data engineering - Analytics engineering - Data science / AIML - BI, Reports and other products","title":"CICD and Repository"},{"location":"level_2/#tools","text":"Github Azure Devops Databricks Asset Bundles","title":"Tools"},{"location":"level_2/#repositories","text":"Infrastructure dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles","title":"Repositories"},{"location":"level_2/#observability","text":"Various tools can be used to provide insight into different aspects of the architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring","title":"Observability"},{"location":"level_2/#dbt-observability-elementary","text":"Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention.","title":"dbt observability - Elementary"},{"location":"level_2/#networking","text":"By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies are enabled.","title":"Networking"},{"location":"level_2/#orchestration","text":"","title":"Orchestration"},{"location":"level_2/#tools_1","text":"Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt)","title":"Tools"},{"location":"level_2/#security","text":"","title":"Security"},{"location":"level_2/#tools_2","text":"Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Tools"},{"location":"level_2/#data-architecture","text":"Data Architecture refers to how data is physically structured, stored, and accessed within an organization. It encompasses the design and management of data storage systems, data models, data integration processes, and data governance practices.","title":"Data Architecture"},{"location":"level_2/#data-and-information-models","text":"Domain-level data and information models are typically closer aligned to real-world business semantics and business rules, which may not necessarily align with the broader enterprise or other domains. See Bounded context","title":"Data and information models"},{"location":"level_2/#domain-glossary","text":"Expand on the enterprise glossary and add domain specific terms and definitions. In cases where domain definitions are synonymous with enterprise definitions, the enterprise glossary should be referenced. In cases where definitions are conflicting, governance should be applied to resolve the conflict.","title":"Domain glossary"},{"location":"level_2/#domain-data-and-warehouse-models","text":"Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities.","title":"Domain data and warehouse models"},{"location":"level_2/#data-layers-and-stages","text":"Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Top level layers follow the Medallion architecture . Within each layer, data is transformed through a series of stages. Bronze: Data according to source. Silver: Data according to business. Gold: Data according to requirements.","title":"Data layers and stages"},{"location":"level_2/#metadata","text":"Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets","title":"Metadata"},{"location":"level_2/#bronze","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing","title":"Bronze"},{"location":"level_2/#silver","text":"The Silver layer is source centric and focuses on transforming raw data into cleaned, enriched, and validated datasets. Base Models Representation of source data with no changes. Used as a foundation for staging models as well as data quality checks. Staging Models Source-system and object centric transformations that are core to all downstream consumption. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Enrichment Models Still source-centric, however: - more complex business logic and transformations are applied e.g. common calculations and derivations. - may combine multiple staging objects from the same source By separating enrichment from core staging, we can schedule these processes independently. This allows for flexibility in updating or refreshing only the parts of the data pipeline that need it, reducing unnecessary computation and improving efficiency. It also allows for change and versioning of those business rules with minimal impact on core staging objects. Source Reference Data For convenience, reference data specific to the source can be segregated here and aligned to standards and downstream needs. Raw Vault Data vault 2.0 aligned raw data warehouse.","title":"Silver"},{"location":"level_2/#gold","text":"The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. Business Vault Data vault 2.0 aligned business data warehouse where business rules and transformations are applied. Intermediate Models These act as building blocks for marts, transforming and aggregating data further. Then be thought of as mart staging https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business-specific transformations such as: Pivoting Aggregation Joining Funnel creation Conformance Desensitization Enterprise Reference Data Reference data, independent of source can be aggregated here for broad consumption. Marts - Facts and dimensions Kimball style marts that represent business entities and processes. They may serve foundational or narrow requirements. be scoped to specific systems or conformed across the enterprise Marts - Denormalised Single table / view objects that combine data from multiple objects (e.g. facts and dimensions)","title":"Gold"},{"location":"level_2/#lakehouse-catalog-to-storage-mapping","text":"Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Example:","title":"Lakehouse Catalog to Storage Mapping"},{"location":"level_2/#data-engineering","text":"","title":"Data Engineering"},{"location":"level_2/#ingestion","text":"Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: - Scalable, Resilient, Maintainable, Governed - Pattern-based, automated and Metadata-driven where possible - Batch and stream-based Example batch ingestion options:","title":"Ingestion"},{"location":"level_2/#ingestion-patterns-and-notes","text":"Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks)","title":"Ingestion patterns and notes:"},{"location":"level_2/#transformation","text":"This section is a work in progress","title":"Transformation"},{"location":"level_2/#batch-and-micro-batch-sql-transformation","text":"dbt see dbt standards","title":"Batch and Micro-batch SQL transformation"},{"location":"level_2/#streaming-sql-transformation","text":"This section is a work in progress","title":"Streaming SQL transformation"},{"location":"level_2/#non-sql-transformation","text":"This section is a work in progress","title":"Non SQL transformation"},{"location":"level_2/#data-sharing-and-delivery-patterns","text":"Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below.","title":"Data sharing and delivery patterns"},{"location":"level_2/#row-level-security","text":"see Row Level Security","title":"Row Level Security"},{"location":"level_2/#pull-direct-access","text":"","title":"Pull / direct access"},{"location":"level_2/#databricks-delta-sharing-practices","text":"Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing","title":"Databricks Delta sharing practices"},{"location":"level_2/#adlsgen2-access-to-data","text":"Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing","title":"ADLSGen2 Access to Data"},{"location":"level_2/#duckdb-access-to-data-via-unity-catalog","text":"Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported","title":"DuckDB Access to Data (via Unity Catalog)"},{"location":"level_2/#sql-access","text":"SQL Access is provided by the Databricks SQL (serverless) endpoint.","title":"SQL Access"},{"location":"level_2/#api-access","text":"This section is a work in progress - The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html","title":"API Access"},{"location":"level_2/#snowflake-access","text":"This section is a work in progress Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial","title":"Snowflake Access"},{"location":"level_2/#microsoft-fabric-access","text":"The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to get data from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption","title":"Microsoft Fabric Access"},{"location":"level_2/#push","text":"This section is a work in progress For consideration: adf databricks lakeflow","title":"Push"},{"location":"level_2/#visualisation","text":"This section is a work in progress For consideration: Powerbi Databricks dashboards Apps Open source visual options","title":"Visualisation"},{"location":"level_2/#aiml","text":"This section is a work in progress For consideration: MLOps Training Databricks Azure ML","title":"AI/ML"},{"location":"level_2/#data-governance","text":"This section describes how Enterprise-level governance will be implemented through solutions at the domain level.","title":"Data governance"},{"location":"level_2/#data-lifecycle-and-asset-management","text":"This section is a work in progress For consideration: data contracts and policy data asset tagging","title":"Data lifecycle and asset management"},{"location":"level_2/#data-access-management","text":"This section is a work in progress For consideration: data access request management data contracts access audit activity audit","title":"Data access management"},{"location":"level_2/#data-quality","text":"This section is a work in progress For consideration: data quality checking and reporting data standards and quality business rule management","title":"Data quality"},{"location":"level_2/#data-understandability","text":"This section is a work in progress For consideration: data lineage data object metadata","title":"Data understandability"},{"location":"level_2/#privacy-preservation","text":"This section is a work in progress For consideration: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names.","title":"Privacy Preservation"},{"location":"level_2/#row-level-security_1","text":"This section is a work in progress For consideration: dynamic views precomputed views","title":"Row Level Security"},{"location":"level_2/#audit","text":"This section is a work in progress For consideration: audit log queries","title":"Audit"},{"location":"level_2/#example-questions-and-associated-queries","text":"As a Domain (workspace) Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_2/#billing","text":"For more context and concepts, refer to the Enterprise Billing section in Level 1.","title":"Billing"},{"location":"naming_standards_and_conventions/","text":"Naming Conventions {Return to home}(README.md) Table of Contents Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (ADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Group Names Policies Frameworks Mesh Domain Names All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g. intuitas_domain3 Platform Environment Environment: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred) VNET Name: vn-{organisation_name}-{domain_name} e.g. vn-intuitas-domain3 Resource Groups Name: rg-{organisation_name}-{domain_name} e.g. rg-intuitas-domain3 Databricks workspace Name: ws-{organisation_name}-{domain_name} e.g. ws-intuitas-domain3 Key vault Name: kv-{organisation_name}-{domain_name} e.g. kv-intuitas-domain3 Secrets Name: {secret_name} Entra Group Names Name: eg-{organisation_name}-{domain_name} e.g. eg-intuitas-domain3 Azure Data Factory (ADF) Name: adf-{organisation_name}-{domain_name} e.g. adf-intuitas-domain3 SQL Server Name: sql-{organisation_name}-{domain_name} e.g. sql-intuitas-domain3 SQL Database Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g. sqldb-metadata-intuitas-domain3 Storage Lakehouse storage Lakehouse storage account name: dl{organisation_name}{domain_name} Lakehouse storage containers Name: {environment} (dev/test/preprod/prod) Lakehouse storage folders Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob files and folders No standard naming conventions for files and folders. Databricks Workspace and cluster names Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name} Catalog Catalog name: {domain_name}{environment} (prod is implied optional) e.g. intuitas_domain3_dev Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog Externally mounted (lakehouse federation) Catalog Names All lower case: {Domain (owner)}_ext__{source_system}{optional:__other_useful_descriptors} e.g. intuitas_domain3_ext__sqlonpremsource Metadata tags: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Schema and object conventions Schema level external storage locations Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g. abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations. Metadata Schemas and Objects Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__{optional: function} Naming convention: {function/descriptor} e.g. intuitas_domain3_dev.meta__ingestion.ingestion_control Bronze (Raw data according to systems) The Bronze layer stores raw, immutable data as it is ingested from source systems. All schemas are may be optionally prefixed with bronze__ Persistent Landing : N/A (see file naming) Operational Data Store (ODS) : Schema naming convention: ods{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: ods__{source_system}__{source_channel}__{object} e.g. intuitas_domain3_dev.ods.ods__finance_system__adf__accounts Persistent Data Store (PDS) : Schema naming convention: pds{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: pds__{source_system}__{source_channel}__{object} e.g. intuitas_domain3_dev.pds.pds__finance_system__adf__accounts Silver (Data according to business entities) The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. All schemas are may be optionally prefixed with silver All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver) Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts only. Source-specific: Schema naming convention: stg__{source_system}__{source_channel} Object naming convention: {entity}__{object_description}__{n}__{transformation}__{source_system}__{source_channel} e.g. intuitas_domain3_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf e.g. intuitas_domain3_dev.stg__new_finance_system__adf.accounts__02_cleaned__new_finance_system__adf e.g. intuitas_domain3_dev.stg__old_finance_system__adf.accounts__01_renamed_and_typed__old_finance_system__adf Non-source specific Schema naming convention: stg{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align with target mart: stg__(optional:dim/fact)_{entity}__{object_description}__{n}__{transformation} e.g. intuitas_domain3_dev.stg.accounts__01_deduped e.g. intuitas_domain3_dev.stg.accounts__02_business_validated Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g. intuitas_domain3_dev.stg__finance_system__adf.stg__finance_system__adf__account__01_renamed_and_typed (Silver) Marts : Final products after staging Source-specific: Schema naming convention: mart__{source_system}__{source_channel} Object naming convention: (optional:dim/fact)_{entity / _object_description}__{source_system}__{source_channel} e.g. intuitas_domain3_dev.mart__new_finance_system__adf.payment__new_finance_system__adf e.g. intuitas_domain3_dev.mart__new_finance_system__adf.account__new_finance_system__adf e.g. intuitas_domain3_dev.mart__old_finance_system__adf.account__old_finance_system__adf Non-source specific Schema naming convention: mart{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: (optional:dim/fact)__{unified entity / _object_description} e.g. intuitas_domain3_dev.mart.account (unified) e.g. intuitas_domain3_dev.mart__corporate__finance.account (unified) e.g. intuitas_domain3_dev.mart__finance.account (unified) e.g. intuitas_domain3_dev.mart.account_join_with_payments (joined across two systems) Reference Data : Reference data objects that are aligned to business entities and broad requirements. These may also be staged in stg as per silver marts. These are typically not source-aligned but optionality for capturing sources exists. Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference data set name} (optional:__{source_system}__{source_channel}) e.g. intuitas_domain3_dev.ref.account_code Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g. intuitas_domain3_dev.edw_rv.hs_payments__finance_system__adf Business Vault : Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g. intuitas_domain3_dev.edw_bv.hs_late_payments__finance_system__adf Gold (Data according to requirements) The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists in case its needed. All schemas are may be optionally prefixed with gold All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Gold) Staging Models : Staging models serve as intermediary models that transform source data into the target mart model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold marts only. Schema naming convention: stg__{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align to target mart: (optional: dim/fact__){entity / product description}__{n}__{transformation} e.g. intuitas_domain3_dev.stg.fact__late_payments__01__pivoted_by_order e.g. intuitas_domain3_dev.stg__corporate.fact__late_payments__01__pivoted_by_order e.g. intuitas_domain3_dev.stg__corporate__finance.fact__late_payments__01__pivoted_by_order (Gold) Marts : Schema naming convention: mart{optional: __domain name}{optional: __subdomain name(s)} Dimension naming convention: dim__{entity / product description} (optional: __{source_system}__{source_channel}) Fact naming convention: fact__{entity / product description} (optional: __{source_system}__{source_channel}) Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}) Required transformation: Business-specific transformations such as: pivoting aggregation joining funnel creation conformance desensitization e.g. intuitas_domain3_dev.mart.fact__late_payments e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments__old_finance_system__adf e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments__new_finance_system__adf e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments (union of old and new) Delta Sharing Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g. intuitas_domain3__finance__reporting__account_payments__payments Azure Data Factory The following are in lower case: Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g. ls_financedb_intuitas_domain3 Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency} Streaming The following are in lower case: Cluster name: {domain_name} cluster {optional:environment} Topic names: {domain_name} {object/entity?} {optional:source_system} {optional:source channel} {optional:environment} Consumer group names: {domain_name} {unique_group_name} {optional:environment} dbt The following are in lower case: Documentation and model metadata Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g. models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g. models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml Sources Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g. bronze__ods__ambo_sim__kafka__local__sources.yml Model and Folder Names dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards. Bronze Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/ Silver Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} Example: `silver\\new_finance_system__adf\\stg\\intuitas_domain3__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql` or `silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql` materialises to: *intuitas_domain3_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:dim/fact}{__entity /_object_description} __{ordinal}_{transformation description} e.g. intuitas_domain3_dev.stg.accounts__01_deduped e.g. intuitas_domain3_dev.stg.accounts__02_business_validated Example: `silver\\mart\\accounts\\stg\\intuitas_domain3__silver__stg__accounts__01_deduped.sql` or `silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql` materialises to: *e.g. intuitas_domain3_dev.stg.accounts__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim/fact}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim/fact}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:dim/fact} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards} Gold Staging: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} Dimensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__dim{__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__fact{__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}} Example: {{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__dim_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__dim_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__dim_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__dim_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g. finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql dbt_project.yml example models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name CI/CD Repository naming All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-domain3-dbt - intuitas-domain3-ingestion-framework - intuitas-domain3-cicd-templates - intuitas-domain3-infrastructure Branch naming All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests Branch lifecycle Simple branch lifecycle: main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master Comprehensive team branch lifecycle: Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes Databricks Asset Bundles Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g. yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com Security Entra Group Names TBC Policies Data Retention Key Retention Frameworks Engineering Security","title":"Naming Conventions"},{"location":"naming_standards_and_conventions/#naming-conventions","text":"{Return to home}(README.md)","title":"Naming Conventions"},{"location":"naming_standards_and_conventions/#table-of-contents","text":"Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (ADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Group Names Policies Frameworks","title":"Table of Contents"},{"location":"naming_standards_and_conventions/#mesh","text":"","title":"Mesh"},{"location":"naming_standards_and_conventions/#domain-names","text":"All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g. intuitas_domain3","title":"Domain Names"},{"location":"naming_standards_and_conventions/#platform","text":"","title":"Platform"},{"location":"naming_standards_and_conventions/#environment","text":"Environment: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred)","title":"Environment"},{"location":"naming_standards_and_conventions/#vnet","text":"Name: vn-{organisation_name}-{domain_name} e.g. vn-intuitas-domain3","title":"VNET"},{"location":"naming_standards_and_conventions/#resource-groups","text":"Name: rg-{organisation_name}-{domain_name} e.g. rg-intuitas-domain3","title":"Resource Groups"},{"location":"naming_standards_and_conventions/#databricks-workspace","text":"Name: ws-{organisation_name}-{domain_name} e.g. ws-intuitas-domain3","title":"Databricks workspace"},{"location":"naming_standards_and_conventions/#key-vault","text":"Name: kv-{organisation_name}-{domain_name} e.g. kv-intuitas-domain3","title":"Key vault"},{"location":"naming_standards_and_conventions/#secrets","text":"Name: {secret_name}","title":"Secrets"},{"location":"naming_standards_and_conventions/#entra-group-names","text":"Name: eg-{organisation_name}-{domain_name} e.g. eg-intuitas-domain3","title":"Entra Group Names"},{"location":"naming_standards_and_conventions/#azure-data-factory-adf","text":"Name: adf-{organisation_name}-{domain_name} e.g. adf-intuitas-domain3","title":"Azure Data Factory (ADF)"},{"location":"naming_standards_and_conventions/#sql-server","text":"Name: sql-{organisation_name}-{domain_name} e.g. sql-intuitas-domain3","title":"SQL Server"},{"location":"naming_standards_and_conventions/#sql-database","text":"Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g. sqldb-metadata-intuitas-domain3","title":"SQL Database"},{"location":"naming_standards_and_conventions/#storage","text":"","title":"Storage"},{"location":"naming_standards_and_conventions/#lakehouse-storage","text":"Lakehouse storage account name: dl{organisation_name}{domain_name}","title":"Lakehouse storage"},{"location":"naming_standards_and_conventions/#lakehouse-storage-containers","text":"Name: {environment} (dev/test/preprod/prod)","title":"Lakehouse storage containers"},{"location":"naming_standards_and_conventions/#lakehouse-storage-folders","text":"Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog","title":"Lakehouse storage folders"},{"location":"naming_standards_and_conventions/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"naming_standards_and_conventions/#generic-blob-files-and-folders","text":"No standard naming conventions for files and folders.","title":"Generic Blob files and folders"},{"location":"naming_standards_and_conventions/#databricks","text":"","title":"Databricks"},{"location":"naming_standards_and_conventions/#workspace-and-cluster-names","text":"Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name}","title":"Workspace and cluster names"},{"location":"naming_standards_and_conventions/#catalog","text":"Catalog name: {domain_name}{environment} (prod is implied optional) e.g. intuitas_domain3_dev Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog","title":"Catalog"},{"location":"naming_standards_and_conventions/#externally-mounted-lakehouse-federation-catalog-names","text":"All lower case: {Domain (owner)}_ext__{source_system}{optional:__other_useful_descriptors} e.g. intuitas_domain3_ext__sqlonpremsource Metadata tags: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain","title":"Externally mounted (lakehouse federation) Catalog Names"},{"location":"naming_standards_and_conventions/#schema-and-object-conventions","text":"","title":"Schema and object conventions"},{"location":"naming_standards_and_conventions/#schema-level-external-storage-locations","text":"Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g. abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations.","title":"Schema level external storage locations"},{"location":"naming_standards_and_conventions/#metadata-schemas-and-objects","text":"Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__{optional: function} Naming convention: {function/descriptor} e.g. intuitas_domain3_dev.meta__ingestion.ingestion_control","title":"Metadata Schemas and Objects"},{"location":"naming_standards_and_conventions/#bronze-raw-data-according-to-systems","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. All schemas are may be optionally prefixed with bronze__ Persistent Landing : N/A (see file naming) Operational Data Store (ODS) : Schema naming convention: ods{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: ods__{source_system}__{source_channel}__{object} e.g. intuitas_domain3_dev.ods.ods__finance_system__adf__accounts Persistent Data Store (PDS) : Schema naming convention: pds{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: pds__{source_system}__{source_channel}__{object} e.g. intuitas_domain3_dev.pds.pds__finance_system__adf__accounts","title":"Bronze (Raw data according to systems)"},{"location":"naming_standards_and_conventions/#silver-data-according-to-business-entities","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. All schemas are may be optionally prefixed with silver All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver) Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts only. Source-specific: Schema naming convention: stg__{source_system}__{source_channel} Object naming convention: {entity}__{object_description}__{n}__{transformation}__{source_system}__{source_channel} e.g. intuitas_domain3_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf e.g. intuitas_domain3_dev.stg__new_finance_system__adf.accounts__02_cleaned__new_finance_system__adf e.g. intuitas_domain3_dev.stg__old_finance_system__adf.accounts__01_renamed_and_typed__old_finance_system__adf Non-source specific Schema naming convention: stg{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align with target mart: stg__(optional:dim/fact)_{entity}__{object_description}__{n}__{transformation} e.g. intuitas_domain3_dev.stg.accounts__01_deduped e.g. intuitas_domain3_dev.stg.accounts__02_business_validated Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g. intuitas_domain3_dev.stg__finance_system__adf.stg__finance_system__adf__account__01_renamed_and_typed (Silver) Marts : Final products after staging Source-specific: Schema naming convention: mart__{source_system}__{source_channel} Object naming convention: (optional:dim/fact)_{entity / _object_description}__{source_system}__{source_channel} e.g. intuitas_domain3_dev.mart__new_finance_system__adf.payment__new_finance_system__adf e.g. intuitas_domain3_dev.mart__new_finance_system__adf.account__new_finance_system__adf e.g. intuitas_domain3_dev.mart__old_finance_system__adf.account__old_finance_system__adf Non-source specific Schema naming convention: mart{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: (optional:dim/fact)__{unified entity / _object_description} e.g. intuitas_domain3_dev.mart.account (unified) e.g. intuitas_domain3_dev.mart__corporate__finance.account (unified) e.g. intuitas_domain3_dev.mart__finance.account (unified) e.g. intuitas_domain3_dev.mart.account_join_with_payments (joined across two systems) Reference Data : Reference data objects that are aligned to business entities and broad requirements. These may also be staged in stg as per silver marts. These are typically not source-aligned but optionality for capturing sources exists. Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference data set name} (optional:__{source_system}__{source_channel}) e.g. intuitas_domain3_dev.ref.account_code Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g. intuitas_domain3_dev.edw_rv.hs_payments__finance_system__adf Business Vault : Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g. intuitas_domain3_dev.edw_bv.hs_late_payments__finance_system__adf","title":"Silver (Data according to business entities)"},{"location":"naming_standards_and_conventions/#gold-data-according-to-requirements","text":"The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists in case its needed. All schemas are may be optionally prefixed with gold All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Gold) Staging Models : Staging models serve as intermediary models that transform source data into the target mart model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold marts only. Schema naming convention: stg__{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align to target mart: (optional: dim/fact__){entity / product description}__{n}__{transformation} e.g. intuitas_domain3_dev.stg.fact__late_payments__01__pivoted_by_order e.g. intuitas_domain3_dev.stg__corporate.fact__late_payments__01__pivoted_by_order e.g. intuitas_domain3_dev.stg__corporate__finance.fact__late_payments__01__pivoted_by_order (Gold) Marts : Schema naming convention: mart{optional: __domain name}{optional: __subdomain name(s)} Dimension naming convention: dim__{entity / product description} (optional: __{source_system}__{source_channel}) Fact naming convention: fact__{entity / product description} (optional: __{source_system}__{source_channel}) Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}) Required transformation: Business-specific transformations such as: pivoting aggregation joining funnel creation conformance desensitization e.g. intuitas_domain3_dev.mart.fact__late_payments e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments__old_finance_system__adf e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments__new_finance_system__adf e.g. intuitas_domain3_dev.mart.regionally_grouped_account_payments (union of old and new)","title":"Gold (Data according to requirements)"},{"location":"naming_standards_and_conventions/#delta-sharing","text":"Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g. intuitas_domain3__finance__reporting__account_payments__payments","title":"Delta Sharing"},{"location":"naming_standards_and_conventions/#azure-data-factory","text":"The following are in lower case: Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g. ls_financedb_intuitas_domain3 Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency}","title":"Azure Data Factory"},{"location":"naming_standards_and_conventions/#streaming","text":"The following are in lower case: Cluster name: {domain_name} cluster {optional:environment} Topic names: {domain_name} {object/entity?} {optional:source_system} {optional:source channel} {optional:environment} Consumer group names: {domain_name} {unique_group_name} {optional:environment}","title":"Streaming"},{"location":"naming_standards_and_conventions/#dbt","text":"The following are in lower case:","title":"dbt"},{"location":"naming_standards_and_conventions/#documentation-and-model-metadata","text":"Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g. models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g. models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml","title":"Documentation and model metadata"},{"location":"naming_standards_and_conventions/#sources","text":"Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g. bronze__ods__ambo_sim__kafka__local__sources.yml","title":"Sources"},{"location":"naming_standards_and_conventions/#model-and-folder-names","text":"dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards.","title":"Model and Folder Names"},{"location":"naming_standards_and_conventions/#bronze","text":"Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/","title":"Bronze"},{"location":"naming_standards_and_conventions/#silver","text":"Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} Example: `silver\\new_finance_system__adf\\stg\\intuitas_domain3__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql` or `silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql` materialises to: *intuitas_domain3_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:dim/fact}{__entity /_object_description} __{ordinal}_{transformation description} e.g. intuitas_domain3_dev.stg.accounts__01_deduped e.g. intuitas_domain3_dev.stg.accounts__02_business_validated Example: `silver\\mart\\accounts\\stg\\intuitas_domain3__silver__stg__accounts__01_deduped.sql` or `silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql` materialises to: *e.g. intuitas_domain3_dev.stg.accounts__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim/fact}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim/fact}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:dim/fact} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards}","title":"Silver"},{"location":"naming_standards_and_conventions/#gold","text":"Staging: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} Dimensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__dim{__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__fact{__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}}","title":"Gold"},{"location":"naming_standards_and_conventions/#example","text":"{{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__dim_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__dim_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__dim_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__dim_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g. finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql","title":"Example:"},{"location":"naming_standards_and_conventions/#dbt_projectyml","text":"example models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name","title":"dbt_project.yml"},{"location":"naming_standards_and_conventions/#cicd","text":"","title":"CI/CD"},{"location":"naming_standards_and_conventions/#repository-naming","text":"All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-domain3-dbt - intuitas-domain3-ingestion-framework - intuitas-domain3-cicd-templates - intuitas-domain3-infrastructure","title":"Repository naming"},{"location":"naming_standards_and_conventions/#branch-naming","text":"All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests","title":"Branch naming"},{"location":"naming_standards_and_conventions/#branch-lifecycle","text":"","title":"Branch lifecycle"},{"location":"naming_standards_and_conventions/#simple-branch-lifecycle","text":"main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master","title":"Simple branch lifecycle:"},{"location":"naming_standards_and_conventions/#comprehensive-team-branch-lifecycle","text":"Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes","title":"Comprehensive team branch lifecycle:"},{"location":"naming_standards_and_conventions/#databricks-asset-bundles","text":"Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g. yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com","title":"Databricks Asset Bundles"},{"location":"naming_standards_and_conventions/#security","text":"","title":"Security"},{"location":"naming_standards_and_conventions/#entra-group-names_1","text":"TBC","title":"Entra Group Names"},{"location":"naming_standards_and_conventions/#policies","text":"Data Retention Key Retention","title":"Policies"},{"location":"naming_standards_and_conventions/#frameworks","text":"Engineering Security","title":"Frameworks"}]}