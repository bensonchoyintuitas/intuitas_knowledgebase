{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intuitas knowledgebase This knowledgebase contains a collection of resources that describe Intuitas' approach to designing and delivering Data and AI solutions. Copyright This knowledge base and its contents are \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. Licence Permission is granted for free use, reproduction, and adaptation of this material, provided prior consent is obtained and appropriate attribution is given to the original author. Referenced third-party content is subject to the copyright terms of their respective owners. Table of Contents Level 0 - Enterprise-level context Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures Level 1 - Enterprise-level architecture Key concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Enterprise Security Enterprise Data Governance Audit Enterprise Billing Level 2 - Domain-level (solution) architecture Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data and information models Domain glossary Domain data and warehouse models Data Architecture Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Naming standards and conventions Naming standards and conventions","title":"Home"},{"location":"#intuitas-knowledgebase","text":"This knowledgebase contains a collection of resources that describe Intuitas' approach to designing and delivering Data and AI solutions.","title":"Intuitas knowledgebase"},{"location":"#copyright","text":"This knowledge base and its contents are \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials.","title":"Copyright"},{"location":"#licence","text":"Permission is granted for free use, reproduction, and adaptation of this material, provided prior consent is obtained and appropriate attribution is given to the original author. Referenced third-party content is subject to the copyright terms of their respective owners.","title":"Licence"},{"location":"#table-of-contents","text":"","title":"Table of Contents"},{"location":"#level-0-enterprise-level-context","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures","title":"Level 0 - Enterprise-level context"},{"location":"#level-1-enterprise-level-architecture","text":"Key concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Enterprise Security Enterprise Data Governance Audit Enterprise Billing","title":"Level 1 - Enterprise-level architecture"},{"location":"#level-2-domain-level-solution-architecture","text":"Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data and information models Domain glossary Domain data and warehouse models Data Architecture Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit","title":"Level 2 - Domain-level (solution) architecture"},{"location":"#naming-standards-and-conventions","text":"Naming standards and conventions","title":"Naming standards and conventions"},{"location":"about/","text":"Intuitas provides expertise in data architecture, governance, and real-time analytics , helping organisations build scalable and well-structured data systems. Our focus is on designing practical solutions that support efficient data management and decision-making. Areas of Expertise Enterprise Data Architecture & Governance Cloud & Hybrid Data Solutions Real-Time Data & Streaming Analytics AI-Driven Data Insights We take a pragmatic and structured approach to solving complex data challenges, ensuring systems are adaptable and aligned with business needs. For more information, contact us at office@intuitas.com.","title":"About"},{"location":"about/#areas-of-expertise","text":"Enterprise Data Architecture & Governance Cloud & Hybrid Data Solutions Real-Time Data & Streaming Analytics AI-Driven Data Insights We take a pragmatic and structured approach to solving complex data challenges, ensuring systems are adaptable and aligned with business needs. For more information, contact us at office@intuitas.com.","title":"Areas of Expertise"},{"location":"data_model/","text":"Sample Mermaid Diagram in Markdown Here is a class diagram: classDiagram class Patient { +String patientId +String firstName +String lastName +Date birthDate +String gender } class Encounter { +String encounterId +Date encounterDate +String encounterType +String patientId } class Condition { +String conditionId +String conditionName +Date onsetDate +String encounterId } class Procedure { +String procedureId +String procedureName +Date procedureDate +String encounterId } Patient \"1\" --> \"0..*\" Encounter : has Encounter \"1\" --> \"0..*\" Condition : includes Encounter \"1\" --> \"0..*\" Procedure : includes","title":"Sample Mermaid Diagram in Markdown"},{"location":"data_model/#sample-mermaid-diagram-in-markdown","text":"Here is a class diagram: classDiagram class Patient { +String patientId +String firstName +String lastName +Date birthDate +String gender } class Encounter { +String encounterId +Date encounterDate +String encounterType +String patientId } class Condition { +String conditionId +String conditionName +Date onsetDate +String encounterId } class Procedure { +String procedureId +String procedureName +Date procedureDate +String encounterId } Patient \"1\" --> \"0..*\" Encounter : has Encounter \"1\" --> \"0..*\" Condition : includes Encounter \"1\" --> \"0..*\" Procedure : includes","title":"Sample Mermaid Diagram in Markdown"},{"location":"level_0/","text":"Level 0 - Enterprise-level context Return to home This section describes enterprise-wide concepts which set the necessary business and strategic context for data and platform architecture at lower levels. Table of Contents Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures Org + Domain Definition Recommended: Description of the organisational boundary and any external organisations that are within the scope of concern. Description of domains and subdomains within the organisation. Strategies and Objectives Recommended: Description of the organisation's strategies and objectives at the whole of enterprise level: Business strategies, objectives, plans and initiatives Technology strategies, objectives, plans and initiatives Data and AI strategies, objectives, plans and initiatives Key Systems and Data Assets Recommended: Description of the organisation's key systems within the scope of concern. (CMDB) Information Asset Register (IAR) Team Capabilities and Roles Recommended: Description of in-scope key teams/parties associated with: Strategy and portfolio / prioritisation of data initiatives Data creation within the business domains Data management Data governance (quality, access) Data consumption Data engineering and integration Data analysis and reporting Data application development Data ops support Data security Data and information architecture Infrastructure and platform provisioning and management RACI matrix for key teams/parties Current and target operating and service management model Maturity and skill assessment of key teams/parties Governance Structures Recommended: Description of the organisation's governance frameworks, policies, standards, processes and bodies that are relevant to the scope of concern. Billing Structures Recommended: Description of the organisation's structures in terms of billing, and how cost centres may relate to billing reports, delegations and associated observability requirements.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#level-0-enterprise-level-context","text":"Return to home This section describes enterprise-wide concepts which set the necessary business and strategic context for data and platform architecture at lower levels.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#table-of-contents","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Billing Structures","title":"Table of Contents"},{"location":"level_0/#org-domain-definition","text":"Recommended: Description of the organisational boundary and any external organisations that are within the scope of concern. Description of domains and subdomains within the organisation.","title":"Org + Domain Definition"},{"location":"level_0/#strategies-and-objectives","text":"Recommended: Description of the organisation's strategies and objectives at the whole of enterprise level: Business strategies, objectives, plans and initiatives Technology strategies, objectives, plans and initiatives Data and AI strategies, objectives, plans and initiatives","title":"Strategies and Objectives"},{"location":"level_0/#key-systems-and-data-assets","text":"Recommended: Description of the organisation's key systems within the scope of concern. (CMDB) Information Asset Register (IAR)","title":"Key Systems and Data Assets"},{"location":"level_0/#team-capabilities-and-roles","text":"Recommended: Description of in-scope key teams/parties associated with: Strategy and portfolio / prioritisation of data initiatives Data creation within the business domains Data management Data governance (quality, access) Data consumption Data engineering and integration Data analysis and reporting Data application development Data ops support Data security Data and information architecture Infrastructure and platform provisioning and management RACI matrix for key teams/parties Current and target operating and service management model Maturity and skill assessment of key teams/parties","title":"Team Capabilities and Roles"},{"location":"level_0/#governance-structures","text":"Recommended: Description of the organisation's governance frameworks, policies, standards, processes and bodies that are relevant to the scope of concern.","title":"Governance Structures"},{"location":"level_0/#billing-structures","text":"Recommended: Description of the organisation's structures in terms of billing, and how cost centres may relate to billing reports, delegations and associated observability requirements.","title":"Billing Structures"},{"location":"level_1/","text":"Level 1 - Enterprise-level architecture Return to home This section describes enterprise-wide and cross-domain data and data platform architecture concepts. Table of Contents Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Key concepts The following key concepts are used throughout this knowledgebase. Domain Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. see Domain driven design Subdomain A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Domain-Centric Design Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Data Mesh A data mesh is a decentralized approach to data management that empowers domain teams to own their data and build data products. The then shares data as products with other domains. It emphasizes autonomy, flexibility, and interoperability. This approach is not necessarily appropriate for all organisations and organisations will embody its principles with varying degrees of maturity and success. see Data Mesh: Principles Domain Topology A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. see Data Mesh: Topologies and domain granularity Data Fabric A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles. Data Mesh vs Fabric A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric. Reference topologies Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Source: Data Mesh: Topologies and domain granularity Hybrid federated mesh topology Hybrid of Data Fabric and Data Mesh: - Combines centralised governance with domain-specific autonomy. - Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: - Scalable, unified platform: Connects diverse data sources across the organisation. - Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. - Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: - Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. - Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. - Federated governance: Ensures standards while allowing teams to manage their data locally. Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organization. Distributed Local engineering Maintaining a local bronze layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access gold data from other domains and, where appropriate, silver or bronze, facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability. Enterprise Data Platform Reference Architecture Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example: Enterprise (Logical) Data Warehouse Reference Architecture Logical Data Warehouse topology Reflects the domain topology. Provides unified access to data warehouse products from across domains via a common catalog. Example: Enterprise Information and Data Architecture Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models Enterprise Metadata Architecture Recommended: Description of applicable metadata standards Enterprise Metadata Architecture and Metamodel Description of metadata governance, stewards and processes Databricks Unity Catalog Metastore Only one metastore exists per region Documentation Metastore-level storage accounts are used to store the metastore and its associated data. Recommendation: Databricks recommends that you assign managed storage at the catalog level for logical data isolation, with metastore-level and schema-level as options. Catalog layouts Metastore admin is optional - all cases, the metastore admin role should be assigned to a group instead of an individual. The enterprise domain topology has a direct bearing on UC catalog layout and design. Enterprise Security Recommended: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs Enterprise Data Governance Recommended: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management Audit Recommended: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions. Example questions and associated queries This section is a work in progress As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Enterprise Billing Out of the box Databricks usage dashboard to be tested at workspace level Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform (i.e. Databricks) must to align. Recommendations here align with the following Domain topology: Databricks features for usage tracking Metadata and tags In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per https://docs.databricks.com/aws/en/admin/account-settings/usage-detail-tags Cluster policies Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions. System tables System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. https://learn.microsoft.com/en-au/azure/databricks/admin/system-tables/ Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (repo available on request) Usage reports Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables). https://learn.microsoft.com/en-au/azure/databricks/admin/account-settings/usage Domain / Workspace Administrator Role Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts Tagging convention All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain In addition all workloads may need more granular tracking in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id Environment: dev, test, uat, prod As an Enterprise Admin: 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin: 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries Recommended practices \"Top 10 Queries to use with System Tables\": https://community.databricks.com/t5/technical-blog/top-10-queries-to-use-with-system-tables/ba-p/82331 \"Unlocking Cost Optimization Insights with Databricks System Tables\" https://www.linkedin.com/pulse/unlocking-cost-optimization-insights-databricks-system-toraskar-nniaf **Selections from: \"Unlocking Cost Optimization Insights with Databricks System Tables\"** - Deenar Toraskar #### 1. Cluster Efficiency: CPU & Memory Utilization - Measures whether clusters are right-sized for workloads. - Low utilization indicates wasted resources. **Optimization Strategies:** - Select appropriate instance types. - Enable auto-scaling to dynamically allocate resources. - Configure auto-termination and timeouts to reduce idle time. #### 2. Cost Breakdown: All-Purpose vs. Job Clusters - All-Purpose Clusters cost twice as much DBUs as Job Clusters. **Best Practice:** Use All-Purpose Clusters primarily for development, exploratory data analysis, or collaboration. **Optimization:** Move non-interactive workloads to Job Clusters for lower costs. #### 3. Databricks Runtime (DBR) Version Compliance - Newer DBR versions improve performance, security, and reliability. - This metric measures the number of versions behind the latest DBR (weighted by spend). **Recommendation:** Regularly update to the latest DBR versions to ensure optimal performance and cost efficiency. #### 4. Job & Query Success Rate - Tracks the percentage of costs in successful jobs/queries. - Failed jobs result in wasted resources and potential re-runs. **Action:** Investigate high failure rates to identify and address workflow inefficiencies. #### 5. Use of On-Demand vs. Spot Clusters - Spot instances offer significant savings but can be preempted by cloud providers. - Databricks' auto-fallback mechanism (`SPOT_WITH_FALLBACK_AZURE`) ensures jobs finish using on-demand resources if spot capacity is preempted. **Best Practice:** Use spot instances for test/dev workloads and workloads with flexible SLAs. #### 6. Startup Time vs. Processing Time - Measures the time spent in startup overhead versus actual processing. - Small workloads may be inefficient on Spark clusters. **Optimization Options:** - Use serverless compute or instance pools. - Consolidate small jobs into larger workloads. - Move non-distributed workloads to alternative compute solutions (e.g., AKS). #### 7. SQL Warehouse Utilization & Costs - Evaluates whether SQL Warehouses are right-sized for workloads. - Identifies underutilized instances, highlighting cost-saving opportunities. **Best Practice:** Tune warehouse size, auto-scaling, and timeout settings. #### 8. Instance Type Efficiency Score - Using latest-generation cloud instance types offers better price-performance compared to older generations.","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#level-1-enterprise-level-architecture","text":"Return to home This section describes enterprise-wide and cross-domain data and data platform architecture concepts.","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#table-of-contents","text":"Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing","title":"Table of Contents"},{"location":"level_1/#key-concepts","text":"The following key concepts are used throughout this knowledgebase.","title":"Key concepts"},{"location":"level_1/#domain","text":"Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. see Domain driven design","title":"Domain"},{"location":"level_1/#subdomain","text":"A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area.","title":"Subdomain"},{"location":"level_1/#domain-centric-design","text":"Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy.","title":"Domain-Centric Design"},{"location":"level_1/#data-mesh","text":"A data mesh is a decentralized approach to data management that empowers domain teams to own their data and build data products. The then shares data as products with other domains. It emphasizes autonomy, flexibility, and interoperability. This approach is not necessarily appropriate for all organisations and organisations will embody its principles with varying degrees of maturity and success. see Data Mesh: Principles","title":"Data Mesh"},{"location":"level_1/#domain-topology","text":"A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. see Data Mesh: Topologies and domain granularity","title":"Domain Topology"},{"location":"level_1/#data-fabric","text":"A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles.","title":"Data Fabric"},{"location":"level_1/#data-mesh-vs-fabric","text":"A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric.","title":"Data Mesh vs Fabric"},{"location":"level_1/#reference-topologies","text":"Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Source: Data Mesh: Topologies and domain granularity","title":"Reference topologies"},{"location":"level_1/#hybrid-federated-mesh-topology","text":"Hybrid of Data Fabric and Data Mesh: - Combines centralised governance with domain-specific autonomy. - Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: - Scalable, unified platform: Connects diverse data sources across the organisation. - Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. - Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: - Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. - Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. - Federated governance: Ensures standards while allowing teams to manage their data locally. Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organization. Distributed Local engineering Maintaining a local bronze layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access gold data from other domains and, where appropriate, silver or bronze, facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability.","title":"Hybrid federated mesh topology"},{"location":"level_1/#enterprise-data-platform-reference-architecture","text":"Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example:","title":"Enterprise Data Platform Reference Architecture"},{"location":"level_1/#enterprise-logical-data-warehouse-reference-architecture","text":"Logical Data Warehouse topology Reflects the domain topology. Provides unified access to data warehouse products from across domains via a common catalog. Example:","title":"Enterprise (Logical) Data Warehouse Reference Architecture"},{"location":"level_1/#enterprise-information-and-data-architecture","text":"Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models","title":"Enterprise Information and Data Architecture"},{"location":"level_1/#enterprise-metadata-architecture","text":"Recommended: Description of applicable metadata standards Enterprise Metadata Architecture and Metamodel Description of metadata governance, stewards and processes","title":"Enterprise Metadata Architecture"},{"location":"level_1/#databricks-unity-catalog-metastore","text":"Only one metastore exists per region Documentation Metastore-level storage accounts are used to store the metastore and its associated data. Recommendation: Databricks recommends that you assign managed storage at the catalog level for logical data isolation, with metastore-level and schema-level as options. Catalog layouts Metastore admin is optional - all cases, the metastore admin role should be assigned to a group instead of an individual. The enterprise domain topology has a direct bearing on UC catalog layout and design.","title":"Databricks Unity Catalog Metastore"},{"location":"level_1/#enterprise-security","text":"Recommended: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs","title":"Enterprise Security"},{"location":"level_1/#enterprise-data-governance","text":"Recommended: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management","title":"Enterprise Data Governance"},{"location":"level_1/#audit","text":"Recommended: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions.","title":"Audit"},{"location":"level_1/#example-questions-and-associated-queries","text":"This section is a work in progress As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_1/#enterprise-billing","text":"Out of the box Databricks usage dashboard to be tested at workspace level Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform (i.e. Databricks) must to align. Recommendations here align with the following Domain topology:","title":"Enterprise Billing"},{"location":"level_1/#databricks-features-for-usage-tracking","text":"","title":"Databricks features for usage tracking"},{"location":"level_1/#metadata-and-tags","text":"In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per https://docs.databricks.com/aws/en/admin/account-settings/usage-detail-tags","title":"Metadata and tags"},{"location":"level_1/#cluster-policies","text":"Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions.","title":"Cluster policies"},{"location":"level_1/#system-tables","text":"System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. https://learn.microsoft.com/en-au/azure/databricks/admin/system-tables/ Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (repo available on request)","title":"System tables"},{"location":"level_1/#usage-reports","text":"Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables). https://learn.microsoft.com/en-au/azure/databricks/admin/account-settings/usage","title":"Usage reports"},{"location":"level_1/#domain-workspace-administrator-role","text":"Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts","title":"Domain / Workspace Administrator Role"},{"location":"level_1/#tagging-convention","text":"All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain In addition all workloads may need more granular tracking in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id Environment: dev, test, uat, prod As an Enterprise Admin: 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin: 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Tagging convention"},{"location":"level_1/#recommended-practices","text":"\"Top 10 Queries to use with System Tables\": https://community.databricks.com/t5/technical-blog/top-10-queries-to-use-with-system-tables/ba-p/82331 \"Unlocking Cost Optimization Insights with Databricks System Tables\" https://www.linkedin.com/pulse/unlocking-cost-optimization-insights-databricks-system-toraskar-nniaf **Selections from: \"Unlocking Cost Optimization Insights with Databricks System Tables\"** - Deenar Toraskar #### 1. Cluster Efficiency: CPU & Memory Utilization - Measures whether clusters are right-sized for workloads. - Low utilization indicates wasted resources. **Optimization Strategies:** - Select appropriate instance types. - Enable auto-scaling to dynamically allocate resources. - Configure auto-termination and timeouts to reduce idle time. #### 2. Cost Breakdown: All-Purpose vs. Job Clusters - All-Purpose Clusters cost twice as much DBUs as Job Clusters. **Best Practice:** Use All-Purpose Clusters primarily for development, exploratory data analysis, or collaboration. **Optimization:** Move non-interactive workloads to Job Clusters for lower costs. #### 3. Databricks Runtime (DBR) Version Compliance - Newer DBR versions improve performance, security, and reliability. - This metric measures the number of versions behind the latest DBR (weighted by spend). **Recommendation:** Regularly update to the latest DBR versions to ensure optimal performance and cost efficiency. #### 4. Job & Query Success Rate - Tracks the percentage of costs in successful jobs/queries. - Failed jobs result in wasted resources and potential re-runs. **Action:** Investigate high failure rates to identify and address workflow inefficiencies. #### 5. Use of On-Demand vs. Spot Clusters - Spot instances offer significant savings but can be preempted by cloud providers. - Databricks' auto-fallback mechanism (`SPOT_WITH_FALLBACK_AZURE`) ensures jobs finish using on-demand resources if spot capacity is preempted. **Best Practice:** Use spot instances for test/dev workloads and workloads with flexible SLAs. #### 6. Startup Time vs. Processing Time - Measures the time spent in startup overhead versus actual processing. - Small workloads may be inefficient on Spark clusters. **Optimization Options:** - Use serverless compute or instance pools. - Consolidate small jobs into larger workloads. - Move non-distributed workloads to alternative compute solutions (e.g., AKS). #### 7. SQL Warehouse Utilization & Costs - Evaluates whether SQL Warehouses are right-sized for workloads. - Identifies underutilized instances, highlighting cost-saving opportunities. **Best Practice:** Tune warehouse size, auto-scaling, and timeout settings. #### 8. Instance Type Efficiency Score - Using latest-generation cloud instance types offers better price-performance compared to older generations.","title":"Recommended practices"},{"location":"level_2/","text":"Level 2 - Domain-Level (Solution) Architecture and Patterns Return to home Table of Contents Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Billing Domain-level solutions are instantiations of the enterprise-level reference architecture. (See Enterprise Data Platform Reference Architecture ) Example reference architecture: Business architecture Business processes Business processes are the activities and tasks that are performed to achieve the goals of the business. Understanding them is necessary to understand: - the context in which data is captured and used - concepts and entities that are relevant to the domain - the relationships between different processes and data Business glossary A business glossary is a list of terms and definitions that are relevant to the business. see Domain Glossary. Business metrics Metrics are the measurements of the performance of the business processes. They should be documented according to a defined template that captures, at a minimum, the following: - name - definition - formula (with reference to data elements and definitions in the business glossary) - dimensions - source(s) - metric owner - frequency Infrastructure This section is a work in progress Environments, Workspaces and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage. Secrets This section is a work in progress - Management - Areas of use - Handling practices Storage Lakehouse storage Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod CICD and Repository This section is a work in progress - Description of git workflows for CICD in terms of: - Infrastructure - Data engineering - Analytics engineering - Data science / AIML - BI, Reports and other products Tools Github Azure Devops Databricks Asset Bundles Repositories Infrastructure dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles Observability Tools dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring Networking By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies are enabled. Orchestration Tools Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt) Security Tools Azure Entra Azure Key Vault Unity Catalog System access reports Data Architecture Data Architecture refers to how data is physically structured, stored, and accessed within an organization. It encompasses the design and management of data storage systems, data models, data integration processes, and data governance practices. Data and information models Domain-level data and information models are typically closer aligned to real-world business semantics and business rules, which may not necessarily align with the broader enterprise or other domains. See Bounded context Domain glossary Expand on the enterprise glossary and add domain specific terms and definitions. In cases where domain definitions are synonymous with enterprise definitions, the enterprise glossary should be referenced. In cases where definitions are conflicting, governance should be applied to resolve the conflict. Domain data and warehouse models Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities. Data layers and stages Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Top level layers follow the Medallion architecture . Within each layer, data is transformed through a series of stages. Metadata Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Bronze The Bronze layer stores raw, immutable data as it is ingested from source systems. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing Silver The Silver layer is source centric and focuses on transforming raw data into cleaned, enriched, and validated datasets. Base Models Representation of source data with no changes. Used as a foundation for staging models as well as data quality checks. Staging Models Source-system and object centric transformations that are core to all downstream consumption. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Enrichment Models Still source-centric, however: - more complex business logic and transformations are applied e.g. common calculations and derivations. - may combine multiple staging objects from the same source By separating enrichment from core staging, we can schedule these processes independently. This allows for flexibility in updating or refreshing only the parts of the data pipeline that need it, reducing unnecessary computation and improving efficiency. It also allows for change and versioning of those business rules with minimal impact on core staging objects. Source Reference Data For convenience, reference data specific to the source can be segregated here and aligned to standards and downstream needs. Raw Vault Data vault 2.0 aligned raw data warehouse. Gold The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. Business Vault Data vault 2.0 aligned business data warehouse where business rules and transformations are applied. Intermediate Models These act as building blocks for marts, transforming and aggregating data further. Then be thought of as mart staging https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business-specific transformations such as: Pivoting Aggregation Joining Funnel creation Conformance Desensitization Enterprise Reference Data Reference data, independent of source can be aggregated here for broad consumption. Marts - Facts and dimensions Kimball style marts that represent business entities and processes. They may serve foundational or narrow requirements. be scoped to specific systems or conformed across the enterprise Marts - Denormalised Single table / view objects that combine data from multiple objects (e.g. facts and dimensions) Lakehouse Catalog to Storage Mapping Unity catalog objects (catalogs, schemas, objects) are mapped to: - Storage accounts - Environments (containers: dev, test, prod) - Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) - Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Example: Data Engineering Ingestion This section is a work in progress Improved diagrams and assessments of: lakeflow streaming: kafka -> landing -> databricks autoloader -> ods streaming: kafka -> iceberg assessed in terms of: cost performance resilience maintainability governance CDC patterns sql-server (ct tables, datatyping via views, custom init vs change sources and handling) Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: - Scalable, Resilient, Maintainable, Governed - Pattern-based, automated and Metadata-driven where possible - Batch and stream-based Example batch ingestion options: Ingestion patterns and notes: built and working - PAttern 1: streaming: kafka -> landing -> databricks autoloader -> ods - see repo Bronze Landing to ODS Project built and working - Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods - see repo Bronze landing SQL Server to ODS Project - adf requires azure sql and on-premise integration runtime built and working - Pattern 3: batch: source -> databricks lakehouse federation -> databricks workflows -> ods, pds - see repo External Database to ODS Project - requires network access to source i.e. poor mans datalake. todo - Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Sharepoint ingestion - Pattern 5: sharepoint -> fivetran -> databricks sql warehouse (ods) - see repo fivetran Rejected patterns: - batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) - batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) - batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks) Transformation This section is a work in progress Batch and Micro-batch SQL transformation dbt see dbt standards Streaming SQL transformation This section is a work in progress Non SQL transformation This section is a work in progress Data sharing and delivery patterns Row Level Security see Row Level Security Pull / direct access Databricks Delta sharing practices Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference:https://www.databricks.com/blog/2022/08/01/security-best-practices-for-delta-sharing.html ADLSGen2 access to data ADLSGen2 access, while technically possible, is not recommended as it bypasses the unity catalog and associated governance and observability. Given Delta Sharing, then direct ADLS file sharing is usually unnecessary. However, there are still a few edge cases where ADLS file sharing might be preferable, even when Delta Sharing is available: Unstructured data Large non delta-file transfer Consumers that dont support delta-sharing Duckdb access to data (via Unity Catalog) Duckdb is a popular open source SQL engine that can be used to access data in the lakehouse. Duckdb can be run on a local machine or in process in a databricks cluster. Costs: Duckdb data access will incur costs of the underlying compute, storage access, data transfer etc as per delta sharing. Opportunities / uses: Last mile analysis SQL interface to delta, iceberg, parquet, csv, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualisation tools e.g. Apache Superset see repo Duckdb Limitations: Unity Catalog not yet supported Delta Kernel not yet supported SQL Access SQL Access is provided by the Databricks SQL (serverless) endpoint. API Access This section is a work in progress - The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html Snowflake Access This section is a work in progress Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial Microsoft Fabric Access This section is a work in progress and required validation The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing - Pros: - No duplication - Centralised control over access policies - Compute costs on consumer - Cons: - Less control over access policies than Delta Sharing - No Row Level Security and Masking support (dynamic views required) Option 2. Directlake via ADLSGen2 - Pros: - No duplication - Potentially better PowerBI performance (untested) - Compute costs on consumer - No views - Cons: - Less control over access policies than Delta Sharing (outside of Unity Catalog) - Requires granular ADLSGen2 access controls and service principals, and associated management overhead - No Row Level Security and Masking support Option 3. Fabric mirrored unity catalog - Pros: - No duplication - Convenient access to all Databricks Unity Catalog objects (wihtin credential scope) - Cons: - not GA - lots of limitations Option 4. PowerBI Access Via SQL Endpoint - Pros: - No duplication - Potentially better PowerBI performance (untested) - Row Level Security and Masking support - Cons: - Compute costs on Databricks as well as Fabric Option 5 . Replicate into Fabric - Pros: - Possibly reduced networking costs (depending on workload and networking topology) - Cons: - Duplicated data - Engineering costs and overheads - Latency - Less governance control - No Row Level Security and Masking support Push This section is a work in progress For consideration: - adf - databricks - lakeflow Visualisation This section is a work in progress For consideration: - Powerbi - Databricks dashboards - Apps - Open source visual options AI/ML This section is a work in progress For consideration: - MLOps - Training - Databricks - Azure ML Data governance This section describes how Enterprise-level governance will be implemented through solutions at the domain level. Data lifecycle and asset management This section is a work in progress For consideration: - data contracts and policy - data asset tagging Data access management This section is a work in progress For consideration: - data access request management - data contracts - access audit - activity audit Data quality This section is a work in progress For consideration: - data quality checking and reporting - data standards and quality business rule management Data understandability This section is a work in progress For consideration: - data lineage - data object metadata Privacy Preservation This section is a work in progress For consideration: - row level security - data masking - column level security - data anonymisation - data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names. Row Level Security This section is a work in progress For consideration: - dynamic views - precomputed views Audit This section is a work in progress For consideration: - audit log queries Example questions and associated queries As a Domain (workspace) Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Billing For more context and concepts, refer to the Enterprise Billing section in Level 1.","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#level-2-domain-level-solution-architecture-and-patterns","text":"Return to home","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#table-of-contents","text":"Business architecture Business processes Business glossary Business metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data Sharing and Delivery Patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Billing Domain-level solutions are instantiations of the enterprise-level reference architecture. (See Enterprise Data Platform Reference Architecture ) Example reference architecture:","title":"Table of Contents"},{"location":"level_2/#business-architecture","text":"","title":"Business architecture"},{"location":"level_2/#business-processes","text":"Business processes are the activities and tasks that are performed to achieve the goals of the business. Understanding them is necessary to understand: - the context in which data is captured and used - concepts and entities that are relevant to the domain - the relationships between different processes and data","title":"Business processes"},{"location":"level_2/#business-glossary","text":"A business glossary is a list of terms and definitions that are relevant to the business. see Domain Glossary.","title":"Business glossary"},{"location":"level_2/#business-metrics","text":"Metrics are the measurements of the performance of the business processes. They should be documented according to a defined template that captures, at a minimum, the following: - name - definition - formula (with reference to data elements and definitions in the business glossary) - dimensions - source(s) - metric owner - frequency","title":"Business metrics"},{"location":"level_2/#infrastructure","text":"This section is a work in progress","title":"Infrastructure"},{"location":"level_2/#environments-workspaces-and-storage","text":"This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage.","title":"Environments, Workspaces and Storage"},{"location":"level_2/#secrets","text":"This section is a work in progress - Management - Areas of use - Handling practices","title":"Secrets"},{"location":"level_2/#storage","text":"","title":"Storage"},{"location":"level_2/#lakehouse-storage","text":"Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Lakehouse storage"},{"location":"level_2/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"level_2/#cicd-and-repository","text":"This section is a work in progress - Description of git workflows for CICD in terms of: - Infrastructure - Data engineering - Analytics engineering - Data science / AIML - BI, Reports and other products","title":"CICD and Repository"},{"location":"level_2/#tools","text":"Github Azure Devops Databricks Asset Bundles","title":"Tools"},{"location":"level_2/#repositories","text":"Infrastructure dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles","title":"Repositories"},{"location":"level_2/#observability","text":"","title":"Observability"},{"location":"level_2/#tools_1","text":"dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring","title":"Tools"},{"location":"level_2/#networking","text":"By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies are enabled.","title":"Networking"},{"location":"level_2/#orchestration","text":"","title":"Orchestration"},{"location":"level_2/#tools_2","text":"Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt)","title":"Tools"},{"location":"level_2/#security","text":"","title":"Security"},{"location":"level_2/#tools_3","text":"Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Tools"},{"location":"level_2/#data-architecture","text":"Data Architecture refers to how data is physically structured, stored, and accessed within an organization. It encompasses the design and management of data storage systems, data models, data integration processes, and data governance practices.","title":"Data Architecture"},{"location":"level_2/#data-and-information-models","text":"Domain-level data and information models are typically closer aligned to real-world business semantics and business rules, which may not necessarily align with the broader enterprise or other domains. See Bounded context","title":"Data and information models"},{"location":"level_2/#domain-glossary","text":"Expand on the enterprise glossary and add domain specific terms and definitions. In cases where domain definitions are synonymous with enterprise definitions, the enterprise glossary should be referenced. In cases where definitions are conflicting, governance should be applied to resolve the conflict.","title":"Domain glossary"},{"location":"level_2/#domain-data-and-warehouse-models","text":"Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities.","title":"Domain data and warehouse models"},{"location":"level_2/#data-layers-and-stages","text":"Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Top level layers follow the Medallion architecture . Within each layer, data is transformed through a series of stages.","title":"Data layers and stages"},{"location":"level_2/#metadata","text":"Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets","title":"Metadata"},{"location":"level_2/#bronze","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing","title":"Bronze"},{"location":"level_2/#silver","text":"The Silver layer is source centric and focuses on transforming raw data into cleaned, enriched, and validated datasets. Base Models Representation of source data with no changes. Used as a foundation for staging models as well as data quality checks. Staging Models Source-system and object centric transformations that are core to all downstream consumption. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Enrichment Models Still source-centric, however: - more complex business logic and transformations are applied e.g. common calculations and derivations. - may combine multiple staging objects from the same source By separating enrichment from core staging, we can schedule these processes independently. This allows for flexibility in updating or refreshing only the parts of the data pipeline that need it, reducing unnecessary computation and improving efficiency. It also allows for change and versioning of those business rules with minimal impact on core staging objects. Source Reference Data For convenience, reference data specific to the source can be segregated here and aligned to standards and downstream needs. Raw Vault Data vault 2.0 aligned raw data warehouse.","title":"Silver"},{"location":"level_2/#gold","text":"The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. Business Vault Data vault 2.0 aligned business data warehouse where business rules and transformations are applied. Intermediate Models These act as building blocks for marts, transforming and aggregating data further. Then be thought of as mart staging https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business-specific transformations such as: Pivoting Aggregation Joining Funnel creation Conformance Desensitization Enterprise Reference Data Reference data, independent of source can be aggregated here for broad consumption. Marts - Facts and dimensions Kimball style marts that represent business entities and processes. They may serve foundational or narrow requirements. be scoped to specific systems or conformed across the enterprise Marts - Denormalised Single table / view objects that combine data from multiple objects (e.g. facts and dimensions)","title":"Gold"},{"location":"level_2/#lakehouse-catalog-to-storage-mapping","text":"Unity catalog objects (catalogs, schemas, objects) are mapped to: - Storage accounts - Environments (containers: dev, test, prod) - Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) - Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Example:","title":"Lakehouse Catalog to Storage Mapping"},{"location":"level_2/#data-engineering","text":"","title":"Data Engineering"},{"location":"level_2/#ingestion","text":"This section is a work in progress Improved diagrams and assessments of: lakeflow streaming: kafka -> landing -> databricks autoloader -> ods streaming: kafka -> iceberg assessed in terms of: cost performance resilience maintainability governance CDC patterns sql-server (ct tables, datatyping via views, custom init vs change sources and handling) Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: - Scalable, Resilient, Maintainable, Governed - Pattern-based, automated and Metadata-driven where possible - Batch and stream-based Example batch ingestion options:","title":"Ingestion"},{"location":"level_2/#ingestion-patterns-and-notes","text":"built and working - PAttern 1: streaming: kafka -> landing -> databricks autoloader -> ods - see repo Bronze Landing to ODS Project built and working - Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods - see repo Bronze landing SQL Server to ODS Project - adf requires azure sql and on-premise integration runtime built and working - Pattern 3: batch: source -> databricks lakehouse federation -> databricks workflows -> ods, pds - see repo External Database to ODS Project - requires network access to source i.e. poor mans datalake. todo - Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Sharepoint ingestion - Pattern 5: sharepoint -> fivetran -> databricks sql warehouse (ods) - see repo fivetran Rejected patterns: - batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) - batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) - batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks)","title":"Ingestion patterns and notes:"},{"location":"level_2/#transformation","text":"This section is a work in progress","title":"Transformation"},{"location":"level_2/#batch-and-micro-batch-sql-transformation","text":"dbt see dbt standards","title":"Batch and Micro-batch SQL transformation"},{"location":"level_2/#streaming-sql-transformation","text":"This section is a work in progress","title":"Streaming SQL transformation"},{"location":"level_2/#non-sql-transformation","text":"This section is a work in progress","title":"Non SQL transformation"},{"location":"level_2/#data-sharing-and-delivery-patterns","text":"","title":"Data sharing and delivery patterns"},{"location":"level_2/#row-level-security","text":"see Row Level Security","title":"Row Level Security"},{"location":"level_2/#pull-direct-access","text":"","title":"Pull / direct access"},{"location":"level_2/#databricks-delta-sharing-practices","text":"Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference:https://www.databricks.com/blog/2022/08/01/security-best-practices-for-delta-sharing.html","title":"Databricks Delta sharing practices"},{"location":"level_2/#adlsgen2-access-to-data","text":"ADLSGen2 access, while technically possible, is not recommended as it bypasses the unity catalog and associated governance and observability. Given Delta Sharing, then direct ADLS file sharing is usually unnecessary. However, there are still a few edge cases where ADLS file sharing might be preferable, even when Delta Sharing is available: Unstructured data Large non delta-file transfer Consumers that dont support delta-sharing","title":"ADLSGen2 access to data"},{"location":"level_2/#duckdb-access-to-data-via-unity-catalog","text":"Duckdb is a popular open source SQL engine that can be used to access data in the lakehouse. Duckdb can be run on a local machine or in process in a databricks cluster. Costs: Duckdb data access will incur costs of the underlying compute, storage access, data transfer etc as per delta sharing. Opportunities / uses: Last mile analysis SQL interface to delta, iceberg, parquet, csv, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualisation tools e.g. Apache Superset see repo Duckdb Limitations: Unity Catalog not yet supported Delta Kernel not yet supported","title":"Duckdb access to data (via Unity Catalog)"},{"location":"level_2/#sql-access","text":"SQL Access is provided by the Databricks SQL (serverless) endpoint.","title":"SQL Access"},{"location":"level_2/#api-access","text":"This section is a work in progress - The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html","title":"API Access"},{"location":"level_2/#snowflake-access","text":"This section is a work in progress Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial","title":"Snowflake Access"},{"location":"level_2/#microsoft-fabric-access","text":"This section is a work in progress and required validation The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing - Pros: - No duplication - Centralised control over access policies - Compute costs on consumer - Cons: - Less control over access policies than Delta Sharing - No Row Level Security and Masking support (dynamic views required) Option 2. Directlake via ADLSGen2 - Pros: - No duplication - Potentially better PowerBI performance (untested) - Compute costs on consumer - No views - Cons: - Less control over access policies than Delta Sharing (outside of Unity Catalog) - Requires granular ADLSGen2 access controls and service principals, and associated management overhead - No Row Level Security and Masking support Option 3. Fabric mirrored unity catalog - Pros: - No duplication - Convenient access to all Databricks Unity Catalog objects (wihtin credential scope) - Cons: - not GA - lots of limitations Option 4. PowerBI Access Via SQL Endpoint - Pros: - No duplication - Potentially better PowerBI performance (untested) - Row Level Security and Masking support - Cons: - Compute costs on Databricks as well as Fabric Option 5 . Replicate into Fabric - Pros: - Possibly reduced networking costs (depending on workload and networking topology) - Cons: - Duplicated data - Engineering costs and overheads - Latency - Less governance control - No Row Level Security and Masking support","title":"Microsoft Fabric Access"},{"location":"level_2/#push","text":"This section is a work in progress For consideration: - adf - databricks - lakeflow","title":"Push"},{"location":"level_2/#visualisation","text":"This section is a work in progress For consideration: - Powerbi - Databricks dashboards - Apps - Open source visual options","title":"Visualisation"},{"location":"level_2/#aiml","text":"This section is a work in progress For consideration: - MLOps - Training - Databricks - Azure ML","title":"AI/ML"},{"location":"level_2/#data-governance","text":"This section describes how Enterprise-level governance will be implemented through solutions at the domain level.","title":"Data governance"},{"location":"level_2/#data-lifecycle-and-asset-management","text":"This section is a work in progress For consideration: - data contracts and policy - data asset tagging","title":"Data lifecycle and asset management"},{"location":"level_2/#data-access-management","text":"This section is a work in progress For consideration: - data access request management - data contracts - access audit - activity audit","title":"Data access management"},{"location":"level_2/#data-quality","text":"This section is a work in progress For consideration: - data quality checking and reporting - data standards and quality business rule management","title":"Data quality"},{"location":"level_2/#data-understandability","text":"This section is a work in progress For consideration: - data lineage - data object metadata","title":"Data understandability"},{"location":"level_2/#privacy-preservation","text":"This section is a work in progress For consideration: - row level security - data masking - column level security - data anonymisation - data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names.","title":"Privacy Preservation"},{"location":"level_2/#row-level-security_1","text":"This section is a work in progress For consideration: - dynamic views - precomputed views","title":"Row Level Security"},{"location":"level_2/#audit","text":"This section is a work in progress For consideration: - audit log queries","title":"Audit"},{"location":"level_2/#example-questions-and-associated-queries","text":"As a Domain (workspace) Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_2/#billing","text":"For more context and concepts, refer to the Enterprise Billing section in Level 1.","title":"Billing"},{"location":"naming_standards_and_conventions/","text":"Naming Conventions Return to home Table of Contents Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (ADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model Folders Model Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Group Names Policies Frameworks Mesh Domain Names All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g. intuitas_domain3 Platform Environment Environment: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred) VNET Name: vn-{organisation_name}-{domain_name} e.g. vn-intuitas-domain3 Resource Groups Name: rg-{organisation_name}-{domain_name} e.g. rg-intuitas-domain3 Databricks workspace Name: ws-{organisation_name}-{domain_name} e.g. ws-intuitas-domain3 Key vault Name: kv-{organisation_name}-{domain_name} e.g. kv-intuitas-domain3 Secrets Name: {secret_name} Entra Group Names Name: eg-{organisation_name}-{domain_name} e.g. eg-intuitas-domain3 Azure Data Factory (ADF) Name: adf-{organisation_name}-{domain_name} e.g. adf-intuitas-domain3 SQL Server Name: sql-{organisation_name}-{domain_name} e.g. sql-intuitas-domain3 SQL Database Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g. sqldb-metadata-intuitas-domain3 Storage Lakehouse storage Lakehouse storage account name: dl{organisation_name}{domain_name} Lakehouse storage containers Name: {environment} (dev/test/preprod/prod) Lakehouse storage folders Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} bronze/landing --- tbc --- might be managed by databricks within the catalog storage root silver/base silver/staging silver/enriched silver/edw_rv silver/edw_bv gold/mart Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob files and folders No standard naming conventions for files and folders. Databricks Workspace and cluster names Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name} Catalog Catalog name: {domain_name}_{environment} (prod is optional) e.g. intuitas_domain3_dev Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog Externally mounted (lakehouse federation) Catalog Names All lower case: {Domain (owner)}_ext__{source_system}{optional:__other_useful_descriptors} e.g. intuitas_domain3_ext__sqlonpremsource Metadata tags: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Schema and object conventions Schema level external storage locations Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g. abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations. Metadata Schemas and Objects Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__[optional: function] Naming convention: [function/descriptor] e.g. intuitas_domain3_dev.meta__ingestion.ingestion_control Bronze (Raw) Schemas and Objects The Bronze layer stores raw, immutable data as it is ingested from source systems. All schemas are may be optionally prefixed with bronze__ Persistent Landing : N/A (see file naming) Operational Data Store (ODS) : Schema naming convention: ods Object naming convention: ods__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.ods.ods__finance_system__adf__accounts Persistent Data Store (PDS) : Schema naming convention: pds Object naming convention: pds__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.pds.pds__finance_system__adf__accounts Silver (Source-Centric - Filtered, Cleaned, Augmented) The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. All schemas are may be optionally prefixed with silver__ Base Views : Schema naming convention: base__[source_system]__[source_channel] Object naming convention: base__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.base__finance_system__adf.base__finance_system__adf__accounts Staging Objects (Optional) : Schema naming convention: stg__[source_system]__[source_channel] Object naming convention: stg__[source_system]__[source_channel]__[object]__[n]__[transformation] Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g. intuitas_domain3_dev.stg__finance_system__adf.stg__finance_system__adf__accounts__01_renamed_and_typed Enriched Data : Schema naming convention: enr__[source_system]__[source_channel] Object naming convention: enr__[source_system]__[source_channel]__[new_object] e.g. intuitas_domain3_dev.enr__finance_system__adf.enr__finance_system__adf__accounts_join_with_payments Reference Data : Schema naming convention: ref__[source_system]__[source_channel] Object naming convention: ref__[source_system]__[source_channel]__[entity] e.g. intuitas_domain3_dev.ref__finance_system__adf.ref__finance_system__adf__account_codes Raw Vault : Schema naming convention: edw_rv Object naming convention: edw_rv__[vault object] e.g. intuitas_domain3_dev.edw_rv.hs_payments__finance_system__adf Gold (Business-Centric - Optionally Source-Decomposed) The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. All schemas are may be optionally prefixed with gold__ Business Vault : Schema naming convention: edw_bv Object naming convention: edw_bv__[vault object] e.g. intuitas_domain3_dev.edw_bv.hs_late_payments__finance_system__adf Intermediate Models : Schema naming convention: int Object naming convention: int__[entity]__[optional_transformation] Purpose: Business-specific transformations such as: pivoting aggregation joining funnel creation conformance desensitization e.g. intuitas_domain3_dev.int.int__payments_pivoted_to_orders Dimensions and Facts : Schema naming convention: mart Dimension naming convention: dim__[entity (singular)]__[optional_source_system]__[optional_source_channel] Fact naming convention: fact__[entity (plural)]__[optional_source_system]__[optional_source_channel] e.g. intuitas_domain3_dev.mart.dim__account e.g. intuitas_domain3_dev.mart.fact__payments Denormalized Views (One Big Table) : Schema naming convention: mart Object naming convention: mart__[product]__[optional_source_system]__[optional_source_channel]__[transformation] e.g. intuitas_domain3_dev.mart.mart__account_payments__old_finance_system__adf e.g. intuitas_domain3_dev.mart.mart__account_payments__new_finance_system__adf e.g. intuitas_domain3_dev.mart.mart__account_payments (union of old and new) Reference Data : Schema naming convention: ref Object naming convention: ref__[entity (singular)] e.g. intuitas_domain3_dev.ref.ref__account_code Delta Sharing Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g. intuitas_domain3__finance__reporting__account_payments__payments Azure Data Factory The following are in lower case: Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g. ls_financedb_intuitas_domain3 Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency} Streaming The following are in lower case: Cluster name: {domain_name} cluster {optional:environment} Topic names: {domain_name} {object/entity?} {optional:source_system} {optional:source channel} {optional:environment} Consumer group names: {domain_name} {unique_group_name} {optional:environment} dbt The following are in lower case: Documentation and model metadata Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g. models/silver/ambo_sim__kafka__local/_ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g. models/silver/ambo_sim__kafka__local/_ambo_sim__kafka__local__models.yml Sources Folder: models/sources/[bronze/silver/gold] yml: {schema}__sources.yml (one for each source schema) e.g. bronze__ods__ambo_sim__kafka__local__sources.yml Model folders models/bronze/ models/silver/{source system}/{base/staging/enriched/edw} models/silver/{edw}__{domain_name} models/gold/{domain_name}/{intermediate/marts} sources/bronze/ sources/silver/ sources/gold/ Model Names Bronze objects are likely to be referenced in sources/bronze Silver base object naming: base__{source name} {source channel} {source object name} Silver stage object naming: stg__{source name} {source channel} {source object name}__{ordinal}_{transformation description} Silver enriched object naming: enr__ optional: {source name} optional: {source channel} __{description} // alternatively - just use stg or edw Silver edw object naming: edw {domain name} {description} Gold object name: mart__{domain name} optional: {subdomain name(s)} {description} Example: [[domain/enterprise] _project_name] \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze [domain/enterprise] \u2502 \u251c\u2500\u2500 _bronze.md \u2502 \u251c\u2500\u2500 [domain/enterprise] sources \u251c\u2500\u2500 models/silver \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 edw__[domain_name] \u2502 \u2514\u2500\u2500dim_date \u2502 \u2514\u2500\u2500 reference_data \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_[entity].sql \u2502 \u251c\u2500\u2500 source_system_1 \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__docs.md \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__models.yml \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_source_system_1__object.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 base_source_system_1__deleted_object.sql \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__(new object) \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object_desensitised \u2502 \u2502 \u251c\u2500\u2500 stg \u2502 \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object_01step.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 stg_source_system_1__object_02step_.sqll \u2502 \u2502 \u2514\u2500\u2500 enrichment (optional) \u2502 \u2502 \u251c\u2500\u2500 enr_source_system_1__object.sql \u2502 \u2502 \u251c\u2500\u2500 enr_source_system_1__object_01step.sql \u2502 \u2502 \u2514\u2500\u2500 enr_source_system_1__object_02step.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u251c\u2500\u2500 domain_name e.g. finance \u2502 \u2514\u2500\u2500 intermediate (building blocks for marts) \u2502 | \u251c\u2500\u2500 _int_finance__models.yml \u2502 | \u2514\u2500\u2500 int_payments_pivoted_to_orders.sql \u2502 \u251c\u2500\u2500 marts \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql dbt_project.yml example bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name CI/CD Repository naming All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-domain3-dbt - intuitas-domain3-ingestion-framework - intuitas-domain3-cicd-templates - intuitas-domain3-infrastructure Branch naming All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests Branch lifecycle Simple branch lifecycle: main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master Comprehensive team branch lifecycle: Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes Databricks Asset Bundles Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g. yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com Security Entra Group Names TBC Policies Data Retention Key Retention Frameworks Engineering Security","title":"Naming Conventions"},{"location":"naming_standards_and_conventions/#naming-conventions","text":"Return to home","title":"Naming Conventions"},{"location":"naming_standards_and_conventions/#table-of-contents","text":"Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (ADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model Folders Model Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Group Names Policies Frameworks","title":"Table of Contents"},{"location":"naming_standards_and_conventions/#mesh","text":"","title":"Mesh"},{"location":"naming_standards_and_conventions/#domain-names","text":"All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g. intuitas_domain3","title":"Domain Names"},{"location":"naming_standards_and_conventions/#platform","text":"","title":"Platform"},{"location":"naming_standards_and_conventions/#environment","text":"Environment: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred)","title":"Environment"},{"location":"naming_standards_and_conventions/#vnet","text":"Name: vn-{organisation_name}-{domain_name} e.g. vn-intuitas-domain3","title":"VNET"},{"location":"naming_standards_and_conventions/#resource-groups","text":"Name: rg-{organisation_name}-{domain_name} e.g. rg-intuitas-domain3","title":"Resource Groups"},{"location":"naming_standards_and_conventions/#databricks-workspace","text":"Name: ws-{organisation_name}-{domain_name} e.g. ws-intuitas-domain3","title":"Databricks workspace"},{"location":"naming_standards_and_conventions/#key-vault","text":"Name: kv-{organisation_name}-{domain_name} e.g. kv-intuitas-domain3","title":"Key vault"},{"location":"naming_standards_and_conventions/#secrets","text":"Name: {secret_name}","title":"Secrets"},{"location":"naming_standards_and_conventions/#entra-group-names","text":"Name: eg-{organisation_name}-{domain_name} e.g. eg-intuitas-domain3","title":"Entra Group Names"},{"location":"naming_standards_and_conventions/#azure-data-factory-adf","text":"Name: adf-{organisation_name}-{domain_name} e.g. adf-intuitas-domain3","title":"Azure Data Factory (ADF)"},{"location":"naming_standards_and_conventions/#sql-server","text":"Name: sql-{organisation_name}-{domain_name} e.g. sql-intuitas-domain3","title":"SQL Server"},{"location":"naming_standards_and_conventions/#sql-database","text":"Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g. sqldb-metadata-intuitas-domain3","title":"SQL Database"},{"location":"naming_standards_and_conventions/#storage","text":"","title":"Storage"},{"location":"naming_standards_and_conventions/#lakehouse-storage","text":"Lakehouse storage account name: dl{organisation_name}{domain_name}","title":"Lakehouse storage"},{"location":"naming_standards_and_conventions/#lakehouse-storage-containers","text":"Name: {environment} (dev/test/preprod/prod)","title":"Lakehouse storage containers"},{"location":"naming_standards_and_conventions/#lakehouse-storage-folders","text":"Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} bronze/landing --- tbc --- might be managed by databricks within the catalog storage root silver/base silver/staging silver/enriched silver/edw_rv silver/edw_bv gold/mart","title":"Lakehouse storage folders"},{"location":"naming_standards_and_conventions/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"naming_standards_and_conventions/#generic-blob-files-and-folders","text":"No standard naming conventions for files and folders.","title":"Generic Blob files and folders"},{"location":"naming_standards_and_conventions/#databricks","text":"","title":"Databricks"},{"location":"naming_standards_and_conventions/#workspace-and-cluster-names","text":"Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name}","title":"Workspace and cluster names"},{"location":"naming_standards_and_conventions/#catalog","text":"Catalog name: {domain_name}_{environment} (prod is optional) e.g. intuitas_domain3_dev Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog","title":"Catalog"},{"location":"naming_standards_and_conventions/#externally-mounted-lakehouse-federation-catalog-names","text":"All lower case: {Domain (owner)}_ext__{source_system}{optional:__other_useful_descriptors} e.g. intuitas_domain3_ext__sqlonpremsource Metadata tags: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain","title":"Externally mounted (lakehouse federation) Catalog Names"},{"location":"naming_standards_and_conventions/#schema-and-object-conventions","text":"","title":"Schema and object conventions"},{"location":"naming_standards_and_conventions/#schema-level-external-storage-locations","text":"Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g. abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations.","title":"Schema level external storage locations"},{"location":"naming_standards_and_conventions/#metadata-schemas-and-objects","text":"Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__[optional: function] Naming convention: [function/descriptor] e.g. intuitas_domain3_dev.meta__ingestion.ingestion_control","title":"Metadata Schemas and Objects"},{"location":"naming_standards_and_conventions/#bronze-raw-schemas-and-objects","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. All schemas are may be optionally prefixed with bronze__ Persistent Landing : N/A (see file naming) Operational Data Store (ODS) : Schema naming convention: ods Object naming convention: ods__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.ods.ods__finance_system__adf__accounts Persistent Data Store (PDS) : Schema naming convention: pds Object naming convention: pds__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.pds.pds__finance_system__adf__accounts","title":"Bronze (Raw) Schemas and Objects"},{"location":"naming_standards_and_conventions/#silver-source-centric-filtered-cleaned-augmented","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. All schemas are may be optionally prefixed with silver__ Base Views : Schema naming convention: base__[source_system]__[source_channel] Object naming convention: base__[source_system]__[source_channel]__[object] e.g. intuitas_domain3_dev.base__finance_system__adf.base__finance_system__adf__accounts Staging Objects (Optional) : Schema naming convention: stg__[source_system]__[source_channel] Object naming convention: stg__[source_system]__[source_channel]__[object]__[n]__[transformation] Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g. intuitas_domain3_dev.stg__finance_system__adf.stg__finance_system__adf__accounts__01_renamed_and_typed Enriched Data : Schema naming convention: enr__[source_system]__[source_channel] Object naming convention: enr__[source_system]__[source_channel]__[new_object] e.g. intuitas_domain3_dev.enr__finance_system__adf.enr__finance_system__adf__accounts_join_with_payments Reference Data : Schema naming convention: ref__[source_system]__[source_channel] Object naming convention: ref__[source_system]__[source_channel]__[entity] e.g. intuitas_domain3_dev.ref__finance_system__adf.ref__finance_system__adf__account_codes Raw Vault : Schema naming convention: edw_rv Object naming convention: edw_rv__[vault object] e.g. intuitas_domain3_dev.edw_rv.hs_payments__finance_system__adf","title":"Silver (Source-Centric - Filtered, Cleaned, Augmented)"},{"location":"naming_standards_and_conventions/#gold-business-centric-optionally-source-decomposed","text":"The Gold layer focuses on business-ready datasets, aggregations, and reporting structures. All schemas are may be optionally prefixed with gold__ Business Vault : Schema naming convention: edw_bv Object naming convention: edw_bv__[vault object] e.g. intuitas_domain3_dev.edw_bv.hs_late_payments__finance_system__adf Intermediate Models : Schema naming convention: int Object naming convention: int__[entity]__[optional_transformation] Purpose: Business-specific transformations such as: pivoting aggregation joining funnel creation conformance desensitization e.g. intuitas_domain3_dev.int.int__payments_pivoted_to_orders Dimensions and Facts : Schema naming convention: mart Dimension naming convention: dim__[entity (singular)]__[optional_source_system]__[optional_source_channel] Fact naming convention: fact__[entity (plural)]__[optional_source_system]__[optional_source_channel] e.g. intuitas_domain3_dev.mart.dim__account e.g. intuitas_domain3_dev.mart.fact__payments Denormalized Views (One Big Table) : Schema naming convention: mart Object naming convention: mart__[product]__[optional_source_system]__[optional_source_channel]__[transformation] e.g. intuitas_domain3_dev.mart.mart__account_payments__old_finance_system__adf e.g. intuitas_domain3_dev.mart.mart__account_payments__new_finance_system__adf e.g. intuitas_domain3_dev.mart.mart__account_payments (union of old and new) Reference Data : Schema naming convention: ref Object naming convention: ref__[entity (singular)] e.g. intuitas_domain3_dev.ref.ref__account_code","title":"Gold (Business-Centric - Optionally Source-Decomposed)"},{"location":"naming_standards_and_conventions/#delta-sharing","text":"Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g. intuitas_domain3__finance__reporting__account_payments__payments","title":"Delta Sharing"},{"location":"naming_standards_and_conventions/#azure-data-factory","text":"The following are in lower case: Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g. ls_financedb_intuitas_domain3 Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency}","title":"Azure Data Factory"},{"location":"naming_standards_and_conventions/#streaming","text":"The following are in lower case: Cluster name: {domain_name} cluster {optional:environment} Topic names: {domain_name} {object/entity?} {optional:source_system} {optional:source channel} {optional:environment} Consumer group names: {domain_name} {unique_group_name} {optional:environment}","title":"Streaming"},{"location":"naming_standards_and_conventions/#dbt","text":"The following are in lower case:","title":"dbt"},{"location":"naming_standards_and_conventions/#documentation-and-model-metadata","text":"Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g. models/silver/ambo_sim__kafka__local/_ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g. models/silver/ambo_sim__kafka__local/_ambo_sim__kafka__local__models.yml","title":"Documentation and model metadata"},{"location":"naming_standards_and_conventions/#sources","text":"Folder: models/sources/[bronze/silver/gold] yml: {schema}__sources.yml (one for each source schema) e.g. bronze__ods__ambo_sim__kafka__local__sources.yml","title":"Sources"},{"location":"naming_standards_and_conventions/#model-folders","text":"models/bronze/ models/silver/{source system}/{base/staging/enriched/edw} models/silver/{edw}__{domain_name} models/gold/{domain_name}/{intermediate/marts} sources/bronze/ sources/silver/ sources/gold/","title":"Model folders"},{"location":"naming_standards_and_conventions/#model-names","text":"Bronze objects are likely to be referenced in sources/bronze Silver base object naming: base__{source name} {source channel} {source object name} Silver stage object naming: stg__{source name} {source channel} {source object name}__{ordinal}_{transformation description} Silver enriched object naming: enr__ optional: {source name} optional: {source channel} __{description} // alternatively - just use stg or edw Silver edw object naming: edw {domain name} {description} Gold object name: mart__{domain name} optional: {subdomain name(s)} {description}","title":"Model Names"},{"location":"naming_standards_and_conventions/#example","text":"[[domain/enterprise] _project_name] \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze [domain/enterprise] \u2502 \u251c\u2500\u2500 _bronze.md \u2502 \u251c\u2500\u2500 [domain/enterprise] sources \u251c\u2500\u2500 models/silver \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 edw__[domain_name] \u2502 \u2514\u2500\u2500dim_date \u2502 \u2514\u2500\u2500 reference_data \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_[entity].sql \u2502 \u251c\u2500\u2500 source_system_1 \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__docs.md \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__models.yml \u2502 \u2502 \u251c\u2500\u2500 base \u2502 \u2502 \u2502 \u251c\u2500\u2500 base_source_system_1__object.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 base_source_system_1__deleted_object.sql \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__(new object) \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object_desensitised \u2502 \u2502 \u251c\u2500\u2500 stg \u2502 \u2502 \u2502 \u251c\u2500\u2500 stg_source_system_1__object_01step.sql \u2502 \u2502 \u2502 \u2514\u2500\u2500 stg_source_system_1__object_02step_.sqll \u2502 \u2502 \u2514\u2500\u2500 enrichment (optional) \u2502 \u2502 \u251c\u2500\u2500 enr_source_system_1__object.sql \u2502 \u2502 \u251c\u2500\u2500 enr_source_system_1__object_01step.sql \u2502 \u2502 \u2514\u2500\u2500 enr_source_system_1__object_02step.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2502 \u251c\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u251c\u2500\u2500 domain_name e.g. finance \u2502 \u2514\u2500\u2500 intermediate (building blocks for marts) \u2502 | \u251c\u2500\u2500 _int_finance__models.yml \u2502 | \u2514\u2500\u2500 int_payments_pivoted_to_orders.sql \u2502 \u251c\u2500\u2500 marts \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql","title":"Example:"},{"location":"naming_standards_and_conventions/#dbt_projectyml","text":"example bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name","title":"dbt_project.yml"},{"location":"naming_standards_and_conventions/#cicd","text":"","title":"CI/CD"},{"location":"naming_standards_and_conventions/#repository-naming","text":"All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-domain3-dbt - intuitas-domain3-ingestion-framework - intuitas-domain3-cicd-templates - intuitas-domain3-infrastructure","title":"Repository naming"},{"location":"naming_standards_and_conventions/#branch-naming","text":"All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests","title":"Branch naming"},{"location":"naming_standards_and_conventions/#branch-lifecycle","text":"","title":"Branch lifecycle"},{"location":"naming_standards_and_conventions/#simple-branch-lifecycle","text":"main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master","title":"Simple branch lifecycle:"},{"location":"naming_standards_and_conventions/#comprehensive-team-branch-lifecycle","text":"Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes","title":"Comprehensive team branch lifecycle:"},{"location":"naming_standards_and_conventions/#databricks-asset-bundles","text":"Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name} e.g. if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g. yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com","title":"Databricks Asset Bundles"},{"location":"naming_standards_and_conventions/#security","text":"","title":"Security"},{"location":"naming_standards_and_conventions/#entra-group-names_1","text":"TBC","title":"Entra Group Names"},{"location":"naming_standards_and_conventions/#policies","text":"Data Retention Key Retention","title":"Policies"},{"location":"naming_standards_and_conventions/#frameworks","text":"Engineering Security","title":"Frameworks"}]}