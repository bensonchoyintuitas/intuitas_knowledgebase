{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"We help organisations cut through the complexity, turning data and AI into results. Enterprise Data Intelligence Blueprint This blueprint brings together resources that capture Intuitas\u2019 approach to designing and delivering Data, AI, and Governance solutions. It is a continually evolving resource, offering insights into strategic, enterprise, and solution-level practices\u2014distilled from our R&D, common questions, and real-world experience. The ideas and patterns reflect Intuitas\u2019 design philosophy: grounded in large, multi-domain enterprise deployments, yet adaptable to organisations of any size or type evolving, opinionated, and open to challenge demonstrable in working environments on request We share this blueprint with our customer and partner community to promote our vision of 'good design', help avoid pitfalls, and speed up delivery. We encourage you to explore, share and build on this information, with proper attribution to Intuitas and consideration as set out in our license and disclaimer . \u201cTo build a home, you need a plan \u2014 but for it to thrive, you need a town plan.\" Get help Contact us at office@intuitas.com to: Find out more, or provide feedback. Access our demonstration environment or any of the tools and technologies presented Get further help in: strategy, architecture, planning, training, implementation and governance. Table of Contents Overview Design Principles Getting Started Licensing and disclaimer Level 0: Enterprise Context Organisational & Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures Level 1: Enterprise Architecture Key Concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Principles Semantic and Data Lineage Metadata Objects and Elements Consolidation and Synchronisation Governance Metadata Enterprise Security Enterprise Data Governance Enterprise Billing Level 2: Domain Architecture Business Architecture Business Processes Business Glossary Business Metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data & Information Models Domain Glossary Domain Data & Warehouse Models Data Architecture Data Layers and Stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data Governance Data lifecycle and asset management Data access management Data Quality Data Understandability Privacy Preservation Audit Standards and conventions Standards & Conventions Overview This blueprint is comprehensive and intentionally opinionated, providing practical patterns for designing and delivering enterprise-scale Data, AI, and Governance solutions. It is continuously refined through ongoing R&D, customer engagements, lessons learned, and evaluations of the evolving product landscape. It follows an iterative approach where each level builds on the previous one, while remaining flexible to be revisited and refined as capabilities mature. Organisations may begin at any level depending on their maturity, but all levels should be considered and validated for applicability: Level 0: Context Setting - Define organisational objectives, domain structures, and strategic goals Level 1: Enterprise Architecture - Apply enterprise-wide patterns and reference topologies Level 2: Domain Solutions - Implement domain-specific solutions using enterprise patterns Standards & Conventions - Apply consistent naming and conventions throughout See the Copyright, Licensing and Disclaimer information Design Principles \u201cThe alternative to good design is always bad design. There is no such thing as no design.\u201d - Adam Judge Our approach is guided by nine foundational principles that ensure transparency in decision-making, effective trade-off evaluation, and strategic alignment. Readers should consider the priority and implication of these principles alongside any existing principles applicable within their organisation. # Principle Description 1 Think big, start small Balance rapid delivery with strategic positioning. Deliver value iteratively. Ensure investments deliver sustained benefits without creating technical debt. 2 Empowered Autonomy Enable domain experts to manage data independently while leveraging shared, scalable foundations. 3 Disciplined Core, Flexible Edge Federated governance model that ensures policy consistency while enabling rapid, domain-specific delivery. 4 Actionable Data Real-time, comprehensive data extraction (structured & unstructured) with platforms ready for immediate insights and AI/ML. 5 Make It Easy to Do the Right Thing Provide automation and platforms that guide users toward secure, policy-aligned practices effortlessly. 6 Cost Transparency and Efficiency Transparent cost models with pay-for-value usage while promoting reuse and removing sharing barriers. 7 Adaptability for Growth Platforms seamlessly adapt to evolving business needs, data growth, and diverse workloads. 8 Interoperability and Inclusion Smooth integration across cloud, on-premises, and diverse technology stacks (bring-your-own). 9 Flexibility Through Standards Technology-agnostic, standards-based components that maintain flexibility and prevent vendor lock-in. Getting Started Recommended starting point by role: Role Recommended Path Focus Areas Leaders & Architects Levels 0-1 Strategic alignment, enterprise patterns, governance frameworks Domain & Technical Teams Level 2 Practical implementation within established enterprise frameworks Licensing and disclaimer Copyright: This knowledgebase and associated content are the original works of \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. License: Free use, reproduction, and adaptation permitted with prior consent and appropriate attribution to Intuitas PTY LTD. Referenced third-party content is subject to the copyright terms of their respective owners. Disclaimer: Content is provided for general information only The information provided is general in nature and may not cover all scenarios or workloads. It does not constitute professional advice and should not be relied on as a substitute for advice tailored to your circumstances. The information provided reflects the product landscape and functionality available in general release at the time of publishing. While every effort is made to maintain accuracy and update information as features evolve, the timeliness of these updates cannot be guaranteed. No liability is accepted by Intuitas PTY LTD for errors or omissions. Readers are encouraged to independently validate all claims and undertake benchmark against their own use cases and projected workloads. \ud83d\udce7 office@intuitas.com","title":"Home"},{"location":"#enterprise-data-intelligence-blueprint","text":"This blueprint brings together resources that capture Intuitas\u2019 approach to designing and delivering Data, AI, and Governance solutions. It is a continually evolving resource, offering insights into strategic, enterprise, and solution-level practices\u2014distilled from our R&D, common questions, and real-world experience. The ideas and patterns reflect Intuitas\u2019 design philosophy: grounded in large, multi-domain enterprise deployments, yet adaptable to organisations of any size or type evolving, opinionated, and open to challenge demonstrable in working environments on request We share this blueprint with our customer and partner community to promote our vision of 'good design', help avoid pitfalls, and speed up delivery. We encourage you to explore, share and build on this information, with proper attribution to Intuitas and consideration as set out in our license and disclaimer .","title":"Enterprise Data Intelligence Blueprint"},{"location":"#to-build-a-home-you-need-a-plan-but-for-it-to-thrive-you-need-a-town-plan","text":"","title":"\u201cTo build a home, you need a plan \u2014 but for it to thrive, you need a town plan.\""},{"location":"#get-help","text":"Contact us at office@intuitas.com to: Find out more, or provide feedback. Access our demonstration environment or any of the tools and technologies presented Get further help in: strategy, architecture, planning, training, implementation and governance.","title":"Get help"},{"location":"#table-of-contents","text":"Overview Design Principles Getting Started Licensing and disclaimer","title":"Table of Contents"},{"location":"#level-0-enterprise-context","text":"Organisational & Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures","title":"Level 0: Enterprise Context"},{"location":"#level-1-enterprise-architecture","text":"Key Concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Principles Semantic and Data Lineage Metadata Objects and Elements Consolidation and Synchronisation Governance Metadata Enterprise Security Enterprise Data Governance Enterprise Billing","title":"Level 1: Enterprise Architecture"},{"location":"#level-2-domain-architecture","text":"Business Architecture Business Processes Business Glossary Business Metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data & Information Models Domain Glossary Domain Data & Warehouse Models Data Architecture Data Layers and Stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data Governance Data lifecycle and asset management Data access management Data Quality Data Understandability Privacy Preservation Audit","title":"Level 2: Domain Architecture"},{"location":"#standards-and-conventions","text":"Standards & Conventions","title":"Standards and conventions"},{"location":"#overview","text":"This blueprint is comprehensive and intentionally opinionated, providing practical patterns for designing and delivering enterprise-scale Data, AI, and Governance solutions. It is continuously refined through ongoing R&D, customer engagements, lessons learned, and evaluations of the evolving product landscape. It follows an iterative approach where each level builds on the previous one, while remaining flexible to be revisited and refined as capabilities mature. Organisations may begin at any level depending on their maturity, but all levels should be considered and validated for applicability: Level 0: Context Setting - Define organisational objectives, domain structures, and strategic goals Level 1: Enterprise Architecture - Apply enterprise-wide patterns and reference topologies Level 2: Domain Solutions - Implement domain-specific solutions using enterprise patterns Standards & Conventions - Apply consistent naming and conventions throughout See the Copyright, Licensing and Disclaimer information","title":"Overview"},{"location":"#design-principles","text":"","title":"Design Principles"},{"location":"#the-alternative-to-good-design-is-always-bad-design-there-is-no-such-thing-as-no-design-adam-judge","text":"Our approach is guided by nine foundational principles that ensure transparency in decision-making, effective trade-off evaluation, and strategic alignment. Readers should consider the priority and implication of these principles alongside any existing principles applicable within their organisation. # Principle Description 1 Think big, start small Balance rapid delivery with strategic positioning. Deliver value iteratively. Ensure investments deliver sustained benefits without creating technical debt. 2 Empowered Autonomy Enable domain experts to manage data independently while leveraging shared, scalable foundations. 3 Disciplined Core, Flexible Edge Federated governance model that ensures policy consistency while enabling rapid, domain-specific delivery. 4 Actionable Data Real-time, comprehensive data extraction (structured & unstructured) with platforms ready for immediate insights and AI/ML. 5 Make It Easy to Do the Right Thing Provide automation and platforms that guide users toward secure, policy-aligned practices effortlessly. 6 Cost Transparency and Efficiency Transparent cost models with pay-for-value usage while promoting reuse and removing sharing barriers. 7 Adaptability for Growth Platforms seamlessly adapt to evolving business needs, data growth, and diverse workloads. 8 Interoperability and Inclusion Smooth integration across cloud, on-premises, and diverse technology stacks (bring-your-own). 9 Flexibility Through Standards Technology-agnostic, standards-based components that maintain flexibility and prevent vendor lock-in.","title":"\u201cThe alternative to good design is always bad design. There is no such thing as no design.\u201d - Adam Judge"},{"location":"#getting-started","text":"Recommended starting point by role: Role Recommended Path Focus Areas Leaders & Architects Levels 0-1 Strategic alignment, enterprise patterns, governance frameworks Domain & Technical Teams Level 2 Practical implementation within established enterprise frameworks","title":"Getting Started"},{"location":"#licensing-and-disclaimer","text":"Copyright: This knowledgebase and associated content are the original works of \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. License: Free use, reproduction, and adaptation permitted with prior consent and appropriate attribution to Intuitas PTY LTD. Referenced third-party content is subject to the copyright terms of their respective owners. Disclaimer: Content is provided for general information only The information provided is general in nature and may not cover all scenarios or workloads. It does not constitute professional advice and should not be relied on as a substitute for advice tailored to your circumstances. The information provided reflects the product landscape and functionality available in general release at the time of publishing. While every effort is made to maintain accuracy and update information as features evolve, the timeliness of these updates cannot be guaranteed. No liability is accepted by Intuitas PTY LTD for errors or omissions. Readers are encouraged to independently validate all claims and undertake benchmark against their own use cases and projected workloads.","title":"Licensing and disclaimer"},{"location":"#_1","text":"\ud83d\udce7 office@intuitas.com","title":""},{"location":"about/","text":"Redirecting to Intuitas Redirecting to Intuitas ... window.location.href = \"https://www.intuitas.com\";","title":"About"},{"location":"level_0/","text":"Level 0 - Enterprise-level context Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made. Why It Matters Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals. Table of Contents Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures Org + Domain Definition Understanding the organisation\u2019s structure and domains is key to setting governance, environment, and metadata boundaries. While industry patterns and reference architectures can guide, each organisation is unique. See Domain-Centric Design for more. Recommended artefacts: Description of the organisational boundary, including any external organisations in scope. Description of domains and subdomains that impact the data governance and management model. Illustrative example of domains for a Healthcare organisation Strategies and Objectives Investing in data initiatives without a clear strategic context risks misalignment and wasted effort. A well-defined strategic context for data, technology, and governance ensures initiatives align with organisational goals, set the right priorities, have a clear scope, support effective governance, and build strong investment and benefits cases. Recommended artefacts Assessment of relevant business, technology, and data/AI strategies, plans, and initiatives. Key Systems and Data Assets Identifying and profiling key systems and data assets in the context of strategic benefits and use cases provides a strong foundation for effective planning, design, and risk management. Recommended artefacts Description of the organisation's relevant systems and data sets (e.g. CMDB, Information Asset Register), including associated governance arrangements and technical characteristics. Team Capabilities and Roles Clarity on the responsibilities and capabilities of key teams is essential to executing the data strategy, maintaining accountability, ensuring all support functions are adequately fulfilled, and costs remain predictable. Recommended artefacts RACI matrix for associated teams/parties with consideration of functions including but not limted to: Data lifecycle \u2013 creation, management, governance (quality, access), security, and consumption Data enablement \u2013 engineering, integration, analysis, reporting, and application development Data foundations \u2013 information architecture, infrastructure, and platform provisioning and management Operational support \u2013 DataOps, monitoring, and ongoing maintenance Current and target operating and service management model Maturity and skill assessment of key teams/parties relative to role definition Strategy for using external partnerships and vendors to address capability gaps Governance Structures Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. These may already exist in some organisations, while in others they need to be developed. Recommended artefacts Description of governance frameworks, policies, standards, processes, and bodies relevant to the scope of concern. In some cases - proposals and terms of reference for new governance structures. Funding and costing structures How funding and costs are allocated directly shape the internal \u201cmarketplace\u201d for data and analytics. These structures determine whether teams collaborate or compete, influence speed of delivery and standards alignment, and define the organisation\u2019s capacity to scale and innovate. The architecture reflects this through: system-wide, domain, and role-level accountabilities. policies and guardrails to prevent bill-shock observability and optimisation See: - Enterprise Billing Solutions - Observability Solutions Recommended artefacts Billing structures \u2013 including how cost centres map to reports, delegations, and observability (important for transparency and accountability) Cost allocation and chargeback models \u2013 critical for encouraging reuse, managing shared services, and clarifying ownership Budgeting and forecasting \u2013 ensures sustainable funding for data platforms, products, and capabilities Cost optimisation and monitoring \u2013 helps manage platform efficiency and avoid waste, and how the accountabilities are distributed for their control Next Steps Once the enterprise-level context is established, proceed to Level 1 - Domain Architecture to explore domain-centric design principles and architectural patterns that support the organisation's strategic objectives.","title":"Level 0"},{"location":"level_0/#level-0-enterprise-level-context","text":"Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#why-it-matters","text":"Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals.","title":"Why It Matters"},{"location":"level_0/#table-of-contents","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures","title":"Table of Contents"},{"location":"level_0/#org-domain-definition","text":"Understanding the organisation\u2019s structure and domains is key to setting governance, environment, and metadata boundaries. While industry patterns and reference architectures can guide, each organisation is unique. See Domain-Centric Design for more. Recommended artefacts: Description of the organisational boundary, including any external organisations in scope. Description of domains and subdomains that impact the data governance and management model. Illustrative example of domains for a Healthcare organisation","title":"Org + Domain Definition"},{"location":"level_0/#strategies-and-objectives","text":"Investing in data initiatives without a clear strategic context risks misalignment and wasted effort. A well-defined strategic context for data, technology, and governance ensures initiatives align with organisational goals, set the right priorities, have a clear scope, support effective governance, and build strong investment and benefits cases. Recommended artefacts Assessment of relevant business, technology, and data/AI strategies, plans, and initiatives.","title":"Strategies and Objectives"},{"location":"level_0/#key-systems-and-data-assets","text":"Identifying and profiling key systems and data assets in the context of strategic benefits and use cases provides a strong foundation for effective planning, design, and risk management. Recommended artefacts Description of the organisation's relevant systems and data sets (e.g. CMDB, Information Asset Register), including associated governance arrangements and technical characteristics.","title":"Key Systems and Data Assets"},{"location":"level_0/#team-capabilities-and-roles","text":"Clarity on the responsibilities and capabilities of key teams is essential to executing the data strategy, maintaining accountability, ensuring all support functions are adequately fulfilled, and costs remain predictable. Recommended artefacts RACI matrix for associated teams/parties with consideration of functions including but not limted to: Data lifecycle \u2013 creation, management, governance (quality, access), security, and consumption Data enablement \u2013 engineering, integration, analysis, reporting, and application development Data foundations \u2013 information architecture, infrastructure, and platform provisioning and management Operational support \u2013 DataOps, monitoring, and ongoing maintenance Current and target operating and service management model Maturity and skill assessment of key teams/parties relative to role definition Strategy for using external partnerships and vendors to address capability gaps","title":"Team Capabilities and Roles"},{"location":"level_0/#governance-structures","text":"Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. These may already exist in some organisations, while in others they need to be developed. Recommended artefacts Description of governance frameworks, policies, standards, processes, and bodies relevant to the scope of concern. In some cases - proposals and terms of reference for new governance structures.","title":"Governance Structures"},{"location":"level_0/#funding-and-costing-structures","text":"How funding and costs are allocated directly shape the internal \u201cmarketplace\u201d for data and analytics. These structures determine whether teams collaborate or compete, influence speed of delivery and standards alignment, and define the organisation\u2019s capacity to scale and innovate. The architecture reflects this through: system-wide, domain, and role-level accountabilities. policies and guardrails to prevent bill-shock observability and optimisation See: - Enterprise Billing Solutions - Observability Solutions Recommended artefacts Billing structures \u2013 including how cost centres map to reports, delegations, and observability (important for transparency and accountability) Cost allocation and chargeback models \u2013 critical for encouraging reuse, managing shared services, and clarifying ownership Budgeting and forecasting \u2013 ensures sustainable funding for data platforms, products, and capabilities Cost optimisation and monitoring \u2013 helps manage platform efficiency and avoid waste, and how the accountabilities are distributed for their control","title":"Funding and costing structures"},{"location":"level_0/#next-steps","text":"Once the enterprise-level context is established, proceed to Level 1 - Domain Architecture to explore domain-centric design principles and architectural patterns that support the organisation's strategic objectives.","title":"Next Steps"},{"location":"level_1/","text":"Level 1 - Enterprise-level architecture Return to home This section builds on the Level 0 - Enterprise and Strategic Context to describe enterprise-wide and cross-domain data and data platform architecture concepts. Why It Matters Establishing a clear enterprise-wide, cross-domain design\u2014the \u201ctown plan\u201d for data\u2014ensures that platform architecture, capabilities, and investments are purposefully coordinated across the organisation. This reduces duplication, enables shared infrastructure, and is critical to building a scalable, interoperable, and sustainable data platform aligned with the organisation\u2019s long-term vision. Table of Contents Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Metadata Architecture Principles Semantic and Data lineage Metadata Objects and Elements Metadata Consolidation and Synchronisation Data Architecture and Governance Metadata Semantic modelling, mastering and lineage Unified metadata repository Analytics engineering metadata Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Databricks features for usage tracking Metadata and tags Cluster policies System tables Usage reports Domain / Workspace Administrator Role Tagging convention Key concepts The following key concepts are used throughout this knowledgebase. Domain-Centric Design Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. See Domain driven design Illustative example domains for a Healthcare organisation Domain Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, costing and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Domain can exist at different levels of granularity and their boundaries may not be obvious. They are not necessarily a reflection of the organisational chart. Subdomain A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Example of authoring domains using Intuitas' Domain builder tool Data Mesh A data mesh is a decentralized approach to data management that shifts ownership and accountability to domain teams, enabling them to treat their data as a product. Each domain is responsible for creating, maintaining, and sharing high-quality, discoverable, and interoperable data products with other domains. The data mesh approach emphasizes domain autonomy, self-serve infrastructure, interoperability, and federated governance. It is not a one-size-fits-all model; its suitability depends on an organisation\u2019s context, culture, and capabilities, and its adoption will vary in maturity and success across organisations. See Data Mesh: Principles Domain Topology A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. See Data Mesh: Topologies and domain granularity Data Fabric A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles. Data Mesh vs Fabric A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric. Reference topologies Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Enterprise Domain Reference Topologies Source: Data Mesh: Topologies and domain granularity, Strengholt P., 2022 Hybrid federated mesh topology This blueprint depicts a Hybrid Federated Mesh Topology, increasingly common in large enterprises and mature engineering practices. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Example of hybrid federated mesh topology: Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organisation. Distributed Local engineering Maintaining a local bronze/raw layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access 'gold' data from other domains and, where appropriate, 'silver' or 'bronze', facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability. Enterprise Data Platform Reference Architecture Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example Platform and Pipeline Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture An enterprise logical data warehouse retains the core properties of a traditional data warehouse\u2014integrated, consistent, and analytics-ready data\u2014while operating in a distributed, domain-oriented model. Logical Data Warehouse topology is characterised by: Federated governance \u2013 shared policies and standards, but distributed custodianship, applied across the domain topology. Unified access \u2013 a common entry point for querying and consuming data regardless of its physical location Enterprise metadata \u2013 consistent definitions, lineage, and discovery across all domains via shared catalog. This approach combines the scalability and agility of decentralised ownership with the trust and coherence of an enterprise-wide data platform. Example logical data warehouse topology Enterprise Information and Data Architecture Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models Enterprise Metadata Architecture Metadata is an umbrella term encompassing various types of data that play critical roles across multiple domains, supporting the description, governance, and operational use of data and technology assets. It can be actively curated or generated as a byproduct of processes and systems. Metadata Architecture Principles The following principles reflect our design philosophy to ensure sustainable and effective metadata capability Principle Description Accessible Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic Automate collection and updates to keep metadata fresh and reduce manual work. Contextual Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated Metadata exists in an ecosystem across tools to support diverse workflows. Consistency Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure Protect metadata as it may contain sensitive details. Accountability Clearly define roles for ownership and stewardship. Agnostic Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability. Semantic and Data lineage Semantic lineage and data lineage are critical concepts in a modern data intelligence capability to ensure clarity, trust, and traceability of data \u2014 from its business meaning to its technical origins and transformations: Semantic Lineage maps business terms, definitions, and relationships across the data ecosystem, showing how concepts are represented and transformed across systems and domains. Data Lineage tracks the technical flow of data from source to destination, including all transformations, to provide visibility, support data quality, and meet compliance and governance needs. Together, they give a complete view of business and technical data flows, enabling stronger governance and management of data assets. Because they are often difficult to align and keep in sync, a unified approach , as provided in the reference architecture, is critical. Data and Semantic Lineage Metadata Objects and Elements Metadata exists in various types, formats, and purposes, each essential for enabling: Data and Information Governance & Architecture \u2013 including semantic and data lineage, as well as privacy, access, and security - controls Data Engineering and Analytics Development Business Interpretation and Understanding \u2013 supporting the context and meaning of information and analytics Data Quality and Integrity Technical and Platform Administration Integration, Data Sharing, and Interoperability The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. Metadata logical architecture Metadata Consolidation and Synchronisation Metadata consolidation and synchronisation are critical for achieving a consistent, unified view of data assets, enabling reliable lineage, governance, and context across the data ecosystem. This approach: Eliminates Silos: Aggregates metadata from diverse tools (e.g. dbt, Unity Catalog, PowerBI, MLflow) into a central catalogue like DataHub, ensuring all stakeholders access the same contextual information. Improves Trust and Traceability: Enables end-to-end lineage and visibility, helping users understand where data comes from, how it is transformed, and how it is used across platforms. Enables Automation and Governance: Supports data quality, access control, and policy enforcement through unified metadata APIs and standardized governance models. The diagram below illustrates metadata objects and elements that are created and managed across diverse tools and contexts\u2014each serving a distinct role in the broader data and technology ecosystem. Metadata flow Data Architecture and Governance Metadata Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability. Semantic modelling, mastering and lineage Snappy serves as a 'business-first' enterprise domain, model, standards and glossary authoring and mastering tool, and acts as the key driver of semantic lineage linking between true on-the-ground semantics, reference models and physical as-built metadata in Datahub. Modelling Domains, Glossaries and Models in Intuitas' snappy tool Unified metadata repository DataHub serves as a consolidation layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, domains, and projects providing a comprehensive view of data flows and relationships throughout the ecosystem. Example: Datahub Lineage Example: Enterprise-wide summary of assets Example: Browse by business domain and filters Example: Metadata search by term Example: User-driven mapping of glossary terms to measures Example: User-driven tagging of PII Analytics engineering metadata dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to standards and conventions . Databricks Unity Catalog Metastore Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Unity catalog is able to store table, column and lineage metadata inherit schema metadata from dbt detect definitions using AI where they are missing Example: Databricks AI-driven semantic detection Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout. Enterprise Security Recommended artefacts: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs Enterprise Data Governance Recommended artefacts: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management Audit Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended artefacts: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions. Example questions and associated queries As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Enterprise Billing Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform must to align. See: - Funding and costing structures - Observability Solutions Recommendations here align with the following Domain topology: Administration and Billing Scopes Databricks features for usage tracking Metadata and tags In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags Cluster policies Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions. System tables System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools Dynamic Billing Solution Usage reports Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request) Additional useful references: - Top 10 Queries to use with System Tables - Unlocking Cost Optimization Insights with Databricks System Tables Domain / Workspace Administrator Role Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts Tagging convention All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id Typical observability requirements by role As an Enterprise Admin 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Level 1"},{"location":"level_1/#level-1-enterprise-level-architecture","text":"Return to home This section builds on the Level 0 - Enterprise and Strategic Context to describe enterprise-wide and cross-domain data and data platform architecture concepts.","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#why-it-matters","text":"Establishing a clear enterprise-wide, cross-domain design\u2014the \u201ctown plan\u201d for data\u2014ensures that platform architecture, capabilities, and investments are purposefully coordinated across the organisation. This reduces duplication, enables shared infrastructure, and is critical to building a scalable, interoperable, and sustainable data platform aligned with the organisation\u2019s long-term vision.","title":"Why It Matters"},{"location":"level_1/#table-of-contents","text":"Key concepts Domain Subdomain Domain-Centric Design Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Metadata Architecture Principles Semantic and Data lineage Metadata Objects and Elements Metadata Consolidation and Synchronisation Data Architecture and Governance Metadata Semantic modelling, mastering and lineage Unified metadata repository Analytics engineering metadata Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Databricks features for usage tracking Metadata and tags Cluster policies System tables Usage reports Domain / Workspace Administrator Role Tagging convention","title":"Table of Contents"},{"location":"level_1/#key-concepts","text":"The following key concepts are used throughout this knowledgebase.","title":"Key concepts"},{"location":"level_1/#domain-centric-design","text":"Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. See Domain driven design Illustative example domains for a Healthcare organisation","title":"Domain-Centric Design"},{"location":"level_1/#domain","text":"Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, costing and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Domain can exist at different levels of granularity and their boundaries may not be obvious. They are not necessarily a reflection of the organisational chart.","title":"Domain"},{"location":"level_1/#subdomain","text":"A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Example of authoring domains using Intuitas' Domain builder tool","title":"Subdomain"},{"location":"level_1/#data-mesh","text":"A data mesh is a decentralized approach to data management that shifts ownership and accountability to domain teams, enabling them to treat their data as a product. Each domain is responsible for creating, maintaining, and sharing high-quality, discoverable, and interoperable data products with other domains. The data mesh approach emphasizes domain autonomy, self-serve infrastructure, interoperability, and federated governance. It is not a one-size-fits-all model; its suitability depends on an organisation\u2019s context, culture, and capabilities, and its adoption will vary in maturity and success across organisations. See Data Mesh: Principles","title":"Data Mesh"},{"location":"level_1/#domain-topology","text":"A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. See Data Mesh: Topologies and domain granularity","title":"Domain Topology"},{"location":"level_1/#data-fabric","text":"A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles.","title":"Data Fabric"},{"location":"level_1/#data-mesh-vs-fabric","text":"A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric.","title":"Data Mesh vs Fabric"},{"location":"level_1/#reference-topologies","text":"Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Enterprise Domain Reference Topologies Source: Data Mesh: Topologies and domain granularity, Strengholt P., 2022","title":"Reference topologies"},{"location":"level_1/#hybrid-federated-mesh-topology","text":"This blueprint depicts a Hybrid Federated Mesh Topology, increasingly common in large enterprises and mature engineering practices. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Example of hybrid federated mesh topology: Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organisation. Distributed Local engineering Maintaining a local bronze/raw layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access 'gold' data from other domains and, where appropriate, 'silver' or 'bronze', facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability.","title":"Hybrid federated mesh topology"},{"location":"level_1/#enterprise-data-platform-reference-architecture","text":"Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example Platform and Pipeline Reference Architecture","title":"Enterprise Data Platform Reference Architecture"},{"location":"level_1/#enterprise-logical-data-warehouse-reference-architecture","text":"An enterprise logical data warehouse retains the core properties of a traditional data warehouse\u2014integrated, consistent, and analytics-ready data\u2014while operating in a distributed, domain-oriented model. Logical Data Warehouse topology is characterised by: Federated governance \u2013 shared policies and standards, but distributed custodianship, applied across the domain topology. Unified access \u2013 a common entry point for querying and consuming data regardless of its physical location Enterprise metadata \u2013 consistent definitions, lineage, and discovery across all domains via shared catalog. This approach combines the scalability and agility of decentralised ownership with the trust and coherence of an enterprise-wide data platform. Example logical data warehouse topology","title":"Enterprise (Logical) Data Warehouse Reference Architecture"},{"location":"level_1/#enterprise-information-and-data-architecture","text":"Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models","title":"Enterprise Information and Data Architecture"},{"location":"level_1/#enterprise-metadata-architecture","text":"Metadata is an umbrella term encompassing various types of data that play critical roles across multiple domains, supporting the description, governance, and operational use of data and technology assets. It can be actively curated or generated as a byproduct of processes and systems.","title":"Enterprise Metadata Architecture"},{"location":"level_1/#metadata-architecture-principles","text":"The following principles reflect our design philosophy to ensure sustainable and effective metadata capability Principle Description Accessible Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic Automate collection and updates to keep metadata fresh and reduce manual work. Contextual Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated Metadata exists in an ecosystem across tools to support diverse workflows. Consistency Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure Protect metadata as it may contain sensitive details. Accountability Clearly define roles for ownership and stewardship. Agnostic Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability.","title":"Metadata Architecture Principles"},{"location":"level_1/#semantic-and-data-lineage","text":"Semantic lineage and data lineage are critical concepts in a modern data intelligence capability to ensure clarity, trust, and traceability of data \u2014 from its business meaning to its technical origins and transformations: Semantic Lineage maps business terms, definitions, and relationships across the data ecosystem, showing how concepts are represented and transformed across systems and domains. Data Lineage tracks the technical flow of data from source to destination, including all transformations, to provide visibility, support data quality, and meet compliance and governance needs. Together, they give a complete view of business and technical data flows, enabling stronger governance and management of data assets. Because they are often difficult to align and keep in sync, a unified approach , as provided in the reference architecture, is critical. Data and Semantic Lineage","title":"Semantic and Data lineage"},{"location":"level_1/#metadata-objects-and-elements","text":"Metadata exists in various types, formats, and purposes, each essential for enabling: Data and Information Governance & Architecture \u2013 including semantic and data lineage, as well as privacy, access, and security - controls Data Engineering and Analytics Development Business Interpretation and Understanding \u2013 supporting the context and meaning of information and analytics Data Quality and Integrity Technical and Platform Administration Integration, Data Sharing, and Interoperability The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. Metadata logical architecture","title":"Metadata Objects and Elements"},{"location":"level_1/#metadata-consolidation-and-synchronisation","text":"Metadata consolidation and synchronisation are critical for achieving a consistent, unified view of data assets, enabling reliable lineage, governance, and context across the data ecosystem. This approach: Eliminates Silos: Aggregates metadata from diverse tools (e.g. dbt, Unity Catalog, PowerBI, MLflow) into a central catalogue like DataHub, ensuring all stakeholders access the same contextual information. Improves Trust and Traceability: Enables end-to-end lineage and visibility, helping users understand where data comes from, how it is transformed, and how it is used across platforms. Enables Automation and Governance: Supports data quality, access control, and policy enforcement through unified metadata APIs and standardized governance models. The diagram below illustrates metadata objects and elements that are created and managed across diverse tools and contexts\u2014each serving a distinct role in the broader data and technology ecosystem. Metadata flow","title":"Metadata Consolidation and Synchronisation"},{"location":"level_1/#data-architecture-and-governance-metadata","text":"Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability.","title":"Data Architecture and Governance Metadata"},{"location":"level_1/#semantic-modelling-mastering-and-lineage","text":"Snappy serves as a 'business-first' enterprise domain, model, standards and glossary authoring and mastering tool, and acts as the key driver of semantic lineage linking between true on-the-ground semantics, reference models and physical as-built metadata in Datahub. Modelling Domains, Glossaries and Models in Intuitas' snappy tool","title":"Semantic modelling, mastering and lineage"},{"location":"level_1/#unified-metadata-repository","text":"DataHub serves as a consolidation layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, domains, and projects providing a comprehensive view of data flows and relationships throughout the ecosystem. Example: Datahub Lineage Example: Enterprise-wide summary of assets Example: Browse by business domain and filters Example: Metadata search by term Example: User-driven mapping of glossary terms to measures Example: User-driven tagging of PII","title":"Unified metadata repository"},{"location":"level_1/#analytics-engineering-metadata","text":"dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to standards and conventions .","title":"Analytics engineering metadata"},{"location":"level_1/#databricks-unity-catalog-metastore","text":"Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Unity catalog is able to store table, column and lineage metadata inherit schema metadata from dbt detect definitions using AI where they are missing Example: Databricks AI-driven semantic detection Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout.","title":"Databricks Unity Catalog Metastore"},{"location":"level_1/#enterprise-security","text":"Recommended artefacts: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs","title":"Enterprise Security"},{"location":"level_1/#enterprise-data-governance","text":"Recommended artefacts: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management","title":"Enterprise Data Governance"},{"location":"level_1/#audit","text":"Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended artefacts: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions.","title":"Audit"},{"location":"level_1/#example-questions-and-associated-queries","text":"As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_1/#enterprise-billing","text":"Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform must to align. See: - Funding and costing structures - Observability Solutions Recommendations here align with the following Domain topology: Administration and Billing Scopes","title":"Enterprise Billing"},{"location":"level_1/#databricks-features-for-usage-tracking","text":"","title":"Databricks features for usage tracking"},{"location":"level_1/#metadata-and-tags","text":"In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags","title":"Metadata and tags"},{"location":"level_1/#cluster-policies","text":"Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions.","title":"Cluster policies"},{"location":"level_1/#system-tables","text":"System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools Dynamic Billing Solution","title":"System tables"},{"location":"level_1/#usage-reports","text":"Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request) Additional useful references: - Top 10 Queries to use with System Tables - Unlocking Cost Optimization Insights with Databricks System Tables","title":"Usage reports"},{"location":"level_1/#domain-workspace-administrator-role","text":"Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts","title":"Domain / Workspace Administrator Role"},{"location":"level_1/#tagging-convention","text":"All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id","title":"Tagging convention"},{"location":"level_1/#typical-observability-requirements-by-role","text":"As an Enterprise Admin 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Typical observability requirements by role"},{"location":"level_2/","text":"Level 2 - Domain-Level (Solution) Architecture and Patterns Return to home This section shows how the Enterprise Data Platform Reference Architecture as well as standards are applied within individual domains to create solutions. Why It Matters Using common patterns and standards at the domain level keeps solutions consistent and compatible. This speeds up delivery, avoids rework, and ensures every solution contributes to and strengthens the overall enterprise \u201ctown plan.\u201d Table of Contents Business architecture Business processes Business glossary Business metrics Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Example Platform and Pipeline Reference Architecture Business architecture Use cases and business requirements Business requirements, use cases, and use case roadmaps together define what a solution must achieve, how it will be applied, and when capabilities will be delivered according to agreed priorities. These priorities may sit at the enterprise, domain level or both. Understanding them is necessary to understand: the outcomes the solution must enable or improve the measures of success, constraints, and scope that shape design how the solution will be used to meet specific business needs the sequence and priority of delivering capabilities to maximise business value Business processes Business processes are the activities and tasks undertaken to achieve business goals. Understanding them allows the data architecture to uncover: the context in which information is captured and used key concepts and entities relevant to the domain or use case relationships between processes and data the basis for defining measures and metrics Business glossary A business glossary is a curated list of terms and definitions relevant to the business (at both the Enterprise and Domain levels). Understanding these terms and how they map across the organisation by stakeholders and systems is critical to consistent understanding and usage of concepts. It is core part of Enterprise Metadata Architecture . Example glossary from Intuitas' Glossary builder tool Contact us at \ud83d\udce7 office@intuitas.com to learn more about the tool. Business metrics Measures are raw, quantifiable values that capture facts about business activities (e.g., total sales, number of customers). Metrics are calculated or derived indicators that assess the performance of business processes, often using one or more measures (e.g., sales growth rate, customer churn rate). Both require capture, at a minimum, of: name definition formula (with reference to concepts and terms as defined in the business glossary) associated dimensions source(s) metric owner frequency Data Architecture Data Architecture defines how data is structured, stored, and accessed. It spans storage design, data models, integration, and governance to ensure trusted, reusable, and consistent data. Data and information models Conceptual Models \u2013 capture high-level business concepts and rules in plain language, creating a shared understanding between business and IT. Logical Models \u2013 refine concepts into entities, attributes, and relationships, ensuring clarity and consistency across domains while remaining technology-agnostic. Physical Models \u2013 implement the design in databases and systems, optimised for performance, scalability, and integration. Domain-level models often align more closely to real-world business semantics and rules than the enterprise-wide model. While they may not map one-to-one with enterprise or other domain models, cross-mapping is essential to identify dependencies, ensure conformance (e.g., shared dimensions, master data), and support integration across the organisation. Example of modelling a Domain-scoped Conceptual Information Model See Bounded context Domain glossary The Domain Glossary complements the Domain Model by describing concepts in clear business language. It defines domain-specific terms, synonyms, and the context in which they are used, along with properties such as attributes, keys, measures, and metrics. A well-curated Domain Glossary: Ensures clarity, reduces ambiguity, and strengthens alignment between business understanding and technical implementation. Builds on the Enterprise Glossary: Extend the enterprise glossary with domain-specific definitions. References when aligned: Where a domain term is synonymous with an enterprise definition, the enterprise glossary should be referenced rather than duplicated. Resolves when conflicting: Where definitions diverge or conflict, governance processes must be applied to reconcile differences and ensure consistency. Example of authoring a Domain-scoped Glossary aligned to the CIM and Enterprise Glossary Example of syncing the Glossary term to real-life Data and Products Domain data and warehouse models Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities. Data layers and stages Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Within each layer, data is transformed through a series of stages. Top level layers follow the Medallion architecture . Bronze: Data according to source. Silver: Data according to business. ( see Data and information models ) Gold: Data according to requirements. Data layers and stages These map to naming standards and conventions for Catalog , Schemas and dbt . Metadata layer Contains engineering and governance of data managed within the platform. The format of this will vary depending on the choice of engineering and governance toolsets and associated metamodels within the solution as well as across the broader enterprise. see Enterprise Metadata Architecture Bronze layer: Data according to source The Bronze layer stores raw, immutable data as it is ingested from source systems. The choice of persistence level will depend on requirements. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing Silver layer: Data according to business The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. These datasets are aligned with broadly accepted business standards and models, making them suitable for a range of downstream requirements. While some interpretations consider Silver to be primarily source-centric , this blueprint adopts a more flexible approach\u2014allowing for integration and harmonization of assets across multiple data sources. Silver Staging Transformations used to shape source data into standardised, conformed, and fit-for-use Reference Data, Data Vault and Base Information Mart objects. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Data Quality Data quality test results from checks applied to source data. Further transformation of these results may be applied to shape them into data quality reports. Reference Data Reference data, being a common asset and provided for broad consumption should be aligned to standards and downstream needs. Historical versioning requirements of reference data may need to be considered. Raw Vault Optional Data vault 2.0 aligned raw data warehouse. Business Vault Optional Business rule applied objects as per Data vault 2.0. Base Information Marts The term base is used to distinguish these marts from the domain- or enterprise-specific marts found in the Gold layer. Base marts are designed for broad usability across multiple downstream use cases\u2014for example, a conformed customer dimension. In some scenarios, it may be beneficial to maintain source-centric base marts alongside a final consolidation (UNION) mart\u2014all conforming to a common logical model. This approach supports decoupled pipelining across multiple sources, improving modularity and maintainability. These marts may be implemented as Kimball-style dimensional models or denormalized flat tables, depending on performance and reporting requirements. However, dimensional modelling is generally encouraged for its clarity, reusability, and alignment with analytic workloads. Gold layer: Data according to requirements The Gold layer focuses on delivering business-ready datasets, including aggregations and reporting structures that directly reflect specific business requirements. In some instances, Gold assets may be reused across multiple use cases or domains\u2014blurring the line with Silver. While this is not inherently problematic, it is important to consider supportability and scalability to ensure these assets remain trustworthy, maintainable, and accessible over time. Consider shifting logic left into the Silver layer\u2014such as common aggregations, reusable business rules, or conformed dimensions. This improves consistency, reduces duplication, and enables faster development of Gold-layer assets by building on stronger, more standardized foundations. Gold Staging Transformations used to shape source data into business-ready datasets, aligned to business requirements. Examples of Business-specific transformations include: Pivoting Aggregation Joining Conformance Desensitization While dbt best practices use the term 'Intermediates' as reuseable building blocks for marts, this is considered a form of staging and are hence optional under this blueprint. https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business Information Marts (Requirement Specific) The term 'business' here is use to distinguish marts in this layer from marts in the Silver layer. These marts are designed for a defined requirement. e.g. sales fact aggregated by region. These marts may be Kimball or denormalised flat tables depending on requirements; although Kimball dimensional models are encouraged. A solution served to the consumption layer is likely to contain a mix of Silver and Gold mart objects. e.g: - silver.dim_customer - gold.fact_sales_aggregated_by_region Lakehouse Catalog to Storage Mapping Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Illustrative example of Catalog to storage mapping in the Intuitas Demo Environment: Data Engineering Ingestion Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: Scalable, Resilient, Maintainable, Governed Pattern-based, automated and Metadata-driven where possible Batch and stream-based Example batch ingestion options: Ingestion patterns and notes: Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: databricks lakeflow: source -> lakeflow connect -> ods requires network access to source Pattern 6: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks) Transformation Under development. (Contact us to know more). Batch and Micro-batch SQL transformation dbt see dbt standards Streaming SQL transformation Under development. (Contact us to know more). Non SQL transformation Under development. (Contact us to know more). Data sharing and delivery patterns Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below. Sharing and delivery visualisation channels Pull / direct access Databricks Delta sharing practices Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing ADLSGen2 Access to Data Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing DuckDB Access to Data (via Unity Catalog) Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported SQL Access SQL Access is provided by the Databricks SQL (serverless) endpoint. API Access Under development. (Contact us to know more). The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html Snowflake Access Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial Microsoft Fabric Access The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to import from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Avoided SQL Endpoint costs for reads Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Requires public workspaces Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption Push Under development. (Contact us to know more). Areas for consideration include: adf databricks lakeflow Visualisation Under development. (Contact us to know more). Areas for consideration include: Powerbi Databricks dashboards Apps Open source visual options AI/ML Under development. (Contact us to know more). Areas for consideration include: MLOps Training Databricks Azure ML Data governance This section describes how Enterprise-level governance will be implemented through solutions at the domain level. Data lifecycle and asset management Under development. (Contact us to know more). Areas for consideration include: data contracts and policy data asset tagging Data access management Under development. (Contact us to know more). Areas for consideration include: data access request management data contracts access audit activity audit Data quality Under development. (Contact us to know more). Areas for consideration include: data quality checking and reporting data standards and quality business rule management Data understandability Under development. (Contact us to know more). Areas for consideration include: data lineage data object metadata Privacy Preservation Under development. (Contact us to know more). Areas for consideration include: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names. Row Level Security Under development. (Contact us to know more). Areas for consideration include: dynamic views precomputed views costs and overheads of various patterns of sharing of RLS-applied data Audit Under development. (Contact us to know more). Areas for consideration include: audit log queries Typical observability requirements by role As a Domain (workspace) Admin 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Infrastructure Under development. (Contact us to know more). Environments, Workspaces and Storage Workspaces, Environments and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage. Secrets Under development. (Contact us to know more). Areas for consideration include: Management Areas of use Handling practices Storage Lakehouse storage Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod CICD and Repository Under development. (Contact us to know more). Areas for consideration include: Description of git workflows for CICD in terms of: Infrastructure Data engineering Analytics engineering Data science / AIML BI, Reports and other products Tools Under development. (Contact us to know more). Areas for consideration include: Github Azure Devops Databricks Asset Bundles Repositories Under development. (Contact us to know more). Areas for consideration include: Infrastructure IAC repos (Terraform) dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles Observability Tools included in reference architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring dbt observability - Elementary Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation dbt warehouse observability Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention. Networking Areas for consideration include: By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies can be enabled - however consider impacts on private endpoints. Orchestration Under development. (Contact us to know more). Tools included in reference architecture Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt) Security Under development. (Contact us to know more). Tools included in reference architecture Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Level 2"},{"location":"level_2/#level-2-domain-level-solution-architecture-and-patterns","text":"Return to home This section shows how the Enterprise Data Platform Reference Architecture as well as standards are applied within individual domains to create solutions.","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#why-it-matters","text":"Using common patterns and standards at the domain level keeps solutions consistent and compatible. This speeds up delivery, avoids rework, and ensures every solution contributes to and strengthens the overall enterprise \u201ctown plan.\u201d","title":"Why It Matters"},{"location":"level_2/#table-of-contents","text":"Business architecture Business processes Business glossary Business metrics Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Example Platform and Pipeline Reference Architecture","title":"Table of Contents"},{"location":"level_2/#business-architecture","text":"","title":"Business architecture"},{"location":"level_2/#use-cases-and-business-requirements","text":"Business requirements, use cases, and use case roadmaps together define what a solution must achieve, how it will be applied, and when capabilities will be delivered according to agreed priorities. These priorities may sit at the enterprise, domain level or both. Understanding them is necessary to understand: the outcomes the solution must enable or improve the measures of success, constraints, and scope that shape design how the solution will be used to meet specific business needs the sequence and priority of delivering capabilities to maximise business value","title":"Use cases and business requirements"},{"location":"level_2/#business-processes","text":"Business processes are the activities and tasks undertaken to achieve business goals. Understanding them allows the data architecture to uncover: the context in which information is captured and used key concepts and entities relevant to the domain or use case relationships between processes and data the basis for defining measures and metrics","title":"Business processes"},{"location":"level_2/#business-glossary","text":"A business glossary is a curated list of terms and definitions relevant to the business (at both the Enterprise and Domain levels). Understanding these terms and how they map across the organisation by stakeholders and systems is critical to consistent understanding and usage of concepts. It is core part of Enterprise Metadata Architecture . Example glossary from Intuitas' Glossary builder tool Contact us at \ud83d\udce7 office@intuitas.com to learn more about the tool.","title":"Business glossary"},{"location":"level_2/#business-metrics","text":"Measures are raw, quantifiable values that capture facts about business activities (e.g., total sales, number of customers). Metrics are calculated or derived indicators that assess the performance of business processes, often using one or more measures (e.g., sales growth rate, customer churn rate). Both require capture, at a minimum, of: name definition formula (with reference to concepts and terms as defined in the business glossary) associated dimensions source(s) metric owner frequency","title":"Business metrics"},{"location":"level_2/#data-architecture","text":"Data Architecture defines how data is structured, stored, and accessed. It spans storage design, data models, integration, and governance to ensure trusted, reusable, and consistent data.","title":"Data Architecture"},{"location":"level_2/#data-and-information-models","text":"Conceptual Models \u2013 capture high-level business concepts and rules in plain language, creating a shared understanding between business and IT. Logical Models \u2013 refine concepts into entities, attributes, and relationships, ensuring clarity and consistency across domains while remaining technology-agnostic. Physical Models \u2013 implement the design in databases and systems, optimised for performance, scalability, and integration. Domain-level models often align more closely to real-world business semantics and rules than the enterprise-wide model. While they may not map one-to-one with enterprise or other domain models, cross-mapping is essential to identify dependencies, ensure conformance (e.g., shared dimensions, master data), and support integration across the organisation. Example of modelling a Domain-scoped Conceptual Information Model See Bounded context","title":"Data and information models"},{"location":"level_2/#domain-glossary","text":"The Domain Glossary complements the Domain Model by describing concepts in clear business language. It defines domain-specific terms, synonyms, and the context in which they are used, along with properties such as attributes, keys, measures, and metrics. A well-curated Domain Glossary: Ensures clarity, reduces ambiguity, and strengthens alignment between business understanding and technical implementation. Builds on the Enterprise Glossary: Extend the enterprise glossary with domain-specific definitions. References when aligned: Where a domain term is synonymous with an enterprise definition, the enterprise glossary should be referenced rather than duplicated. Resolves when conflicting: Where definitions diverge or conflict, governance processes must be applied to reconcile differences and ensure consistency. Example of authoring a Domain-scoped Glossary aligned to the CIM and Enterprise Glossary Example of syncing the Glossary term to real-life Data and Products","title":"Domain glossary"},{"location":"level_2/#domain-data-and-warehouse-models","text":"Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities.","title":"Domain data and warehouse models"},{"location":"level_2/#data-layers-and-stages","text":"Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Within each layer, data is transformed through a series of stages. Top level layers follow the Medallion architecture . Bronze: Data according to source. Silver: Data according to business. ( see Data and information models ) Gold: Data according to requirements. Data layers and stages These map to naming standards and conventions for Catalog , Schemas and dbt .","title":"Data layers and stages"},{"location":"level_2/#metadata-layer","text":"Contains engineering and governance of data managed within the platform. The format of this will vary depending on the choice of engineering and governance toolsets and associated metamodels within the solution as well as across the broader enterprise. see Enterprise Metadata Architecture","title":"Metadata layer"},{"location":"level_2/#bronze-layer-data-according-to-source","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. The choice of persistence level will depend on requirements. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing","title":"Bronze layer: Data according to source"},{"location":"level_2/#silver-layer-data-according-to-business","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. These datasets are aligned with broadly accepted business standards and models, making them suitable for a range of downstream requirements. While some interpretations consider Silver to be primarily source-centric , this blueprint adopts a more flexible approach\u2014allowing for integration and harmonization of assets across multiple data sources. Silver Staging Transformations used to shape source data into standardised, conformed, and fit-for-use Reference Data, Data Vault and Base Information Mart objects. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Data Quality Data quality test results from checks applied to source data. Further transformation of these results may be applied to shape them into data quality reports. Reference Data Reference data, being a common asset and provided for broad consumption should be aligned to standards and downstream needs. Historical versioning requirements of reference data may need to be considered. Raw Vault Optional Data vault 2.0 aligned raw data warehouse. Business Vault Optional Business rule applied objects as per Data vault 2.0. Base Information Marts The term base is used to distinguish these marts from the domain- or enterprise-specific marts found in the Gold layer. Base marts are designed for broad usability across multiple downstream use cases\u2014for example, a conformed customer dimension. In some scenarios, it may be beneficial to maintain source-centric base marts alongside a final consolidation (UNION) mart\u2014all conforming to a common logical model. This approach supports decoupled pipelining across multiple sources, improving modularity and maintainability. These marts may be implemented as Kimball-style dimensional models or denormalized flat tables, depending on performance and reporting requirements. However, dimensional modelling is generally encouraged for its clarity, reusability, and alignment with analytic workloads.","title":"Silver layer: Data according to business"},{"location":"level_2/#gold-layer-data-according-to-requirements","text":"The Gold layer focuses on delivering business-ready datasets, including aggregations and reporting structures that directly reflect specific business requirements. In some instances, Gold assets may be reused across multiple use cases or domains\u2014blurring the line with Silver. While this is not inherently problematic, it is important to consider supportability and scalability to ensure these assets remain trustworthy, maintainable, and accessible over time. Consider shifting logic left into the Silver layer\u2014such as common aggregations, reusable business rules, or conformed dimensions. This improves consistency, reduces duplication, and enables faster development of Gold-layer assets by building on stronger, more standardized foundations. Gold Staging Transformations used to shape source data into business-ready datasets, aligned to business requirements. Examples of Business-specific transformations include: Pivoting Aggregation Joining Conformance Desensitization While dbt best practices use the term 'Intermediates' as reuseable building blocks for marts, this is considered a form of staging and are hence optional under this blueprint. https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Business Information Marts (Requirement Specific) The term 'business' here is use to distinguish marts in this layer from marts in the Silver layer. These marts are designed for a defined requirement. e.g. sales fact aggregated by region. These marts may be Kimball or denormalised flat tables depending on requirements; although Kimball dimensional models are encouraged. A solution served to the consumption layer is likely to contain a mix of Silver and Gold mart objects. e.g: - silver.dim_customer - gold.fact_sales_aggregated_by_region","title":"Gold layer: Data according to requirements"},{"location":"level_2/#lakehouse-catalog-to-storage-mapping","text":"Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Illustrative example of Catalog to storage mapping in the Intuitas Demo Environment:","title":"Lakehouse Catalog to Storage Mapping"},{"location":"level_2/#data-engineering","text":"","title":"Data Engineering"},{"location":"level_2/#ingestion","text":"Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: Scalable, Resilient, Maintainable, Governed Pattern-based, automated and Metadata-driven where possible Batch and stream-based Example batch ingestion options:","title":"Ingestion"},{"location":"level_2/#ingestion-patterns-and-notes","text":"Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: databricks lakeflow: source -> lakeflow connect -> ods requires network access to source Pattern 6: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks)","title":"Ingestion patterns and notes:"},{"location":"level_2/#transformation","text":"Under development. (Contact us to know more).","title":"Transformation"},{"location":"level_2/#batch-and-micro-batch-sql-transformation","text":"dbt see dbt standards","title":"Batch and Micro-batch SQL transformation"},{"location":"level_2/#streaming-sql-transformation","text":"Under development. (Contact us to know more).","title":"Streaming SQL transformation"},{"location":"level_2/#non-sql-transformation","text":"Under development. (Contact us to know more).","title":"Non SQL transformation"},{"location":"level_2/#data-sharing-and-delivery-patterns","text":"Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below. Sharing and delivery visualisation channels","title":"Data sharing and delivery patterns"},{"location":"level_2/#pull-direct-access","text":"","title":"Pull / direct access"},{"location":"level_2/#databricks-delta-sharing-practices","text":"Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing","title":"Databricks Delta sharing practices"},{"location":"level_2/#adlsgen2-access-to-data","text":"Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing","title":"ADLSGen2 Access to Data"},{"location":"level_2/#duckdb-access-to-data-via-unity-catalog","text":"Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported","title":"DuckDB Access to Data (via Unity Catalog)"},{"location":"level_2/#sql-access","text":"SQL Access is provided by the Databricks SQL (serverless) endpoint.","title":"SQL Access"},{"location":"level_2/#api-access","text":"Under development. (Contact us to know more). The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html","title":"API Access"},{"location":"level_2/#snowflake-access","text":"Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial","title":"Snowflake Access"},{"location":"level_2/#microsoft-fabric-access","text":"The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to import from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Avoided SQL Endpoint costs for reads Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Requires public workspaces Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption","title":"Microsoft Fabric Access"},{"location":"level_2/#push","text":"Under development. (Contact us to know more). Areas for consideration include: adf databricks lakeflow","title":"Push"},{"location":"level_2/#visualisation","text":"Under development. (Contact us to know more). Areas for consideration include: Powerbi Databricks dashboards Apps Open source visual options","title":"Visualisation"},{"location":"level_2/#aiml","text":"Under development. (Contact us to know more). Areas for consideration include: MLOps Training Databricks Azure ML","title":"AI/ML"},{"location":"level_2/#data-governance","text":"This section describes how Enterprise-level governance will be implemented through solutions at the domain level.","title":"Data governance"},{"location":"level_2/#data-lifecycle-and-asset-management","text":"Under development. (Contact us to know more). Areas for consideration include: data contracts and policy data asset tagging","title":"Data lifecycle and asset management"},{"location":"level_2/#data-access-management","text":"Under development. (Contact us to know more). Areas for consideration include: data access request management data contracts access audit activity audit","title":"Data access management"},{"location":"level_2/#data-quality","text":"Under development. (Contact us to know more). Areas for consideration include: data quality checking and reporting data standards and quality business rule management","title":"Data quality"},{"location":"level_2/#data-understandability","text":"Under development. (Contact us to know more). Areas for consideration include: data lineage data object metadata","title":"Data understandability"},{"location":"level_2/#privacy-preservation","text":"Under development. (Contact us to know more). Areas for consideration include: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names.","title":"Privacy Preservation"},{"location":"level_2/#row-level-security","text":"Under development. (Contact us to know more). Areas for consideration include: dynamic views precomputed views costs and overheads of various patterns of sharing of RLS-applied data","title":"Row Level Security"},{"location":"level_2/#audit","text":"Under development. (Contact us to know more). Areas for consideration include: audit log queries","title":"Audit"},{"location":"level_2/#typical-observability-requirements-by-role","text":"As a Domain (workspace) Admin 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Typical observability requirements by role"},{"location":"level_2/#infrastructure","text":"Under development. (Contact us to know more).","title":"Infrastructure"},{"location":"level_2/#environments-workspaces-and-storage","text":"Workspaces, Environments and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage.","title":"Environments, Workspaces and Storage"},{"location":"level_2/#secrets","text":"Under development. (Contact us to know more). Areas for consideration include: Management Areas of use Handling practices","title":"Secrets"},{"location":"level_2/#storage","text":"","title":"Storage"},{"location":"level_2/#lakehouse-storage","text":"Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Lakehouse storage"},{"location":"level_2/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"level_2/#cicd-and-repository","text":"Under development. (Contact us to know more). Areas for consideration include: Description of git workflows for CICD in terms of: Infrastructure Data engineering Analytics engineering Data science / AIML BI, Reports and other products","title":"CICD and Repository"},{"location":"level_2/#tools","text":"Under development. (Contact us to know more). Areas for consideration include: Github Azure Devops Databricks Asset Bundles","title":"Tools"},{"location":"level_2/#repositories","text":"Under development. (Contact us to know more). Areas for consideration include: Infrastructure IAC repos (Terraform) dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles","title":"Repositories"},{"location":"level_2/#observability","text":"Tools included in reference architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring","title":"Observability"},{"location":"level_2/#dbt-observability-elementary","text":"Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation dbt warehouse observability Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention.","title":"dbt observability - Elementary"},{"location":"level_2/#networking","text":"Areas for consideration include: By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies can be enabled - however consider impacts on private endpoints.","title":"Networking"},{"location":"level_2/#orchestration","text":"Under development. (Contact us to know more).","title":"Orchestration"},{"location":"level_2/#tools-included-in-reference-architecture","text":"Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt)","title":"Tools included in reference architecture"},{"location":"level_2/#security","text":"Under development. (Contact us to know more).","title":"Security"},{"location":"level_2/#tools-included-in-reference-architecture_1","text":"Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Tools included in reference architecture"},{"location":"standards_and_conventions/","text":"Standards and Conventions Return to home These standards are opinionated and designed to ensure consistency, governance, and automation across an organisation. They largely reflect an Azure Databricks environment, however can be adapted to other platforms. Organisations should adapt these standards to fit their existing internal conventions. Table of Contents Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (standards_and_conventions.mdADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Policies Frameworks Mesh Domain Names All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g: intuitas_corporate Platform Environment Environment name: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred) VNET Name: vn-{organisation_name}-{domain_name} = e.g: vn-intuitas-corporate Resource Groups Name: rg-{organisation_name}-{domain_name} e.g: rg-intuitas-corporate Databricks workspace Name: ws-{organisation_name}-{domain_name} e.g: ws-intuitas-corporate Key vault Name: kv-{organisation_name}-{domain_name} e.g: kv-intuitas-corporate Secrets Name: {secret_name} Entra Group Names Name: eg-{organisation_name}-{domain_name} = e.g: eg-intuitas-corporate Azure Data Factory (ADF) Name: adf-{organisation_name}-{domain_name} e.g: adf-intuitas-corporate SQL Server Name: sql-{organisation_name}-{domain_name} e.g: sql-intuitas-corporate SQL Database Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g: sqldb-metadata-intuitas-corporate Storage The section describes naming standards and conventions for cloud storage resources. Lakehouse storage Lakehouse storage account name: dl{organisation_name}{domain_name} Lakehouse storage containers Name: {environment} (dev/test/preprod/prod) Lakehouse storage folders Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} e.g: bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob files and folders No standard naming conventions for files and folders. Databricks This section provides naming standards and conventions for Databricks. Workspace and cluster names Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name} Catalog naming and conventions Refer to Data layers and stages for further context and definitions applicable to this section. Catalog name: The choice of granularity depends on domain topology, stage/zone convention and desired level of segregation for access and sharing controls (i.e. catalog or schema level) Minimum granularity (domain level): {domain_name}{_environment (dev/test/pat/prod)} (prod is implied optional) e.g: intuitas_corporate_dev Optional granularity (domain-data stage level): {domain_name}{_data_stage: (bronze/silver/gold)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_bronze_dev Optional granularity (domain-data stage and zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/im)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_bronze_ods_dev Optional granularity (domain-data zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/im)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_ods_dev Optional granularity (subdomain-data stage level): {domain_name}{_descriptor (subdomain/subject/project*)}(bronze/silver/gold)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_finance_bronze_dev In the examples provided - we have opted for domain level - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__lakeflow Note that projects are temporary constructs, and hence are not recommended for naming Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog Externally mounted (lakehouse federation) Catalog Names Catalog name: {domain_name (owner)} _ext__{source_system}{optional:__other_useful_descriptors e.g:_environment} e.g: intuitas_corporate_ext__sqlonpremsource Catalog Metadata tags: The following metadata should be added when creating a catalog: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g: if delegating to engineering domain Schema and object conventions Refer to Data layers and stages for further context and definitions applicable to this section. Schema level external storage locations Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g: abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations. Metadata Schemas and Objects Refer to Data layers and stages for further context and definitions applicable to this section. Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__{optional: function} Naming convention: {function/descriptor} e.g: intuitas_corporate_dev.meta__ingestion.ingestion_control Bronze (Raw data according to systems) The Bronze layer stores raw, immutable data as it is ingested from source systems. See Data layers and stages for definitions and context. All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. bronze__ In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__lakeflow Persistent Landing : - N/A (see file naming) Operational Data Store (ODS) : The objective of raw layer conventions is to provide clarity over which zone and stage it belongs, what the data relates to, where it was sourced from, and via what channel it arrived (as there may be nuances in data depending on its channel). ODS can be replicated from source systems, or prepared for use from semi/unstructured data via hard-transformation and hence will have these associated conventions: Database replicated ODS (structured sources like SQL Server):: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (ods)}{__source_database}{if applicable:__source_schema}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow)} - Table naming convention: {named as per source} - e.g: intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__sqlsvr-intuitas-engineering__adf.encounter Prepped semi/unstructured ODS data: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (ods)}{__source_descriptor}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow/kafka/dbrx pipeline)} - Table naming convention: {named as per source or other unique assigned name (e.g. topic/folder name)} - e.g: intuitas_engineering_dev.bronze__ods__ambosim__intuitas-confluent__databricks.encounter Persistent Data Store (PDS) : PDS conventions will mirror ODS conventions: Database replicated PDS (structured sources like SQL Server):: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (pds)}{__source_database}{if applicable:__source_schema}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow)} - Table naming convention: {named as per source} - e.g: intuitas_engineering_dev.bronze__pds__fhirhouse__dbo__sqlsvr-intuitas-engineering__adf.encounter Prepped semi/unstructured PDS data: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (pds)}{__source_descriptor}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow/kafka/dbrx pipeline)} - Table naming convention: {named as per source or other unique assigned name (e.g. topic/folder name)} - e.g: intuitas_engineering_dev.bronze__pds__ambosim__intuitas-confluent__databricks.encounter Silver (Data according to business entities) The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. Refer to Data layers and stages for further context and definitions applicable to this section. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.silver__mart All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. silver__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver) Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts only. Source-specific (note at this stage, pre-normalisation - sourcing channels may still matter): Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (stg)}{__source_system_identifier}{optional:__source_channel} Object naming convention: {entity}{__object_description}{__n}{__transformation}{optional:__source_system_identifier}{optional:__source_channel} e.g: intuitas_corporate_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed e.g: intuitas_corporate_dev.stg__new_finance_system__adf.accounts__02_cleaned e.g: intuitas_corporate_dev.stg__old_finance_system__adf.accounts__01_renamed_and_typed Non-source specific: Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (stg)}{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align with target mart: stg__(optional:d(dim)/f(fact)){_entity}{__object_description}{__n}{__transformation} e.g: intuitas_corporate_dev.stg.accounts__01_deduped e.g: intuitas_corporate_dev.stg.accounts__02_business_validated Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g: intuitas_corporate_dev.stg__finance_system__adf.stg__finance_system__adf__account__01_renamed_and_typed (Silver) Base Information Marts : Final products after staging: Source-specific (note at this stage, post-normalisation - sourcing channels should not differ so may need merging or unioning): Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (mart)}{__source_system_identifier}{optional:__source_channel} Object naming convention: (optional:d(dim)/f(fact)){__entity / __object_description}{optional:__source_system_identifier}{optional:__source_channel} e.g: intuitas_corporate_dev.mart__new_finance_system__adf.payment e.g: intuitas_corporate_dev.mart__new_finance_system__adf.account e.g: intuitas_corporate_dev.mart__old_finance_system__adf.account Non-source specific: Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (mart)}{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: (optional:d(dim)/f(fact)){__unified entity / __object_description} e.g: intuitas_corporate_dev.mart.account (unified) e.g: intuitas_corporate_dev.mart__corporate__finance.account (unified) e.g: intuitas_corporate_dev.mart__finance.account (unified) e.g: intuitas_corporate_dev.mart.account_join_with_payments (joined across two systems) Reference Data : Reference data objects that are aligned to business entities and broad requirements. These may also be staged in stg as per silver marts. These are typically not source-aligned but optionality for capturing sources exists. Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference data set name} (optional:__{source_system}__{source_channel}) e.g: intuitas_corporate_dev.ref.account_code Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g: intuitas_corporate_dev.edw_rv.hs_payments__finance_system__adf Business Vault : Optional warehousing construct. Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g: intuitas_corporate_dev.edw_bv.hs_late_payments__finance_system__adf Gold (Data according to requirements) The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists in case its needed. Refer to Data layers and stages for further context and definitions applicable to this section. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_clinical_dev.gold__mart All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. gold__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Gold) Staging Models : Staging models serve as intermediary models that transform source data into the target mart model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold marts. Schema naming convention: {optional: data_stage__: (gold__)}{data_zone: (stg)}{optional: __domain name}{optional: __subdomain name(s)} d(dim)ension naming convention: d(dim){__entity / __product description} (optional: __{source_system_identifier}__{source_channel}){__n}{__transformation} Fact naming convention: f(fact){__entity / __product description} (optional: __{source_system_identifier}__{source_channel}){__n}{__transformation} Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}){__n}{__transformation} e.g: intuitas_corporate_dev.stg.f__late_payments__01__pivoted_by_order e.g: intuitas_corporate_dev.stg__corporate.f__late_payments__01__pivoted_by_order e.g: intuitas_corporate_dev.stg__corporate__finance.f__late_payments__01__pivoted_by_order (Gold) Information Marts : Schema naming convention: {optional: data_stage__: (gold__)}{data_zone: (mart)}{optional: __domain name}{optional: __subdomain name(s)} Dimension naming convention: d(dim){__entity / __product description} (optional: __{source_system}__{source_channel}) Fact naming convention: f(fact){__entity / __product description} (optional: __{source_system}__{source_channel}) Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}) Required transformation: Business-specific transformations such as: pivoting aggregation joining conformance desensitization e.g: intuitas_corporate_dev.mart.f_late_payments intuitas_corporate_dev.mart.regionally_grouped_account_payments__old_finance_system__adf intuitas_corporate_dev.mart.regionally_grouped_account_payments__new_finance_system__adf intuitas_corporate_dev.mart.regionally_grouped_account_payments (union of old and new) Delta Sharing Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g: intuitas_corporate__finance__reporting__account_payments__payments Azure Data Factory Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g: ls_financedb_intuitas_corporate Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency} Streaming Cluster name: {domain_name}__cluster__{optional:environment} Topic names: {domain_name}__{object/entity?}__{optional:source_system}___{optional:source_channel}__{optional:environment} Consumer group names: {domain_name}__{unique_group_name}__{optional:environment} dbt The following standards and conventions relate to dbt projects. Documentation and model metadata Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml Sources Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g: bronze__ods__ambo_sim__kafka__local__sources.yml Model and Folder Names dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards. Bronze Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/ Silver Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} *e.g:* - *silver\\new_finance_system__adf\\stg\\intuitas_corporate__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - or *silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - materialises to: *intuitas_corporate_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:d(dim)/f(fact)}{__entity /_object_description} __{ordinal}_{transformation description} e.g: intuitas_corporate_dev.stg.accounts__01_deduped e.g: intuitas_corporate_dev.stg.accounts__02_business_validated *e.g:* - *silver\\mart\\accounts\\stg\\intuitas_corporate__silver__stg__accounts__01_deduped.sql* - or *silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql* - materialises to: *e.g: intuitas_corporate_dev.stg.accounts__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:d(dim)/f(fact)}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:d(dim)/f(fact)}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:d(dim)/f(fact)} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention:** Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards} Gold Staging:** Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} d(dim)ensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__d(dim){__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__f(fact){__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}} Example dbt model structure: The model structure below reflects a single catalog for domain+environment and schema separation for layers and stages: {{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__d_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__d_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__d_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__d_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g: finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql dbt_project.yml The yml structure below reflects a single catalog for domain+environment and schema separation for layers and stages: models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name CI/CD The following standards and conventions relate to Continuous Improvement and Continuous Delivery constructs. Repository naming All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-corporate-dbt - intuitas-corporate-ingestion-framework - intuitas-corporate-cicd-templates - intuitas-corporate-infrastructure Branch naming All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests Branch lifecycle Simple branch lifecycle: main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master Comprehensive team branch lifecycle: Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes Databricks Asset Bundles Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name}` e.g: if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g: yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com Security Security standards and conventioned provided here provide a starter set, however existing organisational and applicable industry standards should take precedence. Consult with your cybersecurity advisor. Entra Under development. (Contact us to know more). Most organisations will already have an established set of groups and conventions. Where there are gaps, the following can still be considered. Recommended areas to align to organisational governance and cyber requirements: Naming conventions for admin, service, and user groups Role-based access alignment (least privilege, separation of duties) Alignment to domains - Cross-domain vs. domain-specific group patterns Entra Group Names : Pattern: grp-<org>-<domain>-<plat>-<scope>-<role>-<env>[-<region>][-ext-<partner>][-idx] Lowercase, hyphen-separated; no spaces. Keep to \u2264 120 chars total. No PII in names. Use Security groups (not M365) for RBAC; enable PIM where appropriate e.g. Admins. role : owner \u2014 full control of the named scope admin \u2014 administrative (non-ownership) rights contrib \u2014 create/modify within scope editor \u2014 modify data/artifacts, not permissions reader \u2014 read-only steward \u2014 governance/metadata rights custodian \u2014 key/secret/storage control operator \u2014 run/ops rights (pipelines, jobs) viewer \u2014 read dashboards/reports plat: dbx (Databricks), uc (Unity Catalog), pbi (Power BI), adf (Data Factory), dlk (Data Lake), sql (Azure SQL), kva (Key Vault), syn (Synapse) scope (or object): Databricks Workspace: ws- Unity Catalog: uc-meta (metastore), uc-cat- , uc-sch- . , uc-obj- . . Power BI: pbi-ws- Data Lake: dlk-path-/datalake/ / Examples : GRP-INTUITAS-CLIN-DBX-WS-Analytics-ADMIN-PRD GRP-INTUITAS-CLIN-UC-UC-CAT-Claims-OWNER-PRD GRP-INTUITAS-CLIN-UC-UC-SCH-Claims.Curated-READER-UAT GRP-INTUITAS-FIN-PBI-PBI-WS-ExecDash-VIEWER-PRD GRP-INTUITAS-ENT-KVA-KVA-Keys-CUSTODIAN-PRD GRP-INTUITAS-CLIN-DLK-DLK-PATH-/curated/claims/READER-PRD-AUE GRP-INTUITAS-CLIN-DBX-WS-PartnerLake-READER-PRD-EXT-ACME Policies Under development. (Contact us to know more). Recommended areas to align to non-functional requirements: Data retention (duration, archival, legal hold) Key retention and rotation cycles Backup and recovery standards Incident response and escalation procedures Access review and recertification Frameworks Under development. (Contact us to know more). Recommended areas to align to industry and cyber compliance: Engineering standards (e.g., code repositories, CI/CD security, IaC policies) Security frameworks (e.g., NIST, ISO 27001, CIS Benchmarks, Zero Trust) Compliance mappings (HIPAA, GDPR, SOC2, local regulatory obligations)","title":"Standards"},{"location":"standards_and_conventions/#standards-and-conventions","text":"Return to home These standards are opinionated and designed to ensure consistency, governance, and automation across an organisation. They largely reflect an Azure Databricks environment, however can be adapted to other platforms. Organisations should adapt these standards to fit their existing internal conventions.","title":"Standards and Conventions"},{"location":"standards_and_conventions/#table-of-contents","text":"Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (standards_and_conventions.mdADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Policies Frameworks","title":"Table of Contents"},{"location":"standards_and_conventions/#mesh","text":"","title":"Mesh"},{"location":"standards_and_conventions/#domain-names","text":"All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g: intuitas_corporate","title":"Domain Names"},{"location":"standards_and_conventions/#platform","text":"","title":"Platform"},{"location":"standards_and_conventions/#environment","text":"Environment name: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred)","title":"Environment"},{"location":"standards_and_conventions/#vnet","text":"Name: vn-{organisation_name}-{domain_name} = e.g: vn-intuitas-corporate","title":"VNET"},{"location":"standards_and_conventions/#resource-groups","text":"Name: rg-{organisation_name}-{domain_name} e.g: rg-intuitas-corporate","title":"Resource Groups"},{"location":"standards_and_conventions/#databricks-workspace","text":"Name: ws-{organisation_name}-{domain_name} e.g: ws-intuitas-corporate","title":"Databricks workspace"},{"location":"standards_and_conventions/#key-vault","text":"Name: kv-{organisation_name}-{domain_name} e.g: kv-intuitas-corporate","title":"Key vault"},{"location":"standards_and_conventions/#secrets","text":"Name: {secret_name}","title":"Secrets"},{"location":"standards_and_conventions/#entra-group-names","text":"Name: eg-{organisation_name}-{domain_name} = e.g: eg-intuitas-corporate","title":"Entra Group Names"},{"location":"standards_and_conventions/#azure-data-factory-adf","text":"Name: adf-{organisation_name}-{domain_name} e.g: adf-intuitas-corporate","title":"Azure Data Factory (ADF)"},{"location":"standards_and_conventions/#sql-server","text":"Name: sql-{organisation_name}-{domain_name} e.g: sql-intuitas-corporate","title":"SQL Server"},{"location":"standards_and_conventions/#sql-database","text":"Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g: sqldb-metadata-intuitas-corporate","title":"SQL Database"},{"location":"standards_and_conventions/#storage","text":"The section describes naming standards and conventions for cloud storage resources.","title":"Storage"},{"location":"standards_and_conventions/#lakehouse-storage","text":"Lakehouse storage account name: dl{organisation_name}{domain_name}","title":"Lakehouse storage"},{"location":"standards_and_conventions/#lakehouse-storage-containers","text":"Name: {environment} (dev/test/preprod/prod)","title":"Lakehouse storage containers"},{"location":"standards_and_conventions/#lakehouse-storage-folders","text":"Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} e.g: bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog","title":"Lakehouse storage folders"},{"location":"standards_and_conventions/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"standards_and_conventions/#generic-blob-files-and-folders","text":"No standard naming conventions for files and folders.","title":"Generic Blob files and folders"},{"location":"standards_and_conventions/#databricks","text":"This section provides naming standards and conventions for Databricks.","title":"Databricks"},{"location":"standards_and_conventions/#workspace-and-cluster-names","text":"Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name}","title":"Workspace and cluster names"},{"location":"standards_and_conventions/#catalog-naming-and-conventions","text":"Refer to Data layers and stages for further context and definitions applicable to this section. Catalog name: The choice of granularity depends on domain topology, stage/zone convention and desired level of segregation for access and sharing controls (i.e. catalog or schema level) Minimum granularity (domain level): {domain_name}{_environment (dev/test/pat/prod)} (prod is implied optional) e.g: intuitas_corporate_dev Optional granularity (domain-data stage level): {domain_name}{_data_stage: (bronze/silver/gold)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_bronze_dev Optional granularity (domain-data stage and zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/im)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_bronze_ods_dev Optional granularity (domain-data zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/im)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_ods_dev Optional granularity (subdomain-data stage level): {domain_name}{_descriptor (subdomain/subject/project*)}(bronze/silver/gold)}{_environment (dev/test/pat/prod)} e.g: intuitas_corporate_finance_bronze_dev In the examples provided - we have opted for domain level - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__lakeflow Note that projects are temporary constructs, and hence are not recommended for naming Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog","title":"Catalog naming and conventions"},{"location":"standards_and_conventions/#externally-mounted-lakehouse-federation-catalog-names","text":"Catalog name: {domain_name (owner)} _ext__{source_system}{optional:__other_useful_descriptors e.g:_environment} e.g: intuitas_corporate_ext__sqlonpremsource","title":"Externally mounted (lakehouse federation) Catalog Names"},{"location":"standards_and_conventions/#catalog-metadata-tags","text":"The following metadata should be added when creating a catalog: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g: if delegating to engineering domain","title":"Catalog Metadata tags:"},{"location":"standards_and_conventions/#schema-and-object-conventions","text":"Refer to Data layers and stages for further context and definitions applicable to this section.","title":"Schema and object conventions"},{"location":"standards_and_conventions/#schema-level-external-storage-locations","text":"Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g: abfss://dev@dlintutiasengineering.dfs.core.windows.net/intuitas_engineering_dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations.","title":"Schema level external storage locations"},{"location":"standards_and_conventions/#metadata-schemas-and-objects","text":"Refer to Data layers and stages for further context and definitions applicable to this section. Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta__{optional: function} Naming convention: {function/descriptor} e.g: intuitas_corporate_dev.meta__ingestion.ingestion_control","title":"Metadata Schemas and Objects"},{"location":"standards_and_conventions/#bronze-raw-data-according-to-systems","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. See Data layers and stages for definitions and context. All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. bronze__ In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__lakeflow Persistent Landing : - N/A (see file naming) Operational Data Store (ODS) : The objective of raw layer conventions is to provide clarity over which zone and stage it belongs, what the data relates to, where it was sourced from, and via what channel it arrived (as there may be nuances in data depending on its channel). ODS can be replicated from source systems, or prepared for use from semi/unstructured data via hard-transformation and hence will have these associated conventions: Database replicated ODS (structured sources like SQL Server):: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (ods)}{__source_database}{if applicable:__source_schema}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow)} - Table naming convention: {named as per source} - e.g: intuitas_engineering_dev.bronze__ods__fhirhouse__dbo__sqlsvr-intuitas-engineering__adf.encounter Prepped semi/unstructured ODS data: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (ods)}{__source_descriptor}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow/kafka/dbrx pipeline)} - Table naming convention: {named as per source or other unique assigned name (e.g. topic/folder name)} - e.g: intuitas_engineering_dev.bronze__ods__ambosim__intuitas-confluent__databricks.encounter Persistent Data Store (PDS) : PDS conventions will mirror ODS conventions: Database replicated PDS (structured sources like SQL Server):: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (pds)}{__source_database}{if applicable:__source_schema}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow)} - Table naming convention: {named as per source} - e.g: intuitas_engineering_dev.bronze__pds__fhirhouse__dbo__sqlsvr-intuitas-engineering__adf.encounter Prepped semi/unstructured PDS data: - Schema naming : {optional: data_stage__: (bronze__)}{data_zone: (pds)}{__source_descriptor}{__source_system_identifier}{__source_channel: (adf/fivetran/lakeflow/kafka/dbrx pipeline)} - Table naming convention: {named as per source or other unique assigned name (e.g. topic/folder name)} - e.g: intuitas_engineering_dev.bronze__pds__ambosim__intuitas-confluent__databricks.encounter","title":"Bronze (Raw data according to systems)"},{"location":"standards_and_conventions/#silver-data-according-to-business-entities","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. Refer to Data layers and stages for further context and definitions applicable to this section. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_engineering_dev.silver__mart All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. silver__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver) Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts only. Source-specific (note at this stage, pre-normalisation - sourcing channels may still matter): Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (stg)}{__source_system_identifier}{optional:__source_channel} Object naming convention: {entity}{__object_description}{__n}{__transformation}{optional:__source_system_identifier}{optional:__source_channel} e.g: intuitas_corporate_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed e.g: intuitas_corporate_dev.stg__new_finance_system__adf.accounts__02_cleaned e.g: intuitas_corporate_dev.stg__old_finance_system__adf.accounts__01_renamed_and_typed Non-source specific: Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (stg)}{optional: __domain name}{optional: __subdomain name(s)} Object naming convention to align with target mart: stg__(optional:d(dim)/f(fact)){_entity}{__object_description}{__n}{__transformation} e.g: intuitas_corporate_dev.stg.accounts__01_deduped e.g: intuitas_corporate_dev.stg.accounts__02_business_validated Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised e.g: intuitas_corporate_dev.stg__finance_system__adf.stg__finance_system__adf__account__01_renamed_and_typed (Silver) Base Information Marts : Final products after staging: Source-specific (note at this stage, post-normalisation - sourcing channels should not differ so may need merging or unioning): Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (mart)}{__source_system_identifier}{optional:__source_channel} Object naming convention: (optional:d(dim)/f(fact)){__entity / __object_description}{optional:__source_system_identifier}{optional:__source_channel} e.g: intuitas_corporate_dev.mart__new_finance_system__adf.payment e.g: intuitas_corporate_dev.mart__new_finance_system__adf.account e.g: intuitas_corporate_dev.mart__old_finance_system__adf.account Non-source specific: Schema naming convention: {optional: data_stage__: (silver__)}{data_zone: (mart)}{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: (optional:d(dim)/f(fact)){__unified entity / __object_description} e.g: intuitas_corporate_dev.mart.account (unified) e.g: intuitas_corporate_dev.mart__corporate__finance.account (unified) e.g: intuitas_corporate_dev.mart__finance.account (unified) e.g: intuitas_corporate_dev.mart.account_join_with_payments (joined across two systems) Reference Data : Reference data objects that are aligned to business entities and broad requirements. These may also be staged in stg as per silver marts. These are typically not source-aligned but optionality for capturing sources exists. Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference data set name} (optional:__{source_system}__{source_channel}) e.g: intuitas_corporate_dev.ref.account_code Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g: intuitas_corporate_dev.edw_rv.hs_payments__finance_system__adf Business Vault : Optional warehousing construct. Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g: intuitas_corporate_dev.edw_bv.hs_late_payments__finance_system__adf","title":"Silver (Data according to business entities)"},{"location":"standards_and_conventions/#gold-data-according-to-requirements","text":"The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists in case its needed. Refer to Data layers and stages for further context and definitions applicable to this section. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e intuitas_clinical_dev.gold__mart All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. gold__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Gold) Staging Models : Staging models serve as intermediary models that transform source data into the target mart model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold marts. Schema naming convention: {optional: data_stage__: (gold__)}{data_zone: (stg)}{optional: __domain name}{optional: __subdomain name(s)} d(dim)ension naming convention: d(dim){__entity / __product description} (optional: __{source_system_identifier}__{source_channel}){__n}{__transformation} Fact naming convention: f(fact){__entity / __product description} (optional: __{source_system_identifier}__{source_channel}){__n}{__transformation} Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}){__n}{__transformation} e.g: intuitas_corporate_dev.stg.f__late_payments__01__pivoted_by_order e.g: intuitas_corporate_dev.stg__corporate.f__late_payments__01__pivoted_by_order e.g: intuitas_corporate_dev.stg__corporate__finance.f__late_payments__01__pivoted_by_order (Gold) Information Marts : Schema naming convention: {optional: data_stage__: (gold__)}{data_zone: (mart)}{optional: __domain name}{optional: __subdomain name(s)} Dimension naming convention: d(dim){__entity / __product description} (optional: __{source_system}__{source_channel}) Fact naming convention: f(fact){__entity / __product description} (optional: __{source_system}__{source_channel}) Denormalized (One Big Table) Object naming convention: {entity / product description} (optional: __{source_system}__{source_channel}) Required transformation: Business-specific transformations such as: pivoting aggregation joining conformance desensitization e.g: intuitas_corporate_dev.mart.f_late_payments intuitas_corporate_dev.mart.regionally_grouped_account_payments__old_finance_system__adf intuitas_corporate_dev.mart.regionally_grouped_account_payments__new_finance_system__adf intuitas_corporate_dev.mart.regionally_grouped_account_payments (union of old and new)","title":"Gold (Data according to requirements)"},{"location":"standards_and_conventions/#delta-sharing","text":"Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g: intuitas_corporate__finance__reporting__account_payments__payments","title":"Delta Sharing"},{"location":"standards_and_conventions/#azure-data-factory","text":"Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g: ls_financedb_intuitas_corporate Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency}","title":"Azure Data Factory"},{"location":"standards_and_conventions/#streaming","text":"Cluster name: {domain_name}__cluster__{optional:environment} Topic names: {domain_name}__{object/entity?}__{optional:source_system}___{optional:source_channel}__{optional:environment} Consumer group names: {domain_name}__{unique_group_name}__{optional:environment}","title":"Streaming"},{"location":"standards_and_conventions/#dbt","text":"The following standards and conventions relate to dbt projects.","title":"dbt"},{"location":"standards_and_conventions/#documentation-and-model-metadata","text":"Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml","title":"Documentation and model metadata"},{"location":"standards_and_conventions/#sources","text":"Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g: bronze__ods__ambo_sim__kafka__local__sources.yml","title":"Sources"},{"location":"standards_and_conventions/#model-and-folder-names","text":"dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards.","title":"Model and Folder Names"},{"location":"standards_and_conventions/#bronze","text":"Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/","title":"Bronze"},{"location":"standards_and_conventions/#silver","text":"Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} *e.g:* - *silver\\new_finance_system__adf\\stg\\intuitas_corporate__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - or *silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - materialises to: *intuitas_corporate_dev.stg__new_finance_system__adf.accounts__01_renamed_and_typed__new_finance_system__adf* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:d(dim)/f(fact)}{__entity /_object_description} __{ordinal}_{transformation description} e.g: intuitas_corporate_dev.stg.accounts__01_deduped e.g: intuitas_corporate_dev.stg.accounts__02_business_validated *e.g:* - *silver\\mart\\accounts\\stg\\intuitas_corporate__silver__stg__accounts__01_deduped.sql* - or *silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql* - materialises to: *e.g: intuitas_corporate_dev.stg.accounts__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:d(dim)/f(fact)}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:d(dim)/f(fact)}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:d(dim)/f(fact)} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention:** Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards}","title":"Silver"},{"location":"standards_and_conventions/#gold","text":"Staging:** Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} d(dim)ensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__d(dim){__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__f(fact){__entity / product description} (optional: __{source_system} __{source_channel}) {optional: __{source_system} __{source_channel}} Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}}","title":"Gold"},{"location":"standards_and_conventions/#example-dbt-model-structure","text":"The model structure below reflects a single catalog for domain+environment and schema separation for layers and stages: {{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__d_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__d_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__d_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__d_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g: finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql","title":"Example dbt model structure:"},{"location":"standards_and_conventions/#dbt_projectyml","text":"The yml structure below reflects a single catalog for domain+environment and schema separation for layers and stages: models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: +schema: bronze silver: +schema: silver source_system_1: +schema: silver__source_system_1 base: +materialized: view staging: +materialized: table edw__domain_name: +description: \"Domain-centric EDW objects.\" +schema: silver__edw__domain_name +materialized: table gold: +materialized: view # default for speed +schema: gold domain_name: +schema: gold__domain_name subdomain_name: +schema: gold__domain_name__subdomain_name","title":"dbt_project.yml"},{"location":"standards_and_conventions/#cicd","text":"The following standards and conventions relate to Continuous Improvement and Continuous Delivery constructs.","title":"CI/CD"},{"location":"standards_and_conventions/#repository-naming","text":"All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-corporate-dbt - intuitas-corporate-ingestion-framework - intuitas-corporate-cicd-templates - intuitas-corporate-infrastructure","title":"Repository naming"},{"location":"standards_and_conventions/#branch-naming","text":"All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests","title":"Branch naming"},{"location":"standards_and_conventions/#branch-lifecycle","text":"","title":"Branch lifecycle"},{"location":"standards_and_conventions/#simple-branch-lifecycle","text":"main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master","title":"Simple branch lifecycle:"},{"location":"standards_and_conventions/#comprehensive-team-branch-lifecycle","text":"Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes","title":"Comprehensive team branch lifecycle:"},{"location":"standards_and_conventions/#databricks-asset-bundles","text":"Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name}` e.g: if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g: yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com","title":"Databricks Asset Bundles"},{"location":"standards_and_conventions/#security","text":"Security standards and conventioned provided here provide a starter set, however existing organisational and applicable industry standards should take precedence. Consult with your cybersecurity advisor.","title":"Security"},{"location":"standards_and_conventions/#entra","text":"Under development. (Contact us to know more). Most organisations will already have an established set of groups and conventions. Where there are gaps, the following can still be considered. Recommended areas to align to organisational governance and cyber requirements: Naming conventions for admin, service, and user groups Role-based access alignment (least privilege, separation of duties) Alignment to domains - Cross-domain vs. domain-specific group patterns Entra Group Names : Pattern: grp-<org>-<domain>-<plat>-<scope>-<role>-<env>[-<region>][-ext-<partner>][-idx] Lowercase, hyphen-separated; no spaces. Keep to \u2264 120 chars total. No PII in names. Use Security groups (not M365) for RBAC; enable PIM where appropriate e.g. Admins. role : owner \u2014 full control of the named scope admin \u2014 administrative (non-ownership) rights contrib \u2014 create/modify within scope editor \u2014 modify data/artifacts, not permissions reader \u2014 read-only steward \u2014 governance/metadata rights custodian \u2014 key/secret/storage control operator \u2014 run/ops rights (pipelines, jobs) viewer \u2014 read dashboards/reports plat: dbx (Databricks), uc (Unity Catalog), pbi (Power BI), adf (Data Factory), dlk (Data Lake), sql (Azure SQL), kva (Key Vault), syn (Synapse) scope (or object): Databricks Workspace: ws- Unity Catalog: uc-meta (metastore), uc-cat- , uc-sch- . , uc-obj- . . Power BI: pbi-ws- Data Lake: dlk-path-/datalake/ / Examples : GRP-INTUITAS-CLIN-DBX-WS-Analytics-ADMIN-PRD GRP-INTUITAS-CLIN-UC-UC-CAT-Claims-OWNER-PRD GRP-INTUITAS-CLIN-UC-UC-SCH-Claims.Curated-READER-UAT GRP-INTUITAS-FIN-PBI-PBI-WS-ExecDash-VIEWER-PRD GRP-INTUITAS-ENT-KVA-KVA-Keys-CUSTODIAN-PRD GRP-INTUITAS-CLIN-DLK-DLK-PATH-/curated/claims/READER-PRD-AUE GRP-INTUITAS-CLIN-DBX-WS-PartnerLake-READER-PRD-EXT-ACME","title":"Entra"},{"location":"standards_and_conventions/#policies","text":"Under development. (Contact us to know more). Recommended areas to align to non-functional requirements: Data retention (duration, archival, legal hold) Key retention and rotation cycles Backup and recovery standards Incident response and escalation procedures Access review and recertification","title":"Policies"},{"location":"standards_and_conventions/#frameworks","text":"Under development. (Contact us to know more). Recommended areas to align to industry and cyber compliance: Engineering standards (e.g., code repositories, CI/CD security, IaC policies) Security frameworks (e.g., NIST, ISO 27001, CIS Benchmarks, Zero Trust) Compliance mappings (HIPAA, GDPR, SOC2, local regulatory obligations)","title":"Frameworks"}]}