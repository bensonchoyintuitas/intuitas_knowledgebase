{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"We help organisations cut through the complexity, turning data and AI into results. Enterprise Data Intelligence Blueprint This blueprint brings together resources that capture Intuitas\u2019 approach to designing and delivering Data, AI, and Governance solutions. It is a continually evolving resource, offering insights into strategic, enterprise, and solution-level practices\u2014distilled from our R&D, common questions, and real-world experience. The ideas and patterns reflect Intuitas\u2019 design philosophy: grounded in large, multi-domain enterprise deployments, yet adaptable to organisations of any size or type evolving, opinionated, and open to challenge demonstrable in working environments on request We share this blueprint with our customer and partner community to promote our vision of 'good design', help avoid pitfalls, and speed up delivery. We encourage you to explore, share and build on this information, with proper attribution to Intuitas and consideration as set out in our license and disclaimer . \u201cTo build a home, you need a plan \u2014 but for it to thrive, you need a town plan.\" Get help Contact us at office@intuitas.com to: Find out more, or provide feedback. Access our demonstration environment or any of the tools and technologies presented Get further help in: strategy, architecture, planning, training, implementation and governance. Table of Contents Overview Design Principles Getting Started Licensing and disclaimer Level 0: Enterprise Context Organisational & Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures Level 1: Enterprise Architecture Key Concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Principles Semantic and Data Lineage Metadata Objects and Elements Consolidation and Synchronisation Governance Metadata Enterprise Security Enterprise Data Governance Enterprise Billing Level 2: Domain Architecture Business Architecture Business Processes Business Glossary Business Metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data & Information Models Domain Glossary Domain Data & Warehouse Models Data Architecture Data Layers and Stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data Governance Data lifecycle and asset management Data access management Data Quality Data Understandability Privacy Preservation Audit Standards and conventions Standards & Conventions Modelling framework and standards Modelling framework Modelling standards and conventions Overview This blueprint is comprehensive and intentionally opinionated, providing practical patterns for designing and delivering enterprise-scale Data, AI, and Governance solutions. It is continuously refined through ongoing R&D, customer engagements, lessons learned, and evaluations of the evolving product landscape. It follows an iterative approach where each level builds on the previous one, while remaining flexible to be revisited and refined as capabilities mature. Organisations may begin at any level depending on their maturity, but all levels should be considered and validated for applicability: Level 0: Context Setting - Define organisational objectives, domain structures, and strategic goals Level 1: Enterprise Architecture - Apply enterprise-wide patterns and reference topologies Level 2: Domain Solutions - Implement domain-specific solutions using enterprise patterns Standards & Conventions - Apply consistent naming and conventions throughout See the Copyright, Licensing and Disclaimer information Design Principles \u201cThe alternative to good design is always bad design. There is no such thing as no design.\u201d - Adam Judge Our approach is guided by nine foundational principles that ensure transparency in decision-making, effective trade-off evaluation, and strategic alignment. Readers should consider the priority and implication of these principles alongside any existing principles applicable within their organisation. # Principle Description 1 Think big, start small Balance rapid delivery with strategic positioning. Deliver value iteratively. Ensure investments deliver sustained benefits without creating technical debt. 2 Empowered Autonomy Enable domain experts to manage data independently while leveraging shared, scalable foundations. 3 Disciplined Core, Flexible Edge Federated governance model that ensures policy consistency while enabling rapid, domain-specific delivery. 4 Actionable Data Real-time, comprehensive data extraction (structured & unstructured) with platforms ready for immediate insights and AI/ML. 5 Make It Easy to Do the Right Thing Provide automation and platforms that guide users toward secure, policy-aligned practices effortlessly. 6 Cost Transparency and Efficiency Transparent cost models with pay-for-value usage while promoting reuse and removing sharing barriers. 7 Adaptability for Growth Platforms seamlessly adapt to evolving business needs, data growth, and diverse workloads. 8 Interoperability and Inclusion Smooth integration across cloud, on-premises, and diverse technology stacks (bring-your-own). 9 Flexibility Through Standards Technology-agnostic, standards-based components that maintain flexibility and prevent vendor lock-in. Getting Started Recommended starting point by role: Role Recommended Path Focus Areas Leaders & Architects Levels 0-1 Strategic alignment, enterprise patterns, governance frameworks Domain & Technical Teams Level 2 Practical implementation within established enterprise frameworks Licensing and disclaimer Copyright: This knowledgebase and associated content are the original works of \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. License: Free use, reproduction, and adaptation permitted with prior consent and appropriate attribution to Intuitas PTY LTD. Referenced third-party content is subject to the copyright terms of their respective owners. Disclaimer: Content is provided for general information only The information provided is general in nature and may not cover all scenarios or workloads. It does not constitute professional advice and should not be relied on as a substitute for advice tailored to your circumstances. The information provided reflects the product landscape and functionality available in general release at the time of publishing. While every effort is made to maintain accuracy and update information as features evolve, the timeliness of these updates cannot be guaranteed. No liability is accepted by Intuitas PTY LTD for errors or omissions. Readers are encouraged to independently validate all claims and undertake benchmark against their own use cases and projected workloads. \ud83d\udce7 office@intuitas.com","title":"Home"},{"location":"#enterprise-data-intelligence-blueprint","text":"This blueprint brings together resources that capture Intuitas\u2019 approach to designing and delivering Data, AI, and Governance solutions. It is a continually evolving resource, offering insights into strategic, enterprise, and solution-level practices\u2014distilled from our R&D, common questions, and real-world experience. The ideas and patterns reflect Intuitas\u2019 design philosophy: grounded in large, multi-domain enterprise deployments, yet adaptable to organisations of any size or type evolving, opinionated, and open to challenge demonstrable in working environments on request We share this blueprint with our customer and partner community to promote our vision of 'good design', help avoid pitfalls, and speed up delivery. We encourage you to explore, share and build on this information, with proper attribution to Intuitas and consideration as set out in our license and disclaimer .","title":"Enterprise Data Intelligence Blueprint"},{"location":"#to-build-a-home-you-need-a-plan-but-for-it-to-thrive-you-need-a-town-plan","text":"","title":"\u201cTo build a home, you need a plan \u2014 but for it to thrive, you need a town plan.\""},{"location":"#get-help","text":"Contact us at office@intuitas.com to: Find out more, or provide feedback. Access our demonstration environment or any of the tools and technologies presented Get further help in: strategy, architecture, planning, training, implementation and governance.","title":"Get help"},{"location":"#table-of-contents","text":"Overview Design Principles Getting Started Licensing and disclaimer","title":"Table of Contents"},{"location":"#level-0-enterprise-context","text":"Organisational & Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures","title":"Level 0: Enterprise Context"},{"location":"#level-1-enterprise-architecture","text":"Key Concepts Reference topologies Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Principles Semantic and Data Lineage Metadata Objects and Elements Consolidation and Synchronisation Governance Metadata Enterprise Security Enterprise Data Governance Enterprise Billing","title":"Level 1: Enterprise Architecture"},{"location":"#level-2-domain-architecture","text":"Business Architecture Business Processes Business Glossary Business Metrics Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Data & Information Models Domain Glossary Domain Data & Warehouse Models Data Architecture Data Layers and Stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data Governance Data lifecycle and asset management Data access management Data Quality Data Understandability Privacy Preservation Audit","title":"Level 2: Domain Architecture"},{"location":"#standards-and-conventions","text":"Standards & Conventions","title":"Standards and conventions"},{"location":"#modelling-framework-and-standards","text":"Modelling framework Modelling standards and conventions","title":"Modelling framework and standards"},{"location":"#overview","text":"This blueprint is comprehensive and intentionally opinionated, providing practical patterns for designing and delivering enterprise-scale Data, AI, and Governance solutions. It is continuously refined through ongoing R&D, customer engagements, lessons learned, and evaluations of the evolving product landscape. It follows an iterative approach where each level builds on the previous one, while remaining flexible to be revisited and refined as capabilities mature. Organisations may begin at any level depending on their maturity, but all levels should be considered and validated for applicability: Level 0: Context Setting - Define organisational objectives, domain structures, and strategic goals Level 1: Enterprise Architecture - Apply enterprise-wide patterns and reference topologies Level 2: Domain Solutions - Implement domain-specific solutions using enterprise patterns Standards & Conventions - Apply consistent naming and conventions throughout See the Copyright, Licensing and Disclaimer information","title":"Overview"},{"location":"#design-principles","text":"","title":"Design Principles"},{"location":"#the-alternative-to-good-design-is-always-bad-design-there-is-no-such-thing-as-no-design-adam-judge","text":"Our approach is guided by nine foundational principles that ensure transparency in decision-making, effective trade-off evaluation, and strategic alignment. Readers should consider the priority and implication of these principles alongside any existing principles applicable within their organisation. # Principle Description 1 Think big, start small Balance rapid delivery with strategic positioning. Deliver value iteratively. Ensure investments deliver sustained benefits without creating technical debt. 2 Empowered Autonomy Enable domain experts to manage data independently while leveraging shared, scalable foundations. 3 Disciplined Core, Flexible Edge Federated governance model that ensures policy consistency while enabling rapid, domain-specific delivery. 4 Actionable Data Real-time, comprehensive data extraction (structured & unstructured) with platforms ready for immediate insights and AI/ML. 5 Make It Easy to Do the Right Thing Provide automation and platforms that guide users toward secure, policy-aligned practices effortlessly. 6 Cost Transparency and Efficiency Transparent cost models with pay-for-value usage while promoting reuse and removing sharing barriers. 7 Adaptability for Growth Platforms seamlessly adapt to evolving business needs, data growth, and diverse workloads. 8 Interoperability and Inclusion Smooth integration across cloud, on-premises, and diverse technology stacks (bring-your-own). 9 Flexibility Through Standards Technology-agnostic, standards-based components that maintain flexibility and prevent vendor lock-in.","title":"\u201cThe alternative to good design is always bad design. There is no such thing as no design.\u201d - Adam Judge"},{"location":"#getting-started","text":"Recommended starting point by role: Role Recommended Path Focus Areas Leaders & Architects Levels 0-1 Strategic alignment, enterprise patterns, governance frameworks Domain & Technical Teams Level 2 Practical implementation within established enterprise frameworks","title":"Getting Started"},{"location":"#licensing-and-disclaimer","text":"Copyright: This knowledgebase and associated content are the original works of \u00a9 Intuitas PTY LTD, 2025. All rights reserved. Any referenced or third-party materials remain the property of their respective copyright holders. Every effort has been made to accurately reference and attribute existing content, and no claim of ownership is made over such materials. License: Free use, reproduction, and adaptation permitted with prior consent and appropriate attribution to Intuitas PTY LTD. Referenced third-party content is subject to the copyright terms of their respective owners. Disclaimer: Content is provided for general information only The information provided is general in nature and may not cover all scenarios or workloads. It does not constitute professional advice and should not be relied on as a substitute for advice tailored to your circumstances. The information provided reflects the product landscape and functionality available in general release at the time of publishing. While every effort is made to maintain accuracy and update information as features evolve, the timeliness of these updates cannot be guaranteed. No liability is accepted by Intuitas PTY LTD for errors or omissions. Readers are encouraged to independently validate all claims and undertake benchmark against their own use cases and projected workloads.","title":"Licensing and disclaimer"},{"location":"#_1","text":"\ud83d\udce7 office@intuitas.com","title":""},{"location":"about/","text":"Redirecting to Intuitas Redirecting to Intuitas ... window.location.href = \"https://www.intuitas.com\";","title":"About"},{"location":"level_0/","text":"Level 0 - Enterprise-level context Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made. Why It Matters Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals. Table of Contents Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures Org + Domain Definition Understanding the organisation\u2019s structure and domains is key to setting governance, environment, and metadata boundaries. While industry patterns and reference architectures can guide, each organisation is unique. See Domain-Centric Design for more. Recommended artefacts: Description of the organisational boundary, including any external organisations in scope. Description of domains and subdomains that impact the data governance and management model. Illustrative example of domains for a Healthcare organisation Strategies and Objectives Investing in data initiatives without a clear strategic context risks misalignment and wasted effort. A well-defined strategic context for data, technology, and governance ensures initiatives align with organisational goals, set the right priorities, have a clear scope, support effective governance, and build strong investment and benefits cases. Recommended artefacts Assessment of relevant business, technology, and data/AI strategies, plans, and initiatives. Key Systems and Data Assets Identifying and profiling key systems and data assets in the context of strategic benefits and use cases provides a strong foundation for effective planning, design, and risk management. Recommended artefacts Description of the organisation's relevant systems and data sets (e.g. CMDB, Information Asset Register), including associated governance arrangements and technical characteristics. Team Capabilities and Roles Clarity on the responsibilities and capabilities of key teams is essential to executing the data strategy, maintaining accountability, ensuring all support functions are adequately fulfilled, and costs remain predictable. Recommended artefacts RACI matrix for associated teams/parties with consideration of functions including but not limted to: Data lifecycle \u2013 creation, management, governance (quality, access), security, and consumption Data enablement \u2013 engineering, integration, analysis, reporting, and application development Data foundations \u2013 information architecture, infrastructure, and platform provisioning and management Operational support \u2013 DataOps, monitoring, and ongoing maintenance Current and target operating and service management model Maturity and skill assessment of key teams/parties relative to role definition Strategy for using external partnerships and vendors to address capability gaps Governance Structures Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. These may already exist in some organisations, while in others they need to be developed. Recommended artefacts Description of governance frameworks, policies, standards, processes, and bodies relevant to the scope of concern. In some cases - proposals and terms of reference for new governance structures. Funding and costing structures How funding and costs are allocated directly shape the internal \u201cmarketplace\u201d for data and analytics. These structures determine whether teams collaborate or compete, influence speed of delivery and standards alignment, and define the organisation\u2019s capacity to scale and innovate. The architecture reflects this through: system-wide, domain, and role-level accountabilities. policies and guardrails to prevent bill-shock observability and optimisation See: - Enterprise Billing Solutions - Observability Solutions Recommended artefacts Billing structures \u2013 including how cost centres map to reports, delegations, and observability (important for transparency and accountability) Cost allocation and chargeback models \u2013 critical for encouraging reuse, managing shared services, and clarifying ownership Budgeting and forecasting \u2013 ensures sustainable funding for data platforms, products, and capabilities Cost optimisation and monitoring \u2013 helps manage platform efficiency and avoid waste, and how the accountabilities are distributed for their control Next Steps Once the enterprise-level context is established, proceed to Level 1 - Domain Architecture to explore domain-centric design principles and architectural patterns that support the organisation's strategic objectives.","title":"Level 0"},{"location":"level_0/#level-0-enterprise-level-context","text":"Return to home This section outlines the foundational enterprise-wide concepts that inform and guide data and platform architecture decisions across the organisation. It sets the strategic and business context within which all lower-level architectural decisions should be made.","title":"Level 0 - Enterprise-level context"},{"location":"level_0/#why-it-matters","text":"Establishing a clear enterprise-level context ensures that investments in data, capabilities, and infrastructure are purposefully aligned with the organisation\u2019s strategic vision, operating model, and capacity. This alignment is essential for delivering meaningful, sustainable outcomes that reflect the organisation\u2019s unique context and long-term goals.","title":"Why It Matters"},{"location":"level_0/#table-of-contents","text":"Org + Domain Definition Strategies and Objectives Key Systems and Data Assets Team Capabilities and Roles Governance Structures Funding and costing structures","title":"Table of Contents"},{"location":"level_0/#org-domain-definition","text":"Understanding the organisation\u2019s structure and domains is key to setting governance, environment, and metadata boundaries. While industry patterns and reference architectures can guide, each organisation is unique. See Domain-Centric Design for more. Recommended artefacts: Description of the organisational boundary, including any external organisations in scope. Description of domains and subdomains that impact the data governance and management model. Illustrative example of domains for a Healthcare organisation","title":"Org + Domain Definition"},{"location":"level_0/#strategies-and-objectives","text":"Investing in data initiatives without a clear strategic context risks misalignment and wasted effort. A well-defined strategic context for data, technology, and governance ensures initiatives align with organisational goals, set the right priorities, have a clear scope, support effective governance, and build strong investment and benefits cases. Recommended artefacts Assessment of relevant business, technology, and data/AI strategies, plans, and initiatives.","title":"Strategies and Objectives"},{"location":"level_0/#key-systems-and-data-assets","text":"Identifying and profiling key systems and data assets in the context of strategic benefits and use cases provides a strong foundation for effective planning, design, and risk management. Recommended artefacts Description of the organisation's relevant systems and data sets (e.g. CMDB, Information Asset Register), including associated governance arrangements and technical characteristics.","title":"Key Systems and Data Assets"},{"location":"level_0/#team-capabilities-and-roles","text":"Clarity on the responsibilities and capabilities of key teams is essential to executing the data strategy, maintaining accountability, ensuring all support functions are adequately fulfilled, and costs remain predictable. Recommended artefacts RACI matrix for associated teams/parties with consideration of functions including but not limted to: Data lifecycle \u2013 creation, management, governance (quality, access), security, and consumption Data enablement \u2013 engineering, integration, analysis, reporting, and application development Data foundations \u2013 information architecture, infrastructure, and platform provisioning and management Operational support \u2013 DataOps, monitoring, and ongoing maintenance Current and target operating and service management model Maturity and skill assessment of key teams/parties relative to role definition Strategy for using external partnerships and vendors to address capability gaps","title":"Team Capabilities and Roles"},{"location":"level_0/#governance-structures","text":"Governance frameworks and processes define how decisions are made, enforced, and improved across the data lifecycle. These may already exist in some organisations, while in others they need to be developed. Recommended artefacts Description of governance frameworks, policies, standards, processes, and bodies relevant to the scope of concern. In some cases - proposals and terms of reference for new governance structures.","title":"Governance Structures"},{"location":"level_0/#funding-and-costing-structures","text":"How funding and costs are allocated directly shape the internal \u201cmarketplace\u201d for data and analytics. These structures determine whether teams collaborate or compete, influence speed of delivery and standards alignment, and define the organisation\u2019s capacity to scale and innovate. The architecture reflects this through: system-wide, domain, and role-level accountabilities. policies and guardrails to prevent bill-shock observability and optimisation See: - Enterprise Billing Solutions - Observability Solutions Recommended artefacts Billing structures \u2013 including how cost centres map to reports, delegations, and observability (important for transparency and accountability) Cost allocation and chargeback models \u2013 critical for encouraging reuse, managing shared services, and clarifying ownership Budgeting and forecasting \u2013 ensures sustainable funding for data platforms, products, and capabilities Cost optimisation and monitoring \u2013 helps manage platform efficiency and avoid waste, and how the accountabilities are distributed for their control","title":"Funding and costing structures"},{"location":"level_0/#next-steps","text":"Once the enterprise-level context is established, proceed to Level 1 - Domain Architecture to explore domain-centric design principles and architectural patterns that support the organisation's strategic objectives.","title":"Next Steps"},{"location":"level_1/","text":"Level 1 - Enterprise-level architecture Return to home This section builds on the Level 0 - Enterprise and Strategic Context to describe enterprise-wide and cross-domain data and data platform architecture concepts. Why It Matters Establishing a clear enterprise-wide, cross-domain design\u2014the \u201ctown plan\u201d for data\u2014ensures that platform architecture, capabilities, and investments are purposefully coordinated across the organisation. This reduces duplication, enables shared infrastructure, and is critical to building a scalable, interoperable, and sustainable data platform aligned with the organisation\u2019s long-term vision. Table of Contents Key concepts Domain-Centric Design Domain Subdomain Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Metadata Architecture Principles Semantic and Data lineage Metadata Objects and Elements Metadata Consolidation and Synchronisation Data Architecture and Governance Metadata Semantic modelling, mastering and lineage Unified metadata repository Analytics engineering metadata Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Databricks features for usage tracking Metadata and tags Cluster policies System tables Usage reports Domain / Workspace Administrator Role Tagging convention Key concepts The following key concepts are used throughout this knowledgebase. Domain-Centric Design Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. See Domain driven design Illustative example domains for a Healthcare organisation Domain Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, costing and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Domain can exist at different levels of granularity and their boundaries may not be obvious. They are not necessarily a reflection of the organisational chart. Subdomain A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Example of authoring domains using Intuitas' Domain builder tool Data Mesh A data mesh is a decentralized approach to data management that shifts ownership and accountability to domain teams, enabling them to treat their data as a product. Each domain is responsible for creating, maintaining, and sharing high-quality, discoverable, and interoperable data products with other domains. The data mesh approach emphasizes domain autonomy, self-serve infrastructure, interoperability, and federated governance. It is not a one-size-fits-all model; its suitability depends on an organisation\u2019s context, culture, and capabilities, and its adoption will vary in maturity and success across organisations. See Data Mesh: Principles Domain Topology A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. See Data Mesh: Topologies and domain granularity Data Fabric A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles. Data Mesh vs Fabric A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric. Reference topologies Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Enterprise Domain Reference Topologies Source: Data Mesh: Topologies and domain granularity, Strengholt P., 2022 Hybrid federated mesh topology This blueprint depicts a Hybrid Federated Mesh Topology, increasingly common in large enterprises and mature engineering practices. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Example of hybrid federated mesh topology: Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organisation. Distributed Local engineering Maintaining a local bronze/raw layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access 'gold' data from other domains and, where appropriate, 'silver' or 'bronze', facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability. Enterprise Data Platform Reference Architecture Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example Platform and Pipeline Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture An enterprise logical data warehouse retains the core properties of a traditional data warehouse\u2014integrated, consistent, and analytics-ready data\u2014while operating in a distributed, domain-oriented model. Logical Data Warehouse topology is characterised by: Federated governance \u2013 shared policies and standards, but distributed custodianship, applied across the domain topology. Unified access \u2013 a common entry point for querying and consuming data regardless of its physical location Enterprise metadata \u2013 consistent definitions, lineage, and discovery across all domains via shared catalog. This approach combines the scalability and agility of decentralised ownership with the trust and coherence of an enterprise-wide data platform. Example logical data warehouse topology Enterprise Information and Data Architecture Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models Enterprise Metadata Architecture Metadata is an umbrella term encompassing various types of data that play critical roles across multiple domains, supporting the description, governance, and operational use of data and technology assets. It can be actively curated or generated as a byproduct of processes and systems. Metadata Architecture Principles The following principles reflect our design philosophy to ensure sustainable and effective metadata capability Principle Description Accessible Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic Automate collection and updates to keep metadata fresh and reduce manual work. Contextual Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated Metadata exists in an ecosystem across tools to support diverse workflows. Consistency Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure Protect metadata as it may contain sensitive details. Accountability Clearly define roles for ownership and stewardship. Agnostic Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability. Semantic and Data lineage Semantic lineage and data lineage are critical concepts in a modern data intelligence capability to ensure clarity, trust, and traceability of data \u2014 from its business meaning to its technical origins and transformations: Semantic Lineage maps business terms, definitions, and relationships across the data ecosystem, showing how concepts are represented and transformed across systems and domains. Data Lineage tracks the technical flow of data from source to destination, including all transformations, to provide visibility, support data quality, and meet compliance and governance needs. Together, they give a complete view of business and technical data flows, enabling stronger governance and management of data assets. Because they are often difficult to align and keep in sync, a unified approach , as provided in the reference architecture, is critical. Data and Semantic Lineage Metadata Objects and Elements Metadata exists in various types, formats, and purposes, each essential for enabling: Data and Information Governance & Architecture \u2013 including semantic and data lineage, as well as privacy, access, and security - controls Data Engineering and Analytics Development Business Interpretation and Understanding \u2013 supporting the context and meaning of information and analytics Data Quality and Integrity Technical and Platform Administration Integration, Data Sharing, and Interoperability The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. Metadata logical architecture Metadata Consolidation and Synchronisation Metadata consolidation and synchronisation are critical for achieving a consistent, unified view of data assets, enabling reliable lineage, governance, and context across the data ecosystem. This approach: Eliminates Silos: Aggregates metadata from diverse tools (e.g. dbt, Unity Catalog, PowerBI, MLflow) into a central catalogue like DataHub, ensuring all stakeholders access the same contextual information. Improves Trust and Traceability: Enables end-to-end lineage and visibility, helping users understand where data comes from, how it is transformed, and how it is used across platforms. Enables Automation and Governance: Supports data quality, access control, and policy enforcement through unified metadata APIs and standardized governance models. The diagram below illustrates metadata objects and elements that are created and managed across diverse tools and contexts\u2014each serving a distinct role in the broader data and technology ecosystem. Metadata flow Data Architecture and Governance Metadata Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability. Semantic modelling, mastering and lineage Snappy serves as a 'business-first' enterprise domain, model, standards and glossary authoring and mastering tool, and acts as the key driver of semantic lineage linking between true on-the-ground semantics, reference models and physical as-built metadata in Datahub. Modelling Domains, Glossaries and Models in Intuitas' snappy tool Unified metadata repository DataHub serves as a consolidation layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, domains, and projects providing a comprehensive view of data flows and relationships throughout the ecosystem. Example: Datahub Lineage Example: Enterprise-wide summary of assets Example: Browse by business domain and filters Example: Metadata search by term Example: User-driven mapping of glossary terms to measures Example: User-driven tagging of PII Analytics engineering metadata dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to standards and conventions . Databricks Unity Catalog Metastore Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Unity catalog is able to store table, column and lineage metadata inherit schema metadata from dbt detect definitions using AI where they are missing Example: Databricks AI-driven semantic detection Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout. Enterprise Security Recommended artefacts: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs Enterprise Data Governance Recommended artefacts: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management Audit Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended artefacts: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions. Example questions and associated queries As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Enterprise Billing Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform must to align. See: - Funding and costing structures - Observability Solutions Recommendations here align with the following Domain topology: Administration and Billing Scopes Databricks features for usage tracking Metadata and tags In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags Cluster policies Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions. System tables System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools Dynamic Billing Solution Usage reports Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request) Additional useful references: - Top 10 Queries to use with System Tables - Unlocking Cost Optimization Insights with Databricks System Tables Domain / Workspace Administrator Role Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts Tagging convention All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id Typical observability requirements by role As an Enterprise Admin 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Level 1"},{"location":"level_1/#level-1-enterprise-level-architecture","text":"Return to home This section builds on the Level 0 - Enterprise and Strategic Context to describe enterprise-wide and cross-domain data and data platform architecture concepts.","title":"Level 1 - Enterprise-level architecture"},{"location":"level_1/#why-it-matters","text":"Establishing a clear enterprise-wide, cross-domain design\u2014the \u201ctown plan\u201d for data\u2014ensures that platform architecture, capabilities, and investments are purposefully coordinated across the organisation. This reduces duplication, enables shared infrastructure, and is critical to building a scalable, interoperable, and sustainable data platform aligned with the organisation\u2019s long-term vision.","title":"Why It Matters"},{"location":"level_1/#table-of-contents","text":"Key concepts Domain-Centric Design Domain Subdomain Data Mesh Domain Topology Data Fabric Data Mesh vs Fabric Reference topologies Hybrid federated mesh topology Enterprise Data Platform Reference Architecture Enterprise (Logical) Data Warehouse Reference Architecture Enterprise Information and Data Architecture Enterprise Metadata Architecture Metadata Architecture Principles Semantic and Data lineage Metadata Objects and Elements Metadata Consolidation and Synchronisation Data Architecture and Governance Metadata Semantic modelling, mastering and lineage Unified metadata repository Analytics engineering metadata Databricks Unity Catalog Metastore Enterprise Security Enterprise Data Governance Audit Enterprise Billing Databricks features for usage tracking Metadata and tags Cluster policies System tables Usage reports Domain / Workspace Administrator Role Tagging convention","title":"Table of Contents"},{"location":"level_1/#key-concepts","text":"The following key concepts are used throughout this knowledgebase.","title":"Key concepts"},{"location":"level_1/#domain-centric-design","text":"Using domains as logical governance boundaries helps ensure data ownership and accountability. This approach aligns with the data mesh principle of decentralizing data management and providing domain teams with autonomy. Our use of this term draws inspiration from Domain-Driven Design and Data Mesh principles. See Domain driven design Illustative example domains for a Healthcare organisation","title":"Domain-Centric Design"},{"location":"level_1/#domain","text":"Domains relate to functional and organisational boundaries, and represent closely related areas of responsibility and focus. Each domain encapsulates functional responsibilities, services, processes, information, expertise, costing and governance. Domains serve their own objectives while also offering products and services of value to other domains and the broader enterprise. Domain can exist at different levels of granularity and their boundaries may not be obvious. They are not necessarily a reflection of the organisational chart.","title":"Domain"},{"location":"level_1/#subdomain","text":"A subdomain is a lower-level domain within a parent domain that groups data and capability related to a specific business or function area. Example of authoring domains using Intuitas' Domain builder tool","title":"Subdomain"},{"location":"level_1/#data-mesh","text":"A data mesh is a decentralized approach to data management that shifts ownership and accountability to domain teams, enabling them to treat their data as a product. Each domain is responsible for creating, maintaining, and sharing high-quality, discoverable, and interoperable data products with other domains. The data mesh approach emphasizes domain autonomy, self-serve infrastructure, interoperability, and federated governance. It is not a one-size-fits-all model; its suitability depends on an organisation\u2019s context, culture, and capabilities, and its adoption will vary in maturity and success across organisations. See Data Mesh: Principles","title":"Data Mesh"},{"location":"level_1/#domain-topology","text":"A Domain Topology is a representation of how domains are structured, positioned in the enterprise, and how they interact with each other. See Data Mesh: Topologies and domain granularity","title":"Domain Topology"},{"location":"level_1/#data-fabric","text":"A data fabric is a unified platform that integrates data from various sources and provides a single source of truth. It enables data sharing and collaboration across domains and supports data mesh principles.","title":"Data Fabric"},{"location":"level_1/#data-mesh-vs-fabric","text":"A data mesh and fabric are not mutually exclusive. In fact, they can be complementary approaches. A data mesh can be built on top of a data fabric.","title":"Data Mesh vs Fabric"},{"location":"level_1/#reference-topologies","text":"Organisations need to consider the current and target topology that best reflects their strategy, capabilities, structure and operating/service model. The arrangement of domains: reflects its operating model defines expectations on how data+products are shared, built, managed and governed impacts accessibility, costs, support and overall experience. Enterprise Domain Reference Topologies Source: Data Mesh: Topologies and domain granularity, Strengholt P., 2022","title":"Reference topologies"},{"location":"level_1/#hybrid-federated-mesh-topology","text":"This blueprint depicts a Hybrid Federated Mesh Topology, increasingly common in large enterprises and mature engineering practices. It integrates various distributed functional domains with a unified raw data engineering capability. While tailored for this topology, the guidance is broadly applicable to other configurations. Key characteristics of this topology include: Hybrid of Data Fabric and Data Mesh: Combines centralised governance with domain-specific autonomy. Offers a unified platform for seamless data integration, alongside domain-driven flexibility. Fabric-Like Features: Scalable, unified platform: Connects diverse data sources across the organisation. Shared infrastructure and standards: Ensures consistency, interoperability, and trusted data. Streamlined access: Simplifies workflows and reduces friction for data usage and insights delivery. Mesh-Like Features: Domain-driven autonomy: Empowers teams to create tailored solutions for their specific data and AI needs. Collaboration-focused: Teams act as both data producers and consumers, fostering reuse and efficiency. Federated governance: Ensures standards while allowing teams to manage their data locally. Example of hybrid federated mesh topology: Hybrid federated mesh topology reflects a common scenario whereby centralisation occurs upstream for improved consolidation and standardisation around engineering, while federation occurs downstream for improved analytical flexibility and responsiveness. Centralising engineering Centralizing engineering tasks related to source data processing allows for specialized teams to efficiently manage data ingestion, quality checks, and initial transformations. This specialization ensures consistency and reliability across the organisation. Distributed Local engineering Maintaining a local bronze/raw layer for non-enterprise-distributed data enables domains to handle their specific raw data requirements, supporting use cases that are not yet enterprise-wide. Cross-Domain Access Allowing domains to access 'gold' data from other domains and, where appropriate, 'silver' or 'bronze', facilitates reuse, cross-domain analytics and collaboration, ensuring data mesh interoperability.","title":"Hybrid federated mesh topology"},{"location":"level_1/#enterprise-data-platform-reference-architecture","text":"Describes the logical components (including infrastructure, applications and common services) that make up a default data and analytics solution, offered and supported by the enterprise. This artefact can then be used as the basis of developing domain-specific overlays. Example Platform and Pipeline Reference Architecture","title":"Enterprise Data Platform Reference Architecture"},{"location":"level_1/#enterprise-logical-data-warehouse-reference-architecture","text":"An enterprise logical data warehouse retains the core properties of a traditional data warehouse\u2014integrated, consistent, and analytics-ready data\u2014while operating in a distributed, domain-oriented model. Logical Data Warehouse topology is characterised by: Federated governance \u2013 shared policies and standards, but distributed custodianship, applied across the domain topology. Unified access \u2013 a common entry point for querying and consuming data regardless of its physical location Enterprise metadata \u2013 consistent definitions, lineage, and discovery across all domains via shared catalog. This approach combines the scalability and agility of decentralised ownership with the trust and coherence of an enterprise-wide data platform. Example logical data warehouse topology","title":"Enterprise (Logical) Data Warehouse Reference Architecture"},{"location":"level_1/#enterprise-information-and-data-architecture","text":"Solutions such as data warehouses and marts should reflect the business semantics relating to the scope of requirements, and will also need to consider: Existing enterprise information, conceptual and logical models Legacy warehouse models Domain information models and glossaries Domain business objects and process models Common entities and synonyms (which may form the basis of conformed dimensions) Data standards Other secondary considerations: Source system data models Industry models Integration models","title":"Enterprise Information and Data Architecture"},{"location":"level_1/#enterprise-metadata-architecture","text":"Metadata is an umbrella term encompassing various types of data that play critical roles across multiple domains, supporting the description, governance, and operational use of data and technology assets. It can be actively curated or generated as a byproduct of processes and systems.","title":"Enterprise Metadata Architecture"},{"location":"level_1/#metadata-architecture-principles","text":"The following principles reflect our design philosophy to ensure sustainable and effective metadata capability Principle Description Accessible Metadata must be easy to find, search, and use and maintain by business, technical and governance stakeholders. Dynamic Automate collection and updates to keep metadata fresh and reduce manual work. Contextual Bridge the gaps between business, technical, governance and architecture perspectives and serve the right metadata, in the right place, in the right format for the audience. Integrated Metadata exists in an ecosystem across tools to support diverse workflows. Consistency Use common standards, terms, and structures; Ensure all metadata is current and in-sync. Secure Protect metadata as it may contain sensitive details. Accountability Clearly define roles for ownership and stewardship. Agnostic Avoid vendor lock-in where possible. Keep metadata portable and open to ensure flexibility and interoperability.","title":"Metadata Architecture Principles"},{"location":"level_1/#semantic-and-data-lineage","text":"Semantic lineage and data lineage are critical concepts in a modern data intelligence capability to ensure clarity, trust, and traceability of data \u2014 from its business meaning to its technical origins and transformations: Semantic Lineage maps business terms, definitions, and relationships across the data ecosystem, showing how concepts are represented and transformed across systems and domains. Data Lineage tracks the technical flow of data from source to destination, including all transformations, to provide visibility, support data quality, and meet compliance and governance needs. Together, they give a complete view of business and technical data flows, enabling stronger governance and management of data assets. Because they are often difficult to align and keep in sync, a unified approach , as provided in the reference architecture, is critical. Data and Semantic Lineage","title":"Semantic and Data lineage"},{"location":"level_1/#metadata-objects-and-elements","text":"Metadata exists in various types, formats, and purposes, each essential for enabling: Data and Information Governance & Architecture \u2013 including semantic and data lineage, as well as privacy, access, and security - controls Data Engineering and Analytics Development Business Interpretation and Understanding \u2013 supporting the context and meaning of information and analytics Data Quality and Integrity Technical and Platform Administration Integration, Data Sharing, and Interoperability The diagram below shows metadata objects and elements created and managed across various tools and contexts, each serving different purposes. Metadata logical architecture","title":"Metadata Objects and Elements"},{"location":"level_1/#metadata-consolidation-and-synchronisation","text":"Metadata consolidation and synchronisation are critical for achieving a consistent, unified view of data assets, enabling reliable lineage, governance, and context across the data ecosystem. This approach: Eliminates Silos: Aggregates metadata from diverse tools (e.g. dbt, Unity Catalog, PowerBI, MLflow) into a central catalogue like DataHub, ensuring all stakeholders access the same contextual information. Improves Trust and Traceability: Enables end-to-end lineage and visibility, helping users understand where data comes from, how it is transformed, and how it is used across platforms. Enables Automation and Governance: Supports data quality, access control, and policy enforcement through unified metadata APIs and standardized governance models. The diagram below illustrates metadata objects and elements that are created and managed across diverse tools and contexts\u2014each serving a distinct role in the broader data and technology ecosystem. Metadata flow","title":"Metadata Consolidation and Synchronisation"},{"location":"level_1/#data-architecture-and-governance-metadata","text":"Metadata is essential for effective data governance, providing necessary context and information about data assets, with the following metadata and tools being core to this capability.","title":"Data Architecture and Governance Metadata"},{"location":"level_1/#semantic-modelling-mastering-and-lineage","text":"Snappy serves as a 'business-first' enterprise domain, model, standards and glossary authoring and mastering tool, and acts as the key driver of semantic lineage linking between true on-the-ground semantics, reference models and physical as-built metadata in Datahub. Modelling Domains, Glossaries and Models in Intuitas' snappy tool","title":"Semantic modelling, mastering and lineage"},{"location":"level_1/#unified-metadata-repository","text":"DataHub serves as a consolidation layer that connects and integrates end-to-end data lineage, business domain models, and their associated glossaries and data assets. The diagram below illustrates how DataHub consolidates lineage across diverse platforms, domains, and projects providing a comprehensive view of data flows and relationships throughout the ecosystem. Example: Datahub Lineage Example: Enterprise-wide summary of assets Example: Browse by business domain and filters Example: Metadata search by term Example: User-driven mapping of glossary terms to measures Example: User-driven tagging of PII","title":"Unified metadata repository"},{"location":"level_1/#analytics-engineering-metadata","text":"dbt Docs is the authoritative source for metadata related to SQL analytics engineering. It captures object, column, and lineage metadata, and provides a rich interface for discovery and documentation. dbt schema metadata is integrated with Databricks Unity Catalog. For more information, refer to standards and conventions .","title":"Analytics engineering metadata"},{"location":"level_1/#databricks-unity-catalog-metastore","text":"Unity Catalog supports centralized governance of data and metadata across Databricks workspaces. Each region can have only one Unity Catalog metastore . The metastore uses designated storage accounts to hold metadata and related data. Unity catalog is able to store table, column and lineage metadata inherit schema metadata from dbt detect definitions using AI where they are missing Example: Databricks AI-driven semantic detection Recommendations and Notes: Assign managed storage at the catalog level to enforce logical data isolation. Metastore-level and schema-level storage options also exist. Review catalog layout strategies to align with domain-oriented design. The metastore admin role is optional but, if used, should always be assigned to a group , not an individual. The enterprise's domain topology directly influences the Unity Catalog design and layout.","title":"Databricks Unity Catalog Metastore"},{"location":"level_1/#enterprise-security","text":"Recommended artefacts: Description of security policies and standards for both the organisation and industry Description of processes, tools, controls, protocols to adhere to during design, deployment and operation. Description of responsibilities and accountabilities. Risk and issues register Description of security management and monitoring tools incl. system audit logs","title":"Enterprise Security"},{"location":"level_1/#enterprise-data-governance","text":"Recommended artefacts: Description of governance frameworks, policies and standards including but not limited to: Custodianship, management/stewardship roles, RACI and mapping to permissions and metadata Privacy controls required, standards and services available Quality management expectations, standards and services available Audit requirements (e.g. data sharing, access) Description of governance bodies and decision rights Description of enterprise-level solutions and services for data governance References to Enterprise Metadata management","title":"Enterprise Data Governance"},{"location":"level_1/#audit","text":"Some organisations are bound to regulatory and policy requirements which mandate auditability. Examples of auditable areas include: data sharing and access; platform access; change history to data. Recommended artefacts: Description of mandatory audit requirements to inform enterprise-level and domain-level audit solutions.","title":"Audit"},{"location":"level_1/#example-questions-and-associated-queries","text":"As an Enterprise Metastore Admin: 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Example questions and associated queries"},{"location":"level_1/#enterprise-billing","text":"Large organisations typically need to track and allocate costs to organisational units, cost centres, projects, teams and individuals. Here is where the Business Architecture of the organisation, domain topology, infrastructure topology (such as workspace delegations) and features of the chosen platform must to align. See: - Funding and costing structures - Observability Solutions Recommendations here align with the following Domain topology: Administration and Billing Scopes","title":"Enterprise Billing"},{"location":"level_1/#databricks-features-for-usage-tracking","text":"","title":"Databricks features for usage tracking"},{"location":"level_1/#metadata-and-tags","text":"In Databricks, metadata can be used to track activity: Workspace level Workpace owners identity Workspace tags Cluster level Authorised cluster users identities Cluster tags Budget Policies (Enforced tagging for serverless clusters) Job level Jobs and associated job metadata (*Note job-specific tags only appear when using Job Clusters) Query level Query comments (Query tagging is not yet a feature) Tags from higher level resources flow through to lower level resources as per Databricks Usage Detail Tags","title":"Metadata and tags"},{"location":"level_1/#cluster-policies","text":"Cluster policies can be used to enforce tagging at the cluster level. Cluster policies can be set in the UI or via Databricks Asset Bundles in resource yaml definitions.","title":"Cluster policies"},{"location":"level_1/#system-tables","text":"System tables provide granular visibility of all activity within Databricks. System tables only provide DBU based billing insights, access to Azure Costs may require alternate reporting to be developed by the Azure administrator. By default, only Databricks Account administrators have access to system tables such as billing. This is a highly privileged role and is not fit for sharing broadly. Learn more Workspace administrators need to be delegated access to system tables, and likely restricted to their domain / workspace via dynamic system catalog views with RLS applied based on workspace ID. (See Dynamic Billing Solution below. Available on request) - see repo Databricks System Tools Dynamic Billing Solution","title":"System tables"},{"location":"level_1/#usage-reports","text":"Databricks supplies an out of the box Databricks Usage Dashboard which requires Account-level rights to view (To use the imported dashboard, a user must have the SELECT permissions on the system.billing.usage and system.billing.list_prices tables. Learn more Once workspace administrators have been delegated access to system tables, they can import a refactored version of the Databricks Usage Dashboard which are repointed to the RLS views. (See Dynamic Billing Solution above. Available on request) Additional useful references: - Top 10 Queries to use with System Tables - Unlocking Cost Optimization Insights with Databricks System Tables","title":"Usage reports"},{"location":"level_1/#domain-workspace-administrator-role","text":"Workspaces are a container for clusters, and hence are a natural fit for representing a Domain scope. Domain administrators (i.e Workspace Admins) shall be delegated functionality necessary to monitor and manage costs withing their domain (Workspace): Ability to audit and shutdown workloads Ability to create budget policies and enforce them on serverless clusters Ability to create cluster tagging policies and enforce them on clusters Ability to delegate/assign appropriate clusters and associated policies to domain users Ability to call on Databrick Account Admin to establish and update Budget Alerts","title":"Domain / Workspace Administrator Role"},{"location":"level_1/#tagging-convention","text":"All workloads (Jobs, serverless, shared compute) need to be attributable to at a minimum: Domain Environment: dev, test, uat, prod In addition all workloads may need more granular tagging in line with cost-centre granularity hence may include one of more of the following depending on your organisation's terminology: Sub-domain Team Business unit Cost centre Project In addition all scheduled Jobs would benefit from further job tags: Job name/id","title":"Tagging convention"},{"location":"level_1/#typical-observability-requirements-by-role","text":"As an Enterprise Admin 1. What workloads are not being tagged/tracked? 2. What is my organisation spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams/Domains spending on within the workspaces I have delegated? - In databricks DBUs - Inclusive of cloud 4. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilisation As a Domain (workspace) Admin 1. What workloads are not being tagged/tracked? 2. What is my domain spending as a whole? - In databricks DBUs - Inclusive of cloud 3. What are my subteams spending on within the workspace I administer? - In databricks DBUs - Inclusive of cloud 4. What are the most expensive activities? - By user - By job 5. Where are we wasting money as an enterprise? - Reinventing the wheel - Over utilising - Redundant tasks - Inefficient queries","title":"Typical observability requirements by role"},{"location":"level_2/","text":"Level 2 - Domain-Level (Solution) Architecture and Patterns Return to home This section shows how the Enterprise Data Platform Reference Architecture as well as standards are applied within individual domains to create solutions. Why It Matters Using common patterns and standards at the domain level keeps solutions consistent and compatible. This speeds up delivery, avoids rework, and ensures every solution contributes to and strengthens the overall enterprise \u201ctown plan.\u201d Table of Contents Business architecture Business processes Business glossary Business metrics Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Example Platform and Pipeline Reference Architecture Business architecture Use cases and business requirements Business requirements, use cases, and use case roadmaps together define what a solution must achieve, how it will be applied, and when capabilities will be delivered according to agreed priorities. These priorities may sit at the enterprise, domain level or both. Understanding them is necessary to understand: the outcomes the solution must enable or improve the measures of success, constraints, and scope that shape design how the solution will be used to meet specific business needs the sequence and priority of delivering capabilities to maximise business value Business processes Business processes are the activities and tasks undertaken to achieve business goals. Understanding them allows the data architecture to uncover: the context in which information is captured and used key concepts and entities relevant to the domain or use case relationships between processes and data the basis for defining measures and metrics Business glossary A business glossary is a curated list of terms and definitions relevant to the business (at both the Enterprise and Domain levels). Understanding these terms and how they map across the organisation by stakeholders and systems is critical to consistent understanding and usage of concepts. It is core part of Enterprise Metadata Architecture . Example glossary from Intuitas' Glossary builder tool Contact us at \ud83d\udce7 office@intuitas.com to learn more about the tool. Business metrics Measures are raw, quantifiable values that capture facts about business activities (e.g., total sales, number of customers). Metrics are calculated or derived indicators that assess the performance of business processes, often using one or more measures (e.g., sales growth rate, customer churn rate). Both require capture, at a minimum, of: name definition formula (with reference to concepts and terms as defined in the business glossary) associated dimensions source(s) metric owner frequency Data Architecture Data Architecture defines how data is structured, stored, and accessed. It spans storage design, data models, integration, and governance to ensure trusted, reusable, and consistent data. Data and information models Conceptual Models \u2013 capture high-level business concepts and rules in plain language, creating a shared understanding between business and IT. Logical Models \u2013 refine concepts into entities, attributes, and relationships, ensuring clarity and consistency across domains while remaining technology-agnostic. Physical Models \u2013 implement the design in databases and systems, optimised for performance, scalability, and integration. Domain-level models often align more closely to real-world business semantics and rules than the enterprise-wide model. While they may not map one-to-one with enterprise or other domain models, cross-mapping is essential to identify dependencies, ensure conformance (e.g., shared dimensions, master data), and support integration across the organisation. Example of modelling a Domain-scoped Conceptual Information Model See Bounded context Domain glossary The Domain Glossary complements the Domain Model by describing concepts in clear business language. It defines domain-specific terms, synonyms, and the context in which they are used, along with properties such as attributes, keys, measures, and metrics. A well-curated Domain Glossary: Ensures clarity, reduces ambiguity, and strengthens alignment between business understanding and technical implementation. Builds on the Enterprise Glossary: Extend the enterprise glossary with domain-specific definitions. References when aligned: Where a domain term is synonymous with an enterprise definition, the enterprise glossary should be referenced rather than duplicated. Resolves when conflicting: Where definitions diverge or conflict, governance processes must be applied to reconcile differences and ensure consistency. Example of authoring a Domain-scoped Glossary aligned to the CIM and Enterprise Glossary Example of syncing the Glossary term to real-life Data and Products Domain data and warehouse models Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities. Data layers and stages Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Within each layer, data is transformed through a series of stages. Top level layers follow the Medallion architecture . Bronze: Data according to source. Silver: Data according to business. ( see Data and information models ) Gold: Data according to requirements. Data layers and stages These map to naming standards and conventions for Catalog , Schemas and dbt . Metadata layer Contains engineering and governance of data managed within the platform. The format of this will vary depending on the choice of engineering and governance toolsets and associated metamodels within the solution as well as across the broader enterprise. see Enterprise Metadata Architecture Bronze/ Raw layer: Data according to source The Bronze layer stores raw, immutable data as it is ingested from source systems. The choice of persistence level will depend on requirements. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) / De-duplicated Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes De-duplicated Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing *subject to deduplication Silver/EDW layer: Data according to business The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. These datasets are aligned with broadly accepted business standards and models, making them suitable for a range of downstream requirements. While some interpretations consider Silver to be primarily source-centric , this blueprint adopts a more flexible approach\u2014allowing for integration and harmonization of assets across multiple data sources. Silver/EDW Staging Transformations used to shape source data into standardised, conformed, and fit-for-use Reference Data, Data Vault and Base Information Mart objects. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Data Quality Data quality test results from checks applied to source data. Further transformation of these results may be applied to shape them into data quality reports. Reference Data Reference data, being a common asset and provided for broad consumption should be aligned to standards and downstream needs. Historical versioning requirements of reference data may need to be considered. EDW (base) Marts The term base or edw is used to distinguish these marts from the product/requirement-specific marts found in the Gold layer. Base marts are designed for broad usability across multiple downstream use cases\u2014for example, a conformed customer dimension. Mappings of keys may be created to resolve and map system keys to universal surrogate keys or business keys. These can then be reused downstream for integration. In some scenarios, it may be beneficial to maintain source-centric base marts alongside a final consolidation (UNION) mart\u2014all conforming to a common logical model. This approach supports decoupled pipelining across multiple sources, improving modularity and maintainability. These marts may be implemented as Kimball-style dimensional models or denormalized flat tables, depending on performance and reporting requirements. However, dimensional modelling is generally encouraged for its clarity, reusability, and alignment with analytic workloads. Gold/ Mart layer: Data according to requirements The Gold layer focuses on delivering business-ready datasets, including aggregations and reporting structures that directly reflect specific business requirements. In some instances, Gold assets may be reused across multiple use cases or domains\u2014blurring the line with Silver. While this is not inherently problematic, it is important to consider supportability and scalability to ensure these assets remain trustworthy, maintainable, and accessible over time. Consider shifting logic left into the Silver layer\u2014such as common aggregations, reusable business rules, or conformed dimensions. This improves consistency, reduces duplication, and enables faster development of Gold-layer assets by building on stronger, more standardized foundations. Mart Staging Transformations used to shape source data into business-ready datasets, aligned to business requirements. Examples of Business-specific transformations include: Pivoting Aggregation Joining Conformance Desensitization While dbt best practices use the term 'Intermediates' as reuseable building blocks for marts, this is considered a form of staging and are hence optional under this blueprint. https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Product (Requirement Specific) Marts ** The term 'business' here is use to distinguish marts in this layer from marts in the Silver layer. These marts are designed for a defined requirement. e.g. sales fact aggregated by region. These marts may be Kimball or denormalised flat tables depending on requirements; although Kimball dimensional models are encouraged. A solution served to the consumption layer is likely to contain a mix of Silver and Gold mart objects. e.g: - silver.dim_customer - gold.fact_sales_aggregated_by_region Lakehouse Catalog to Storage Mapping Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Illustrative example of Catalog to storage mapping in the Intuitas Demo Environment: Data Engineering Ingestion Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: Scalable, Resilient, Maintainable, Governed Pattern-based, automated and Metadata-driven where possible Batch and stream-based Example batch ingestion options: Ingestion patterns and notes: Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: databricks lakeflow: source -> lakeflow connect -> ods requires network access to source Pattern 6: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks) Transformation Under development. (Contact us to know more). Batch and Micro-batch SQL transformation dbt see dbt standards Streaming SQL transformation Under development. (Contact us to know more). Non SQL transformation Under development. (Contact us to know more). Data sharing and delivery patterns Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below. Sharing and delivery visualisation channels Pull / direct access Databricks Delta sharing practices Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing ADLSGen2 Access to Data Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing DuckDB Access to Data (via Unity Catalog) Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported SQL Access SQL Access is provided by the Databricks SQL (serverless) endpoint. API Access Under development. (Contact us to know more). The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html Snowflake Access Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial Microsoft Fabric Access The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to import from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Avoided SQL Endpoint costs for reads Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Requires public workspaces Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption Push Under development. (Contact us to know more). Areas for consideration include: adf databricks lakeflow Visualisation Under development. (Contact us to know more). Areas for consideration include: Powerbi Databricks dashboards Apps Open source visual options AI/ML Under development. (Contact us to know more). Areas for consideration include: MLOps Training Databricks Azure ML Data governance This section describes how Enterprise-level governance will be implemented through solutions at the domain level. Data lifecycle and asset management Under development. (Contact us to know more). Areas for consideration include: data contracts and policy data asset tagging Data access management Under development. (Contact us to know more). Areas for consideration include: data access request management data contracts access audit activity audit Data quality Under development. (Contact us to know more). Areas for consideration include: data quality checking and reporting data standards and quality business rule management Data understandability Under development. (Contact us to know more). Areas for consideration include: data lineage data object metadata Privacy Preservation Under development. (Contact us to know more). Areas for consideration include: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names. Row Level Security Under development. (Contact us to know more). Areas for consideration include: dynamic views precomputed views costs and overheads of various patterns of sharing of RLS-applied data Audit Under development. (Contact us to know more). Areas for consideration include: audit log queries Typical observability requirements by role As a Domain (workspace) Admin 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?) Infrastructure Under development. (Contact us to know more). Environments, Workspaces and Storage Workspaces, Environments and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage. Secrets Under development. (Contact us to know more). Areas for consideration include: Management Areas of use Handling practices Storage Lakehouse storage Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod CICD and Repository Under development. (Contact us to know more). Areas for consideration include: Description of git workflows for CICD in terms of: Infrastructure Data engineering Analytics engineering Data science / AIML BI, Reports and other products Tools Under development. (Contact us to know more). Areas for consideration include: Github Azure Devops Databricks Asset Bundles Repositories Under development. (Contact us to know more). Areas for consideration include: Infrastructure IAC repos (Terraform) dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles Observability Tools included in reference architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring dbt observability - Elementary Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation dbt warehouse observability Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention. Networking Areas for consideration include: By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies can be enabled - however consider impacts on private endpoints. Orchestration Under development. (Contact us to know more). Tools included in reference architecture Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt) Security Under development. (Contact us to know more). Tools included in reference architecture Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Level 2"},{"location":"level_2/#level-2-domain-level-solution-architecture-and-patterns","text":"Return to home This section shows how the Enterprise Data Platform Reference Architecture as well as standards are applied within individual domains to create solutions.","title":"Level 2 - Domain-Level (Solution) Architecture and Patterns"},{"location":"level_2/#why-it-matters","text":"Using common patterns and standards at the domain level keeps solutions consistent and compatible. This speeds up delivery, avoids rework, and ensures every solution contributes to and strengthens the overall enterprise \u201ctown plan.\u201d","title":"Why It Matters"},{"location":"level_2/#table-of-contents","text":"Business architecture Business processes Business glossary Business metrics Data Architecture Data and information models Domain glossary Domain data and warehouse models Data layers and stages Lakehouse Catalog to Storage Mapping Data Engineering Ingestion Transformation Data sharing and delivery patterns Data governance Data lifecycle and asset management Data access management Data quality Data understandability Privacy Preservation Audit Infrastructure Environments, Workspaces and Storage Secrets Storage CICD and Repository Observability Networking Orchestration Example Platform and Pipeline Reference Architecture","title":"Table of Contents"},{"location":"level_2/#business-architecture","text":"","title":"Business architecture"},{"location":"level_2/#use-cases-and-business-requirements","text":"Business requirements, use cases, and use case roadmaps together define what a solution must achieve, how it will be applied, and when capabilities will be delivered according to agreed priorities. These priorities may sit at the enterprise, domain level or both. Understanding them is necessary to understand: the outcomes the solution must enable or improve the measures of success, constraints, and scope that shape design how the solution will be used to meet specific business needs the sequence and priority of delivering capabilities to maximise business value","title":"Use cases and business requirements"},{"location":"level_2/#business-processes","text":"Business processes are the activities and tasks undertaken to achieve business goals. Understanding them allows the data architecture to uncover: the context in which information is captured and used key concepts and entities relevant to the domain or use case relationships between processes and data the basis for defining measures and metrics","title":"Business processes"},{"location":"level_2/#business-glossary","text":"A business glossary is a curated list of terms and definitions relevant to the business (at both the Enterprise and Domain levels). Understanding these terms and how they map across the organisation by stakeholders and systems is critical to consistent understanding and usage of concepts. It is core part of Enterprise Metadata Architecture . Example glossary from Intuitas' Glossary builder tool Contact us at \ud83d\udce7 office@intuitas.com to learn more about the tool.","title":"Business glossary"},{"location":"level_2/#business-metrics","text":"Measures are raw, quantifiable values that capture facts about business activities (e.g., total sales, number of customers). Metrics are calculated or derived indicators that assess the performance of business processes, often using one or more measures (e.g., sales growth rate, customer churn rate). Both require capture, at a minimum, of: name definition formula (with reference to concepts and terms as defined in the business glossary) associated dimensions source(s) metric owner frequency","title":"Business metrics"},{"location":"level_2/#data-architecture","text":"Data Architecture defines how data is structured, stored, and accessed. It spans storage design, data models, integration, and governance to ensure trusted, reusable, and consistent data.","title":"Data Architecture"},{"location":"level_2/#data-and-information-models","text":"Conceptual Models \u2013 capture high-level business concepts and rules in plain language, creating a shared understanding between business and IT. Logical Models \u2013 refine concepts into entities, attributes, and relationships, ensuring clarity and consistency across domains while remaining technology-agnostic. Physical Models \u2013 implement the design in databases and systems, optimised for performance, scalability, and integration. Domain-level models often align more closely to real-world business semantics and rules than the enterprise-wide model. While they may not map one-to-one with enterprise or other domain models, cross-mapping is essential to identify dependencies, ensure conformance (e.g., shared dimensions, master data), and support integration across the organisation. Example of modelling a Domain-scoped Conceptual Information Model See Bounded context","title":"Data and information models"},{"location":"level_2/#domain-glossary","text":"The Domain Glossary complements the Domain Model by describing concepts in clear business language. It defines domain-specific terms, synonyms, and the context in which they are used, along with properties such as attributes, keys, measures, and metrics. A well-curated Domain Glossary: Ensures clarity, reduces ambiguity, and strengthens alignment between business understanding and technical implementation. Builds on the Enterprise Glossary: Extend the enterprise glossary with domain-specific definitions. References when aligned: Where a domain term is synonymous with an enterprise definition, the enterprise glossary should be referenced rather than duplicated. Resolves when conflicting: Where definitions diverge or conflict, governance processes must be applied to reconcile differences and ensure consistency. Example of authoring a Domain-scoped Glossary aligned to the CIM and Enterprise Glossary Example of syncing the Glossary term to real-life Data and Products","title":"Domain glossary"},{"location":"level_2/#domain-data-and-warehouse-models","text":"Domain-level data and warehouse models reflect domain-specific scope, requirements and semantics as expressed in models and glossaries. Conformed dimensions may serve as a bridge between domains for common entities.","title":"Domain data and warehouse models"},{"location":"level_2/#data-layers-and-stages","text":"Data and analytics pipelines flow through data layers and stages. Conventions vary across organisations, however the following is an effective approach: Within each layer, data is transformed through a series of stages. Top level layers follow the Medallion architecture . Bronze: Data according to source. Silver: Data according to business. ( see Data and information models ) Gold: Data according to requirements. Data layers and stages These map to naming standards and conventions for Catalog , Schemas and dbt .","title":"Data layers and stages"},{"location":"level_2/#metadata-layer","text":"Contains engineering and governance of data managed within the platform. The format of this will vary depending on the choice of engineering and governance toolsets and associated metamodels within the solution as well as across the broader enterprise. see Enterprise Metadata Architecture","title":"Metadata layer"},{"location":"level_2/#bronze-raw-layer-data-according-to-source","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. The choice of persistence level will depend on requirements. (Persistent) Landing Initial storage area for raw data from source systems. Stores raw events as JSON or CDC/tabular change records. Data is maintained in a primarily raw format, with the possibility of adding extra fields that might be useful later, such as for identifying duplicates. These fields could include the source file name and the load date. Partitioned by load date (YYYY/MM/DD/HH) Raw data preserved in original format Append-only immitable data. Schema changes tracked but not enforced ODS (Operational Data Store) Current state of source system data with latest changes applied. Maintains latest version of each record Supports merge operations for change data capture (CDC) / De-duplicated Preserves source system relationships PDS (Persistent Data Store) Historical storage of all changes over time. Append-only for all changes De-duplicated Supports point-in-time analysis Configurable retention periods As these may be available in landing - may be realised through views over landing *subject to deduplication","title":"Bronze/ Raw layer: Data according to source"},{"location":"level_2/#silveredw-layer-data-according-to-business","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets. These datasets are aligned with broadly accepted business standards and models, making them suitable for a range of downstream requirements. While some interpretations consider Silver to be primarily source-centric , this blueprint adopts a more flexible approach\u2014allowing for integration and harmonization of assets across multiple data sources. Silver/EDW Staging Transformations used to shape source data into standardised, conformed, and fit-for-use Reference Data, Data Vault and Base Information Mart objects. Examples of transformations: 01_renamed_and_typed 02_deduped 03_cleaned 04_filtered/split 05_column_selected 06_business_validated 07_desensitised Data Quality Data quality test results from checks applied to source data. Further transformation of these results may be applied to shape them into data quality reports. Reference Data Reference data, being a common asset and provided for broad consumption should be aligned to standards and downstream needs. Historical versioning requirements of reference data may need to be considered. EDW (base) Marts The term base or edw is used to distinguish these marts from the product/requirement-specific marts found in the Gold layer. Base marts are designed for broad usability across multiple downstream use cases\u2014for example, a conformed customer dimension. Mappings of keys may be created to resolve and map system keys to universal surrogate keys or business keys. These can then be reused downstream for integration. In some scenarios, it may be beneficial to maintain source-centric base marts alongside a final consolidation (UNION) mart\u2014all conforming to a common logical model. This approach supports decoupled pipelining across multiple sources, improving modularity and maintainability. These marts may be implemented as Kimball-style dimensional models or denormalized flat tables, depending on performance and reporting requirements. However, dimensional modelling is generally encouraged for its clarity, reusability, and alignment with analytic workloads.","title":"Silver/EDW layer: Data according to business"},{"location":"level_2/#gold-mart-layer-data-according-to-requirements","text":"The Gold layer focuses on delivering business-ready datasets, including aggregations and reporting structures that directly reflect specific business requirements. In some instances, Gold assets may be reused across multiple use cases or domains\u2014blurring the line with Silver. While this is not inherently problematic, it is important to consider supportability and scalability to ensure these assets remain trustworthy, maintainable, and accessible over time. Consider shifting logic left into the Silver layer\u2014such as common aggregations, reusable business rules, or conformed dimensions. This improves consistency, reduces duplication, and enables faster development of Gold-layer assets by building on stronger, more standardized foundations. Mart Staging Transformations used to shape source data into business-ready datasets, aligned to business requirements. Examples of Business-specific transformations include: Pivoting Aggregation Joining Conformance Desensitization While dbt best practices use the term 'Intermediates' as reuseable building blocks for marts, this is considered a form of staging and are hence optional under this blueprint. https://docs.getdbt.com/best-practices/how-we-structure/3-intermediate Product (Requirement Specific) Marts ** The term 'business' here is use to distinguish marts in this layer from marts in the Silver layer. These marts are designed for a defined requirement. e.g. sales fact aggregated by region. These marts may be Kimball or denormalised flat tables depending on requirements; although Kimball dimensional models are encouraged. A solution served to the consumption layer is likely to contain a mix of Silver and Gold mart objects. e.g: - silver.dim_customer - gold.fact_sales_aggregated_by_region","title":"Gold/ Mart layer: Data according to requirements"},{"location":"level_2/#lakehouse-catalog-to-storage-mapping","text":"Unity catalog objects (catalogs, schemas, objects) are mapped to: Storage accounts Environments (containers: dev, test, prod) Layers (Level 1 folders: dev.bronze, dev.silver, dev.gold, etc) Stages (Level 2 folders: dev.bronze\\landing, dev.bronze\\ods, dev.silver\\base, dev.silver\\staging etc) Illustrative example of Catalog to storage mapping in the Intuitas Demo Environment:","title":"Lakehouse Catalog to Storage Mapping"},{"location":"level_2/#data-engineering","text":"","title":"Data Engineering"},{"location":"level_2/#ingestion","text":"Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Ingestion is the process of acquiring data from external sources and landing it in the platform landing layer. It should be: Scalable, Resilient, Maintainable, Governed Pattern-based, automated and Metadata-driven where possible Batch and stream-based Example batch ingestion options:","title":"Ingestion"},{"location":"level_2/#ingestion-patterns-and-notes","text":"Pattern 1: streaming: kafka -> landing -> databricks autoloader -> ods see repo Bronze Landing to ODS Project Pattern 2: batch: source -> adf -> landing -> databricks autoloader merge to ods see repo Bronze landing SQL Server to ODS Project adf requires azure sql and on-premise integration runtime see repo External Database to ODS Project requires network access to source Pattern 4: batch/streaming: source -> custom python -> deltalake -> external table Pattern 5: databricks lakeflow: source -> lakeflow connect -> ods requires network access to source Pattern 6: sharepoint -> fivetran -> databricks sql warehouse (ods) see repo fivetran Rejected patterns: batch: adf -> deltalake -> ods (does not support unity catalog, requires target tables to be pre-initialised) batch: adf -> databricks sql endpoint -> ods (no linked service for databricks) batch: adf + databricks notebook -> landing, ods, pds (more undesireable coupling of adf and databricks an associated risks)","title":"Ingestion patterns and notes:"},{"location":"level_2/#transformation","text":"Under development. (Contact us to know more).","title":"Transformation"},{"location":"level_2/#batch-and-micro-batch-sql-transformation","text":"dbt see dbt standards","title":"Batch and Micro-batch SQL transformation"},{"location":"level_2/#streaming-sql-transformation","text":"Under development. (Contact us to know more).","title":"Streaming SQL transformation"},{"location":"level_2/#non-sql-transformation","text":"Under development. (Contact us to know more).","title":"Non SQL transformation"},{"location":"level_2/#data-sharing-and-delivery-patterns","text":"Note: These patterns and findings reflect GA functionality only as as at the date of publication and research. Refer to respective product roadmaps and documentation for the latest guidance on functionality. Data can be shared and delivered to consumers through various channels, each differing in: Cost Functionality Scalability Security Maintainability The following subsections offer more details about the channels depicted below. Sharing and delivery visualisation channels","title":"Data sharing and delivery patterns"},{"location":"level_2/#pull-direct-access","text":"","title":"Pull / direct access"},{"location":"level_2/#databricks-delta-sharing-practices","text":"Databricks Delta Sharing allows read-only access directly to data (table, view, change feed) in the lakehouse storage account. This allows for the use of the data in external tools such as BI tools, ETL tools, etc. without the need to use a databricks cluster / sql endpoint. -Permissions: Delta sharing is a feature of Databricks Unity Catalog that requires enablement and authorised user/group permissions for the feature as well as the shared object. Costs: In Delta Sharing, the cost of compute is generally borne by the data consumer, not the data provider. Other costs include storage API calls and data transfer. Naming standards and conventions see naming standards Tightly scope the share as per the principal of least privilege: Share only the necessary data Single purpose, single recipient Granular access control Set an expiry Use audit logging to track access and usage sql SELECT * FROM system.access.audit WHERE action_name LIKE 'deltaSharing%' ORDER BY event_time DESC LIMIT 100; Limitations: No Row Level Security and Masking support (dynamic views required) Reference: Security Best Practices for Delta Sharing","title":"Databricks Delta sharing practices"},{"location":"level_2/#adlsgen2-access-to-data","text":"Provide direct ADLSGen2 access via Managed Identity, SAS or Account Key Note: While technically possible, ADLSGen2 access is not generally recommended for end user consumption as it bypasses the Unity Catalog and associated governance and observabilit controls. Example Scenarios: Direct ADLS file sharing might be preferable in certain cases, even when Delta Sharing is available: Unstructured data Large non-delta file transfer Consumers that don't support Delta Sharing","title":"ADLSGen2 Access to Data"},{"location":"level_2/#duckdb-access-to-data-via-unity-catalog","text":"Example: DuckDB is a popular open-source SQL engine that can be used to access data in the lakehouse. It can be run on a local machine or in-process in a Databricks cluster. Costs: DuckDB data access will incur costs of the underlying compute, storage access, data transfer, etc., similar to Delta Sharing. Example Opportunities/Uses: Last mile analysis SQL interface to Delta, Iceberg, Parquet, CSV, etc. dbt compatibility Local execution and storage of queries and data Use as feed visualization tools, e.g., Apache Superset See repo DuckDB Limitations: Unity Catalog not yet supported Delta Kernel not yet supported","title":"DuckDB Access to Data (via Unity Catalog)"},{"location":"level_2/#sql-access","text":"SQL Access is provided by the Databricks SQL (serverless) endpoint.","title":"SQL Access"},{"location":"level_2/#api-access","text":"Under development. (Contact us to know more). The Databricks SQL Statement Execution API can be used to execute SQL statements on a SQL warehouse and fetch the result. References: https://docs.databricks.com/api/workspace/statementexecution https://docs.databricks.com/en/dev-tools/sql-execution-tutorial.html","title":"API Access"},{"location":"level_2/#snowflake-access","text":"Snowflake access is provided by Databricks Delta Sharing. Snowflake access is also provided by Databricks Delta Lake external tables over ADLSGen2 see external tables References: - tutorial","title":"Snowflake Access"},{"location":"level_2/#microsoft-fabric-access","text":"The following describes options for providing access to Microsoft Fabric / PowerBI Option 1. Share via Delta Sharing Steps: Create a delta share Use the delta share to import from within PowerBI Evaluation: Pros: No duplication Centralised control over access policies Compute costs on consumer Avoided SQL Endpoint costs for reads Cons: Row Level Security and Masking support via dynamic views only See limitations . e.g. The data that the Delta Sharing connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier. Option 2. Directlake via ADLSGen2 Steps: Create a new connection to ADLSGen2 using a provided credential / token / Service principal Create a lakehouse shortcut in Fabric Evaluation: Pros: No duplication Potentially better PowerBI performance (untested) Compute costs on consumer No views Cons: Less control over access policies than Delta Sharing (outside of Unity Catalog) Requires granular ADLSGen2 access controls and service principals, and associated management overhead No Row Level Security and Masking support May require OneLake Option 3. Fabric mirrored unity catalog Steps: Within a Fabric Workspace, create a new item Mirrored Azure Databricks Catalog Enter the Databricks workspace config to create a new connection Evaluation: Pros: No duplication Convenient access to all Databricks Unity Catalog objects (within credential permissions) Cons: not GA or tested service-principal level identity required to enforce permissions Requires public workspaces Option 4. PowerBI Import Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: Potentially the best PowerBI performance and feature completeness Predictable costs on Databricks Cons: Some, but manageable Compute costs on Databricks Option 5. PowerBI DirectQuery Via SQL Endpoint Steps: Databricks documentation Evaluation: Pros: No duplication Unity Catalog Enforced Row Level Security and Masking Cons: High Compute costs on Databricks on every report interaction Likely deprecated in favour of DirectLake Less feature rich that import mode Option 6. Replicate into Fabric Pros: Possibly reduced networking costs (depending on workload and networking topology) Cons: Duplicated data Engineering costs and overheads Latency in data updates (SQL Endpoint lag) Less governance control compared to Unity Catalog No Row Level Security and Masking support Requires use of Onelake and associated CU consumption","title":"Microsoft Fabric Access"},{"location":"level_2/#push","text":"Under development. (Contact us to know more). Areas for consideration include: adf databricks lakeflow","title":"Push"},{"location":"level_2/#visualisation","text":"Under development. (Contact us to know more). Areas for consideration include: Powerbi Databricks dashboards Apps Open source visual options","title":"Visualisation"},{"location":"level_2/#aiml","text":"Under development. (Contact us to know more). Areas for consideration include: MLOps Training Databricks Azure ML","title":"AI/ML"},{"location":"level_2/#data-governance","text":"This section describes how Enterprise-level governance will be implemented through solutions at the domain level.","title":"Data governance"},{"location":"level_2/#data-lifecycle-and-asset-management","text":"Under development. (Contact us to know more). Areas for consideration include: data contracts and policy data asset tagging","title":"Data lifecycle and asset management"},{"location":"level_2/#data-access-management","text":"Under development. (Contact us to know more). Areas for consideration include: data access request management data contracts access audit activity audit","title":"Data access management"},{"location":"level_2/#data-quality","text":"Under development. (Contact us to know more). Areas for consideration include: data quality checking and reporting data standards and quality business rule management","title":"Data quality"},{"location":"level_2/#data-understandability","text":"Under development. (Contact us to know more). Areas for consideration include: data lineage data object metadata","title":"Data understandability"},{"location":"level_2/#privacy-preservation","text":"Under development. (Contact us to know more). Areas for consideration include: row level security data masking column level security data anonymisation data de-identification https://docs.databricks.com/en/tables/row-and-column-filters.html#limitations \"If you want to filter data when you share it using Delta Sharing, you must use dynamic views.\" Use dynamic views if you need to apply transformation logic, such as filters and masks, to read-only tables and if it is acceptable for users to refer to the dynamic views using different names.","title":"Privacy Preservation"},{"location":"level_2/#row-level-security","text":"Under development. (Contact us to know more). Areas for consideration include: dynamic views precomputed views costs and overheads of various patterns of sharing of RLS-applied data","title":"Row Level Security"},{"location":"level_2/#audit","text":"Under development. (Contact us to know more). Areas for consideration include: audit log queries","title":"Audit"},{"location":"level_2/#typical-observability-requirements-by-role","text":"As a Domain (workspace) Admin 1. Where are there misconfigured catalogs / schemas / objects? 2. Who is sharing what to who and is that permitted (as per access approvals?) 3. Who is accessing data and are they permitted (as per access approvals?)","title":"Typical observability requirements by role"},{"location":"level_2/#infrastructure","text":"Under development. (Contact us to know more).","title":"Infrastructure"},{"location":"level_2/#environments-workspaces-and-storage","text":"Workspaces, Environments and Storage This diagram illustrates a data lakehouse architecture with the following components and flow: Data Sources Data originates from multiple sources such as: - Databases - Kafka or event streaming - APIs or Python scripts - SharePoint (or similar sources) Enterprise Engineering Layer Centralized enterprise workspaces are managed here with multiple environments. While work can be achieved within a single workspace and lakehouse storage account, decoupling the workspaces and storage accounts allow for more isolated change at the infrastructure level - in line with engineering requirements: Each workspace contains: Data from prod catalogs can be shared to other domains. Domain-Specific Layer Each domain (e.g., business units or specific applications) operates independently within a single workspace that houses multiple environments. PROD , TEST , and DEV storage containers within a single lakehouse storage account for domain-specific data management. Local Bronze for domain-specific engineering of domain-local data (not managed by enterprise engineering) Data from prod catalogs can be shared to other domains. Data Catalog A centralized data catalog (unity catalog) serves as a metadata repository for the entire architecture: Enables discovery and governance of data. Optional external catalog storage.","title":"Environments, Workspaces and Storage"},{"location":"level_2/#secrets","text":"Under development. (Contact us to know more). Areas for consideration include: Management Areas of use Handling practices","title":"Secrets"},{"location":"level_2/#storage","text":"","title":"Storage"},{"location":"level_2/#lakehouse-storage","text":"Lakehouse data for all environments and layers, by default, share a single storage account with LRS or GRS redundancy. This can then be modified according to costs, requirements, policies, projected workload and resource limits from both Azure and Databricks. Resource: ADLSGen2 Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Lakehouse storage"},{"location":"level_2/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"level_2/#cicd-and-repository","text":"Under development. (Contact us to know more). Areas for consideration include: Description of git workflows for CICD in terms of: Infrastructure Data engineering Analytics engineering Data science / AIML BI, Reports and other products","title":"CICD and Repository"},{"location":"level_2/#tools","text":"Under development. (Contact us to know more). Areas for consideration include: Github Azure Devops Databricks Asset Bundles","title":"Tools"},{"location":"level_2/#repositories","text":"Under development. (Contact us to know more). Areas for consideration include: Infrastructure IAC repos (Terraform) dbt projects (separate for each domain) potential for enterprise level stitching of lineage Data engineering code (separate for each domain) using Databricks Asset Bundles","title":"Repositories"},{"location":"level_2/#observability","text":"Tools included in reference architecture: dbt observability - Elementary Databricks observability - Databricks monitoring dashboards ADF - Native adf monitoring","title":"Observability"},{"location":"level_2/#dbt-observability-elementary","text":"Elementary is a dbt observability tool available in both Open Source and Cloud Service forms. For more information, visit: Elementary Documentation dbt warehouse observability Example observability dashboard for Intuitas Engineering Domain Elementary acts as a health monitor and quality checker for dbt projects by automatically tracking, alerting, and reporting on: Data freshness: Ensures your data is up to date. Volume anomalies: Detects unexpected changes in row counts. Schema changes: Monitors additions, deletions, or modifications of columns. Test results: Checks if your dbt tests are passing or failing. Custom metrics: Allows you to define your own checks. It leverages dbt artifacts (such as run results and sources) to send alerts to Slack, email, or other tools. Additionally, it can automatically generate reports after dbt runs, enabling early detection of issues without manual intervention.","title":"dbt observability - Elementary"},{"location":"level_2/#networking","text":"Areas for consideration include: By default - all resources reside within the same VNet with private endpoints. Service endpoints and policies can be enabled - however consider impacts on private endpoints.","title":"Networking"},{"location":"level_2/#orchestration","text":"Under development. (Contact us to know more).","title":"Orchestration"},{"location":"level_2/#tools-included-in-reference-architecture","text":"Azure Data Factory (if needed) Databricks Workflows (for both databricks and dbt)","title":"Tools included in reference architecture"},{"location":"level_2/#security","text":"Under development. (Contact us to know more).","title":"Security"},{"location":"level_2/#tools-included-in-reference-architecture_1","text":"Azure Entra Azure Key Vault Unity Catalog System access reports","title":"Tools included in reference architecture"},{"location":"modelling_framework/","text":"Intuitas Data Modelling Framework Return to home Updated 5/12/2025 This resource provides a lightweight framework for describing and developing data models. It draws from a range of suggested practices, with references provided where appropriate. Note: See Modelling Standards and Conventions for notation and modelling standards. Table of Contents Core Model Types Conceptual (Information) Models Logical (Information) Models Physical (Data) Models Enterprise Context Domain Topology Domain Models Canonical Models Specialised Model Types Business Process Models Data Warehouse Models Data Vault Model Dimensional (Kimball) Model Dimensional Bus Matrix Semantic Layers Measures and Metrics Master Data and Reference Data Hierarchies Modelling Concepts We distinguish between business information design and technical data implementation: Information Models (Conceptual and Logical): Semantic models focused on business meaning, relationships, and rules. Technology-agnostic, designed for business stakeholders, analysts, and architects. Data Models (Physical): Implementation-specific models focused on storage, performance, and technical constraints. Designed for database developers and engineers. Some organisations use \"data model\" as an umbrella term for all three layers, which is also acceptable. Conceptual (Information) Models For naming standards and conventions, see Conceptual Models in Modelling Standards and Conventions. Conceptual models represent business meaning without implementation concerns. What to Include: Core business concepts (entities) that are identifiable, meaningful, and properly named Key relationships between concepts High-level business rules What to Exclude: Primary or surrogate keys Data types Precise cardinality (beyond \"one\" vs \"many\") Use the business glossary to map domain-specific terms as synonyms to canonical enterprise terms. Subtypes and Supertypes Choosing the Right Level: Too abstract: Not practical (e.g., \"Thing\") Too granular: Should be attributes instead (e.g., \"Customer Type\") Just right: Meaningful specialisations with distinct data requirements (e.g., \"Individual Customer\" vs \"Corporate Customer\") Subtype Rules: Mutually exclusive: Each instance belongs to one subtype only Collectively exhaustive: Desirable but flexible\u2014models evolve over time Avoid multiple classification hierarchies; choose the primary classification dimension. Logical (Information) Models For naming standards and conventions, see Logical Models in Modelling Standards and Conventions. Logical models extend conceptual models with structure and precision while remaining technology-independent. What to Include: Keys and identifiers: Natural and alternate keys Cardinality: Precise relationships (1:1, 1:M, M:N) Optionality: Mandatory vs optional relationships and attributes Attribute definitions: Clear definitions and business rules Normalisation: Applied where relevant for data integrity Physical (Data) Models For naming standards and conventions, see Physical Models in Modelling Standards and Conventions. Physical models implement conceptual and logical models in specific technologies. They focus on storage, performance, and technical constraints for database developers and engineers. Common Physical Formats: Relational (3NF): Traditional normalised models for transactional systems (OLTP). Used in PostgreSQL, SQL Server, Oracle. Dimensional: Star/snowflake schemas optimised for analytics (OLAP). See Dimensional Models . Semi-Structured: JSON, Parquet, Avro supporting nested data. Used in data lakes, MongoDB, Delta Lake, Databricks. Graph: Node/edge structures for relationship-heavy queries. Used in Neo4j, Amazon Neptune. Key-Value: Simple stores optimised for speed and scale. Used in DynamoDB, Cosmos DB, Redis. Note: Physical models often exist without documented conceptual/logical models, but these can be reverse-engineered to understand business meaning. Enterprise Context Enterprise-wide modelling is challenging due to semantic differences across functional areas. Instead, we focus on common touch points using Domain Models and Canonical Models where systems and domains integrate. Domain Topology Refer to Domain-Centric Design Domains represent functional and organisational boundaries. Each domain encapsulates responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while offering value to other domains and the enterprise. A Domain Topology maps domains across the enterprise and their relationships, typically organised by functional areas (though other dimensions like region may apply). Illustative example domains for a Healthcare organisation Domain Models For entity naming conventions in domain models, see Entity Naming in Modelling Standards and Conventions. Under Domain-Centric Design, domains are authoritative for their information definitions. They own concept meanings, define lifecycle rules, and act as the system of semantic truth for their scope. A Domain Model describes business concepts relevant to a domain, including: Business Objects: Operational entities with identity, lifecycle, and direct participation in business processes (e.g., Patient, Order, Product) Supporting Concepts: Qualities, rules, conditions, and classifications (e.g., Risk, Compliance, Status) Relationship to Business Processes: Domain Models define WHAT (objects, attributes, relationships) Business Process Models define HOW (creation, transformation, consumption) Processes reveal which objects matter, expose lifecycles, uncover rules, and clarify business language Clear domain models enable process design through shared vocabulary Terminology: We prefer \"Domain\" over \"Subject Area\" as it ties to governance and ownership boundaries, supporting clear accountability for data products and outcomes. Canonical Models For canonical entity naming conventions, see Entity Naming in Modelling Standards and Conventions. Canonical models represent authoritative, agreed-upon definitions for concepts standardised across domains and systems (e.g., APIs, interoperability, conformed dimensions). Characteristics: Conceptual and logical at minimum; physical where applicable Represent standardised information contracts Support interoperability and consistent interpretation Use canonical models where: Multiple domains integrate Semantic consistency is required Cross-domain processes exchange information Role in Cross-Domain Processes: Define integration points and standardised entities (e.g., canonical \"Customer\") Enable process flows across domains (e.g., Sales \u2192 Fulfilment \u2192 Finance) Document data contracts at each handoff point Specialised Model Types Business Process Models Business process models illustrate activity sequences, decision points, participants, and work flows within and across domains. They define business objects (e.g., Customer, Order, Invoice) that form the basis of conceptual models within Domain Models and Fact Tables and Business Processes in Dimensional Modeling . Data Warehouse Models Data warehouses transform data through stages optimised for performance and analysis while preserving conceptual and logical semantics: Conform to canonical standards Apply reference data and business terminology Ensure data quality and reliability Preserve history for time-based analysis Enrich with metadata for discovery and governance Optimise for consumption ( dimensional , data vault ) Aggregate for analytical use Each stage has distinct physical structures aligned to underlying logical models. Data Vault Model Data Vault models centre on business concepts and associations from Enterprise/Domain Conceptual Models. Refer to Data Vault 2.0 standards for guidance. Dimensional (Kimball) Model For dimensional model naming standards, SCD columns, and key conventions, see Dimensional Models in Modelling Standards and Conventions. Kimball Dimensional Modelling creates Information Marts using Star Schemas : Fact tables: Quantitative metrics/events from business processes (sales, shipments, payments) Dimension tables: Descriptive context (customer, product, date, region) Intentionally denormalised for query performance and user-friendly analysis. Conformed Dimensions: Conformed dimensions are physical implementations of canonical models in Dimensional models: Ensure consistent definitions across fact tables and marts (e.g., shared Customer dimension across Sales, Support, Marketing) Enable cross-process analysis and drill-across queries Achieve \"single version of truth\" across analytical systems Dimensional Bus Matrix A Bus Matrix maps intersections between business processes (facts) and dimensions for scalable warehouse design. Structure: Rows: Business processes (Orders, Shipments, Payments) Columns: Dimensions (Customer, Product, Time, Geography) Cells: \u2713 indicates dimension usage Benefits: Identifies common/conformed dimensions for consistency Supports modular, independent implementation Enables cross-process analytics Example: Process \\ Dimension Customer Product Time Orders \u2713 \u2713 \u2713 Shipments \u2713 \u2713 \u2713 Guides star schema implementation and dimension standardisation. Semantic Layers For business-facing naming standards used in semantic layers, see Business Names vs Physical Names in Modelling Standards and Conventions. Semantic layers are business-oriented abstraction layers that present data in business terms rather than physical structures. Purpose: Translate physical structures into business concepts Encapsulate business logic, measures, and calculations Provide consistent metric definitions for analytics Simplify access for reporting and self-service use Examples: Power BI semantic models, dbt Semantic Layer Semantic layers depend on domain and canonical models for base entities but establish authoritative definitions for derived metrics within their consumption context. Measures and Metrics For measures and metrics naming standards and suffix conventions, see Measures and Metrics and Quantities & Measures in Modelling Standards and Conventions. Measures and metrics are first-class semantic layer concepts that quantify business performance. Measures: Numeric values directly aggregated from data (e.g., Sales Amount , Bed Days ) Derived from fact events/transactions at a clear grain (per encounter, per day) Aggregated using simple functions (SUM, COUNT, AVG) Sensitive to filter context (time, domain, segment) Metrics: Business expressions combining measures (e.g., Readmission Rate , Gross Margin % ) Express performance or quality vs raw volume Often use complex logic (conditionals, windows, exclusions) Require clear definitions and assumptions Additivity: Classify measures by aggregation behaviour: Additive: Sum across all dimensions (e.g., Sales Amount ) Semi-additive: Sum across some dimensions only (e.g., Inventory Level - products yes, time no) Non-additive: Cannot sum meaningfully (e.g., ratios, rates) Metrics inherit these behaviours. Record additivity as metadata to constrain aggregations in marts and BI tools. Logical Definition: Measures/metrics should be defined against domain and canonical models and documented in the business glossary with: Name, description, owning domain, stakeholders Formal definition and calculation rule Input entities, attributes, filters Validity period and version history Related KPIs, dashboards, reports Physical Implementation: Fact tables: Base measures as columns (e.g., amount , quantity ); sometimes pre-aggregated metrics dbt models: Centralised, version-controlled definitions aligned to glossary BI tools (Power BI): Expressions (DAX) aligned with central definitions; push complex logic upstream when possible Placement Principles: Favour upstream (mart / dbt layer) when: Reused across dashboards/domains Enterprise KPI or regulatory use Complex business rules requiring testing Needs governance and discoverability Allow BI-layer-only when: Experimental or single-report scope Simple presentation variant Clearly not the system of record Example: Base measure Total Encounters defined in dbt. Power BI DAX adds interactive filter-aware metric: Readmission Rate (Current Filters) = DIVIDE([Readmission Count], [Total Encounters]) Governance: Govern like reference data with review/approval process Retain effective dates and versions Notify consumers of changes Make discoverable via catalog/glossary (calculation, dependencies, usage) Master Data and Reference Data For reference data naming standards, logical modelling, and physical implementation conventions, see Reference Data in Modelling Standards and Conventions. Master Data: Core business entities critical to operations and shared across systems (e.g., Patient, Product, Provider, Location) Managed as authoritative \"single source of truth\" Subject to governance and quality processes Forms backbone of conformed dimensions in data warehousing Reference Data: Stable, standardised values for categorisation and validation (e.g., Status codes, Country codes, Product Categories) May be externally standardised (ISO) or internally maintained Can be simple (code/name pairs) or hierarchical (see Hierarchies ) Managed enterprise-wide with optional source-specific variants Logical Representation Master Data: Patient, Product, Provider, Location, Organisation, Payer Reference Data: Status Code, Country Code, Product Category, Location Type, Therapeutic Class, Unit of Measure Physical Implementation Physical form depends on analytical use, query patterns, history requirements, complexity, and performance needs. Physical Forms: 1. Simple Lookup Tables Basic code-to-description translation Minimal attributes (code, name, description) No history tracking Example: ref_status_code , ref_currency 2. Dimension Tables (Star Schema) Rich attributes and hierarchies History tracking via SCD (Slowly Changing Dimensions) Used for slicing, filtering, grouping Example: dim_product , dim_provider , dim_location 3. Embedded in Fact Tables (Denormalised) Attributes embedded directly for performance Trades storage for speed (avoids joins) Example: Fact includes status_code , status_name columns For reference data modelling standards, see Reference Data Standards and Conventions . Hierarchies Hierarchies are structured parent-child relationships representing natural groupings and aggregation levels. Across Model Types: Conceptual/Logical: Business classifications and organisational structures (product categories, regions, org units) Physical: Implemented via self-referencing foreign keys, hierarchy tables, or path encodings Dimensional: Enable drill-down/roll-up analysis (Day \u2192 Month \u2192 Quarter \u2192 Year) Reference data: Standardised classification schemes Modelling Hierarchies: Subtypes vs. Relationships Use Subtypes for \"Is A\" Taxonomies: Type distinctions with mutually exclusive subtypes Example: Individual Customer is a Customer; Inpatient Encounter is an Encounter Primary classification dimension for fundamentally different variants Use Relationships for Other Hierarchies: Compositional (Part Of), categorical (Belongs To), organisational (Reports To) Support multiple concurrent hierarchies Example: Location entities (Room, Ward, Building, Campus) related via \"Part Of\" Examples Relationship Hierarchy: Healthcare Facility Structure Hospital Campus \u251c\u2500\u2500 Building A \u2502 \u251c\u2500\u2500 Ward 1 \u2192 Room 101, Room 102 \u2502 \u2514\u2500\u2500 Ward 2 \u2192 Room 201 \u2514\u2500\u2500 Building B \u2514\u2500\u2500 Outpatient Clinic \u2192 Consultation Room 1 Physical Representations: 1. Parent-Child (Adjacency List): Each record references its parent for recursive traversal. location_id location_name location_type parent_location_id 1 Hospital Campus Facility NULL 2 Building A Facility 1 3 Ward 1 Ward 2 4 Room 101 Room 3 5 Room 102 Room 3 2. Flattened Dimension: Lowest-grain entity with hierarchy level columns. room_key campus_name building_name ward_name room_name effective_from_datetime effective_to_datetime updated_datetime 1001 Hospital Campus Building A Ward 1 Room 101 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 1002 Hospital Campus Building A Ward 1 Room 102 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 Adjacency lists enable flexible navigation; flattened structures optimise star schema query performance. Reference Data Hierarchy: Product Classification All Products \u251c\u2500\u2500 Medical Equipment \u2502 \u251c\u2500\u2500 Diagnostic Equipment \u2192 Imaging Systems (MRI, CT Scanners) \u2502 \u2514\u2500\u2500 Therapeutic Equipment \u2514\u2500\u2500 Pharmaceuticals \u251c\u2500\u2500 Prescription Drugs \u2514\u2500\u2500 Over-the-Counter Physical Representations: 1. Parent-Child (Adjacency List): product_classification_id classification_name parent_classification_id 1 All Products NULL 2 Medical Equipment 1 3 Diagnostic Equipment 2 4 Imaging Systems 3 5 MRI Scanners 4 2. Flattened Dimension: product_key classification_lvl1 classification_lvl2 classification_lvl3 classification_lvl4 effective_from_datetime effective_to_datetime updated_datetime 100 Medical Equipment Diagnostic Equipment Imaging Systems MRI Scanners 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 101 Medical Equipment Diagnostic Equipment Imaging Systems CT Scanners 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 102 Pharmaceuticals Prescription Drugs 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 Note: Naming follows modelling standards : surrogate keys use _key suffix; SCD Type 2 uses effective_from_datetime , effective_to_datetime , updated_datetime ; entities are singular; hierarchy attributes use logical prefixes with incremental numbering.","title":"Models"},{"location":"modelling_framework/#intuitas-data-modelling-framework","text":"Return to home Updated 5/12/2025 This resource provides a lightweight framework for describing and developing data models. It draws from a range of suggested practices, with references provided where appropriate. Note: See Modelling Standards and Conventions for notation and modelling standards.","title":"Intuitas Data Modelling Framework"},{"location":"modelling_framework/#table-of-contents","text":"Core Model Types Conceptual (Information) Models Logical (Information) Models Physical (Data) Models Enterprise Context Domain Topology Domain Models Canonical Models Specialised Model Types Business Process Models Data Warehouse Models Data Vault Model Dimensional (Kimball) Model Dimensional Bus Matrix Semantic Layers Measures and Metrics Master Data and Reference Data Hierarchies","title":"Table of Contents"},{"location":"modelling_framework/#modelling-concepts","text":"We distinguish between business information design and technical data implementation: Information Models (Conceptual and Logical): Semantic models focused on business meaning, relationships, and rules. Technology-agnostic, designed for business stakeholders, analysts, and architects. Data Models (Physical): Implementation-specific models focused on storage, performance, and technical constraints. Designed for database developers and engineers. Some organisations use \"data model\" as an umbrella term for all three layers, which is also acceptable.","title":"Modelling Concepts"},{"location":"modelling_framework/#conceptual-information-models","text":"For naming standards and conventions, see Conceptual Models in Modelling Standards and Conventions. Conceptual models represent business meaning without implementation concerns. What to Include: Core business concepts (entities) that are identifiable, meaningful, and properly named Key relationships between concepts High-level business rules What to Exclude: Primary or surrogate keys Data types Precise cardinality (beyond \"one\" vs \"many\") Use the business glossary to map domain-specific terms as synonyms to canonical enterprise terms.","title":"Conceptual (Information) Models"},{"location":"modelling_framework/#subtypes-and-supertypes","text":"Choosing the Right Level: Too abstract: Not practical (e.g., \"Thing\") Too granular: Should be attributes instead (e.g., \"Customer Type\") Just right: Meaningful specialisations with distinct data requirements (e.g., \"Individual Customer\" vs \"Corporate Customer\") Subtype Rules: Mutually exclusive: Each instance belongs to one subtype only Collectively exhaustive: Desirable but flexible\u2014models evolve over time Avoid multiple classification hierarchies; choose the primary classification dimension.","title":"Subtypes and Supertypes"},{"location":"modelling_framework/#logical-information-models","text":"For naming standards and conventions, see Logical Models in Modelling Standards and Conventions. Logical models extend conceptual models with structure and precision while remaining technology-independent. What to Include: Keys and identifiers: Natural and alternate keys Cardinality: Precise relationships (1:1, 1:M, M:N) Optionality: Mandatory vs optional relationships and attributes Attribute definitions: Clear definitions and business rules Normalisation: Applied where relevant for data integrity","title":"Logical (Information) Models"},{"location":"modelling_framework/#physical-data-models","text":"For naming standards and conventions, see Physical Models in Modelling Standards and Conventions. Physical models implement conceptual and logical models in specific technologies. They focus on storage, performance, and technical constraints for database developers and engineers. Common Physical Formats: Relational (3NF): Traditional normalised models for transactional systems (OLTP). Used in PostgreSQL, SQL Server, Oracle. Dimensional: Star/snowflake schemas optimised for analytics (OLAP). See Dimensional Models . Semi-Structured: JSON, Parquet, Avro supporting nested data. Used in data lakes, MongoDB, Delta Lake, Databricks. Graph: Node/edge structures for relationship-heavy queries. Used in Neo4j, Amazon Neptune. Key-Value: Simple stores optimised for speed and scale. Used in DynamoDB, Cosmos DB, Redis. Note: Physical models often exist without documented conceptual/logical models, but these can be reverse-engineered to understand business meaning.","title":"Physical (Data) Models"},{"location":"modelling_framework/#enterprise-context","text":"Enterprise-wide modelling is challenging due to semantic differences across functional areas. Instead, we focus on common touch points using Domain Models and Canonical Models where systems and domains integrate.","title":"Enterprise Context"},{"location":"modelling_framework/#domain-topology","text":"Refer to Domain-Centric Design Domains represent functional and organisational boundaries. Each domain encapsulates responsibilities, services, processes, information, expertise, and governance. Domains serve their own objectives while offering value to other domains and the enterprise. A Domain Topology maps domains across the enterprise and their relationships, typically organised by functional areas (though other dimensions like region may apply). Illustative example domains for a Healthcare organisation","title":"Domain Topology"},{"location":"modelling_framework/#domain-models","text":"For entity naming conventions in domain models, see Entity Naming in Modelling Standards and Conventions. Under Domain-Centric Design, domains are authoritative for their information definitions. They own concept meanings, define lifecycle rules, and act as the system of semantic truth for their scope. A Domain Model describes business concepts relevant to a domain, including: Business Objects: Operational entities with identity, lifecycle, and direct participation in business processes (e.g., Patient, Order, Product) Supporting Concepts: Qualities, rules, conditions, and classifications (e.g., Risk, Compliance, Status) Relationship to Business Processes: Domain Models define WHAT (objects, attributes, relationships) Business Process Models define HOW (creation, transformation, consumption) Processes reveal which objects matter, expose lifecycles, uncover rules, and clarify business language Clear domain models enable process design through shared vocabulary Terminology: We prefer \"Domain\" over \"Subject Area\" as it ties to governance and ownership boundaries, supporting clear accountability for data products and outcomes.","title":"Domain Models"},{"location":"modelling_framework/#canonical-models","text":"For canonical entity naming conventions, see Entity Naming in Modelling Standards and Conventions. Canonical models represent authoritative, agreed-upon definitions for concepts standardised across domains and systems (e.g., APIs, interoperability, conformed dimensions). Characteristics: Conceptual and logical at minimum; physical where applicable Represent standardised information contracts Support interoperability and consistent interpretation Use canonical models where: Multiple domains integrate Semantic consistency is required Cross-domain processes exchange information Role in Cross-Domain Processes: Define integration points and standardised entities (e.g., canonical \"Customer\") Enable process flows across domains (e.g., Sales \u2192 Fulfilment \u2192 Finance) Document data contracts at each handoff point","title":"Canonical Models"},{"location":"modelling_framework/#specialised-model-types","text":"","title":"Specialised Model Types"},{"location":"modelling_framework/#business-process-models","text":"Business process models illustrate activity sequences, decision points, participants, and work flows within and across domains. They define business objects (e.g., Customer, Order, Invoice) that form the basis of conceptual models within Domain Models and Fact Tables and Business Processes in Dimensional Modeling .","title":"Business Process Models"},{"location":"modelling_framework/#data-warehouse-models","text":"Data warehouses transform data through stages optimised for performance and analysis while preserving conceptual and logical semantics: Conform to canonical standards Apply reference data and business terminology Ensure data quality and reliability Preserve history for time-based analysis Enrich with metadata for discovery and governance Optimise for consumption ( dimensional , data vault ) Aggregate for analytical use Each stage has distinct physical structures aligned to underlying logical models.","title":"Data Warehouse Models"},{"location":"modelling_framework/#data-vault-model","text":"Data Vault models centre on business concepts and associations from Enterprise/Domain Conceptual Models. Refer to Data Vault 2.0 standards for guidance.","title":"Data Vault Model"},{"location":"modelling_framework/#dimensional-kimball-model","text":"For dimensional model naming standards, SCD columns, and key conventions, see Dimensional Models in Modelling Standards and Conventions. Kimball Dimensional Modelling creates Information Marts using Star Schemas : Fact tables: Quantitative metrics/events from business processes (sales, shipments, payments) Dimension tables: Descriptive context (customer, product, date, region) Intentionally denormalised for query performance and user-friendly analysis. Conformed Dimensions: Conformed dimensions are physical implementations of canonical models in Dimensional models: Ensure consistent definitions across fact tables and marts (e.g., shared Customer dimension across Sales, Support, Marketing) Enable cross-process analysis and drill-across queries Achieve \"single version of truth\" across analytical systems","title":"Dimensional (Kimball) Model"},{"location":"modelling_framework/#dimensional-bus-matrix","text":"A Bus Matrix maps intersections between business processes (facts) and dimensions for scalable warehouse design. Structure: Rows: Business processes (Orders, Shipments, Payments) Columns: Dimensions (Customer, Product, Time, Geography) Cells: \u2713 indicates dimension usage Benefits: Identifies common/conformed dimensions for consistency Supports modular, independent implementation Enables cross-process analytics Example: Process \\ Dimension Customer Product Time Orders \u2713 \u2713 \u2713 Shipments \u2713 \u2713 \u2713 Guides star schema implementation and dimension standardisation.","title":"Dimensional Bus Matrix"},{"location":"modelling_framework/#semantic-layers","text":"For business-facing naming standards used in semantic layers, see Business Names vs Physical Names in Modelling Standards and Conventions. Semantic layers are business-oriented abstraction layers that present data in business terms rather than physical structures. Purpose: Translate physical structures into business concepts Encapsulate business logic, measures, and calculations Provide consistent metric definitions for analytics Simplify access for reporting and self-service use Examples: Power BI semantic models, dbt Semantic Layer Semantic layers depend on domain and canonical models for base entities but establish authoritative definitions for derived metrics within their consumption context.","title":"Semantic Layers"},{"location":"modelling_framework/#measures-and-metrics","text":"For measures and metrics naming standards and suffix conventions, see Measures and Metrics and Quantities & Measures in Modelling Standards and Conventions. Measures and metrics are first-class semantic layer concepts that quantify business performance. Measures: Numeric values directly aggregated from data (e.g., Sales Amount , Bed Days ) Derived from fact events/transactions at a clear grain (per encounter, per day) Aggregated using simple functions (SUM, COUNT, AVG) Sensitive to filter context (time, domain, segment) Metrics: Business expressions combining measures (e.g., Readmission Rate , Gross Margin % ) Express performance or quality vs raw volume Often use complex logic (conditionals, windows, exclusions) Require clear definitions and assumptions Additivity: Classify measures by aggregation behaviour: Additive: Sum across all dimensions (e.g., Sales Amount ) Semi-additive: Sum across some dimensions only (e.g., Inventory Level - products yes, time no) Non-additive: Cannot sum meaningfully (e.g., ratios, rates) Metrics inherit these behaviours. Record additivity as metadata to constrain aggregations in marts and BI tools. Logical Definition: Measures/metrics should be defined against domain and canonical models and documented in the business glossary with: Name, description, owning domain, stakeholders Formal definition and calculation rule Input entities, attributes, filters Validity period and version history Related KPIs, dashboards, reports Physical Implementation: Fact tables: Base measures as columns (e.g., amount , quantity ); sometimes pre-aggregated metrics dbt models: Centralised, version-controlled definitions aligned to glossary BI tools (Power BI): Expressions (DAX) aligned with central definitions; push complex logic upstream when possible Placement Principles: Favour upstream (mart / dbt layer) when: Reused across dashboards/domains Enterprise KPI or regulatory use Complex business rules requiring testing Needs governance and discoverability Allow BI-layer-only when: Experimental or single-report scope Simple presentation variant Clearly not the system of record Example: Base measure Total Encounters defined in dbt. Power BI DAX adds interactive filter-aware metric: Readmission Rate (Current Filters) = DIVIDE([Readmission Count], [Total Encounters]) Governance: Govern like reference data with review/approval process Retain effective dates and versions Notify consumers of changes Make discoverable via catalog/glossary (calculation, dependencies, usage)","title":"Measures and Metrics"},{"location":"modelling_framework/#master-data-and-reference-data","text":"For reference data naming standards, logical modelling, and physical implementation conventions, see Reference Data in Modelling Standards and Conventions. Master Data: Core business entities critical to operations and shared across systems (e.g., Patient, Product, Provider, Location) Managed as authoritative \"single source of truth\" Subject to governance and quality processes Forms backbone of conformed dimensions in data warehousing Reference Data: Stable, standardised values for categorisation and validation (e.g., Status codes, Country codes, Product Categories) May be externally standardised (ISO) or internally maintained Can be simple (code/name pairs) or hierarchical (see Hierarchies ) Managed enterprise-wide with optional source-specific variants","title":"Master Data and Reference Data"},{"location":"modelling_framework/#logical-representation","text":"Master Data: Patient, Product, Provider, Location, Organisation, Payer Reference Data: Status Code, Country Code, Product Category, Location Type, Therapeutic Class, Unit of Measure","title":"Logical Representation"},{"location":"modelling_framework/#physical-implementation","text":"Physical form depends on analytical use, query patterns, history requirements, complexity, and performance needs. Physical Forms: 1. Simple Lookup Tables Basic code-to-description translation Minimal attributes (code, name, description) No history tracking Example: ref_status_code , ref_currency 2. Dimension Tables (Star Schema) Rich attributes and hierarchies History tracking via SCD (Slowly Changing Dimensions) Used for slicing, filtering, grouping Example: dim_product , dim_provider , dim_location 3. Embedded in Fact Tables (Denormalised) Attributes embedded directly for performance Trades storage for speed (avoids joins) Example: Fact includes status_code , status_name columns For reference data modelling standards, see Reference Data Standards and Conventions .","title":"Physical Implementation"},{"location":"modelling_framework/#hierarchies","text":"Hierarchies are structured parent-child relationships representing natural groupings and aggregation levels. Across Model Types: Conceptual/Logical: Business classifications and organisational structures (product categories, regions, org units) Physical: Implemented via self-referencing foreign keys, hierarchy tables, or path encodings Dimensional: Enable drill-down/roll-up analysis (Day \u2192 Month \u2192 Quarter \u2192 Year) Reference data: Standardised classification schemes","title":"Hierarchies"},{"location":"modelling_framework/#modelling-hierarchies-subtypes-vs-relationships","text":"Use Subtypes for \"Is A\" Taxonomies: Type distinctions with mutually exclusive subtypes Example: Individual Customer is a Customer; Inpatient Encounter is an Encounter Primary classification dimension for fundamentally different variants Use Relationships for Other Hierarchies: Compositional (Part Of), categorical (Belongs To), organisational (Reports To) Support multiple concurrent hierarchies Example: Location entities (Room, Ward, Building, Campus) related via \"Part Of\"","title":"Modelling Hierarchies: Subtypes vs. Relationships"},{"location":"modelling_framework/#examples","text":"Relationship Hierarchy: Healthcare Facility Structure Hospital Campus \u251c\u2500\u2500 Building A \u2502 \u251c\u2500\u2500 Ward 1 \u2192 Room 101, Room 102 \u2502 \u2514\u2500\u2500 Ward 2 \u2192 Room 201 \u2514\u2500\u2500 Building B \u2514\u2500\u2500 Outpatient Clinic \u2192 Consultation Room 1 Physical Representations: 1. Parent-Child (Adjacency List): Each record references its parent for recursive traversal. location_id location_name location_type parent_location_id 1 Hospital Campus Facility NULL 2 Building A Facility 1 3 Ward 1 Ward 2 4 Room 101 Room 3 5 Room 102 Room 3 2. Flattened Dimension: Lowest-grain entity with hierarchy level columns. room_key campus_name building_name ward_name room_name effective_from_datetime effective_to_datetime updated_datetime 1001 Hospital Campus Building A Ward 1 Room 101 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 1002 Hospital Campus Building A Ward 1 Room 102 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 Adjacency lists enable flexible navigation; flattened structures optimise star schema query performance. Reference Data Hierarchy: Product Classification All Products \u251c\u2500\u2500 Medical Equipment \u2502 \u251c\u2500\u2500 Diagnostic Equipment \u2192 Imaging Systems (MRI, CT Scanners) \u2502 \u2514\u2500\u2500 Therapeutic Equipment \u2514\u2500\u2500 Pharmaceuticals \u251c\u2500\u2500 Prescription Drugs \u2514\u2500\u2500 Over-the-Counter Physical Representations: 1. Parent-Child (Adjacency List): product_classification_id classification_name parent_classification_id 1 All Products NULL 2 Medical Equipment 1 3 Diagnostic Equipment 2 4 Imaging Systems 3 5 MRI Scanners 4 2. Flattened Dimension: product_key classification_lvl1 classification_lvl2 classification_lvl3 classification_lvl4 effective_from_datetime effective_to_datetime updated_datetime 100 Medical Equipment Diagnostic Equipment Imaging Systems MRI Scanners 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 101 Medical Equipment Diagnostic Equipment Imaging Systems CT Scanners 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 102 Pharmaceuticals Prescription Drugs 2024-01-01 00:00:00 NULL 2024-01-01 00:00:00 Note: Naming follows modelling standards : surrogate keys use _key suffix; SCD Type 2 uses effective_from_datetime , effective_to_datetime , updated_datetime ; entities are singular; hierarchy attributes use logical prefixes with incremental numbering.","title":"Examples"},{"location":"modelling_standards_and_conventions/","text":"Intuitas Data Modelling Standards and Conventions Return to home Updated 5/12/2025 Related Documents: This document focuses on naming conventions and standards. For platform-specific implementation patterns and architectural details, see Standards and Conventions . For conceptual framework and model types, see Modelling Framework . Table of Contents General Naming Conventions Entity Naming (General Rules) Business Names vs Physical Names Events and Transactions Relationship Naming Conventions Relationship Directionality Relationship Cardinality Conceptual Models Entities Diagrammatic Representation Logical Models Entities Attributes Measures and Metrics Keys (Logical Models) Diagrammatic Representation Physical Models Relational Structures Naming Diagrammatic Representation Databricks Conventions Dimensional Models Data Warehouse Keys Key resolution Business Keys Surrogate Keys Type 2 Fact Resolution Databricks Surrogate Keys dbt Surrogate Keys Reference Data Logical Modelling Physical Implementation Standard Suffix and Prefix Inventory Identity & Keys Temporal Concepts State & Classification Quantities & Measures Boolean Indicators (Prefix Convention) Audit & Control Summary of Naming Rules General Naming Conventions Entity Naming (General Rules) Use clear business language Use terminology familiar to business stakeholders and consistent with the business glossary In domain models, use terms appropriate to the business context and document synonyms in the business glossary Canonical entities must use terms that are formally agreed and recognised across domains Use singular nouns for entities (e.g., Customer , not Customers ) Plurality is allowed only in dimensional fact tables (e.g., fact_payments ). Dimension tables use singular (e.g., dim_customer ) Avoid system-specific or technical terminology Avoid abbreviations unless universally understood Be specific: add context where needed (e.g., Admin User vs User ) Business Names vs Physical Names Business names use natural language with spaces (e.g., Order Placed ) Business names are authoritative for semantic meaning Machine-safe identifiers (e.g., GUIDs) are internal metadata only Physical names follow platform-specific conventions (e.g., order_placed ) The business glossary is the semantic source of truth Example: Business Name: Order Placed Internal Identifier: GUID-1234-5678 Physical Name: order_placed Events and Transactions Business events and transactions may be modelled in two valid ways: Model the entity itself, treating its lifecycle state as an attribute (e.g., Order with Order Status as an attribute) Model the entity in a state-specific form where the status is intrinsic to the concept (e.g., Order Placed , Invoice Issued ) Choose the approach that best represents how the business actually thinks and works with the concept. Relationship Naming Conventions Use business verbs, not technical terms Names must read as a business sentence Use present tense and active voice Avoid vague terms (e.g., \"related to\", \"linked to\") Choose precise verbs that express meaning Reflect ownership or composition explicitly where applicable Use domain-specific language Direction should be clear Examples: Customer places Order Policy covers Claim Doctor refers Patient Employer employs Worker Patient receives Treatment Supplier delivers Product Avoid: has_fk references linked to related to Relationship Directionality Every relationship has one authoritative direction and name Model one semantic relationship with one verb. Allow alternate read-paths in tooling or documentation, not duplicate relationships Do not create passive inversions (e.g., \"is placed by\" instead of \"places\") If you need a different semantic perspective, model it as a separate relationship Relationship Cardinality There are two options: Use crow's foot notation (shows 'one' with a line, 'many' with a branching crow's foot; commonly denotes relationships such as one-to-many, many-to-many, etc.) Use multiplicity notation (explicitly labels relationship ends with numbers or ranges, e.g., '0..1', '1', '0.. ', '1.. ', indicating how many entities participate in the relationship) Single direction (preferred): Customer places Order \u2713 Avoid passive inversion (same relationship, just reversed grammar): Order is placed by Customer \u2717 Separate relationships (different business concepts): Customer places Order (ordering action) Customer is billed for Order (financial relationship) Conceptual Models For conceptual modelling framework and principles, see Conceptual Models in the Modelling Framework. Entities (Conceptual) Business-Facing format with capitalised first letters and spaces (e.g., Order Placed , Customer , Patient Encounter ) Optimised for clarity and business communication Diagrammatic Representation (Conceptual) Boxes represent business concepts (entities) Lines represent relationships Show: Core concepts Key relationships Major business rules Do not show: Keys Data types Technical constraints Detailed cardinality beyond simple one-to-many Logical Models For logical modelling framework and principles, see Logical Models in the Modelling Framework. Entities (Logical) Use the same format as conceptual models Business-Facing format with capitalised first letters and spaces (e.g., Order Placed , Customer , Patient Encounter ) Optimised for clarity and business communication Attributes Use Business-Facing format with capitalised first letters and spaces Names must be business-meaningful Avoid embedding data types in names Prefer semantic names over technical ones Measures and Metrics For measures and metrics framework, governance, and placement principles, see Measures and Metrics in the Modelling Framework. Use Business-Facing format with capitalised first letters and spaces (e.g., Invoice Amount , Item Count ) Make names consistent with related entities and attributes Use semantic suffixes Quantities & Measures in the Standard Suffix Inventory. Example: Measure: Total Sales Amount \u2014 aggregated sum of sales transactions Metric: Average Order Value \u2014 calculated as Total Sales Amount / Order Count Important: Keep metric names dimension-agnostic : Avoid including dimension references like \"by Product\" or \"by Region\" in metric names\u2014the dimensional model handles slicing (e.g., use Sales Amount not Sales Amount by Product by Time ). Exception\u2014Intrinsic dimensions : Include dimensional qualifiers only when the dimension is intrinsic to the calculation logic and defines how the metric works (e.g., Customer Lifetime Value is calculated at customer grain by definition; Monthly Revenue Growth Rate compares month-to-month by definition). Temporal metrics : Include time window qualifiers when the period defines the calculation logic (e.g., Year-to-Date Sales Amount accumulates from year start; 90-Day Rolling Average uses a 90-day window). Aggregation behavior : Averages and ratios cannot be summed across periods; they must be recalculated at each grain. BI tools like Power BI default to SUM, producing incorrect results for non-additive metrics\u2014define these explicitly with appropriate DAX measures (AVERAGE, DIVIDE) rather than column aggregations. Document additivity to prevent incorrect rollups. Keys (Logical Models) Key Types Natural or Surrogate (implementation characteristic): Natural Key : Uses a real-world business identifier (e.g., Invoice Number , Medicare Number ) Surrogate Key : System-generated identifier (e.g., auto-increment, UUID) This is a metadata property, not encoded in the name Primary or Alternate (role in the model): Primary Key (PK) : The chosen unique identifier for the entity Alternate Key (AK) : Any other unique identifier Tag primary keys as (PK) in logical models All other unique keys are considered alternate keys Key Naming Pattern Naming follows these patterns for readability, but metadata remains the source of truth for key roles (i.e., metadata\u2014not naming\u2014should be used to identify Primary Keys, Alternate Keys, and Foreign Keys). Default: When source systems or business glossaries explicitly define key names, preserve their terminology even if it deviates from this convention. Whether a key is natural or surrogate, primary or alternate is captured as metadata Missing name: Where a name hasn't been provided, then preference ID suffix with natural/business keys and Key suffix with surrogate warehouse keys. Canonical Models Rule: In canonical/enterprise views, fields named \\<Entity> ID must represent business-meaningful identifiers recognizable across domains, not warehouse-generated surrogates. Use \\<Entity> Key for warehouse surrogate keys. Scenario Example: Source system (CRM): Provides a natural business identifier for the customer customer_number . Logical model: The identifier is named Customer Number . Physical staging layer: The identifier is stored as the column customer_number to reflect source system (CRM). Canonical view: Exposes Customer Number with metadata indicating it is a natural primary key (NPK). Dimensional model (Customer dimension): Introduces a warehouse surrogate key called Customer Key (physical column customer_key ) to support SCD Type 2. Fact tables (e.g. Sales fact): Store a foreign key column customer_key that references the Customer dimension. Domain contracts (other marts using the conformed dimension): A Sales mart and a Billing mart both join their fact tables to the conformed Customer dimension via customer_key internally. The dimension still carries the natural identifier as Customer Number (physical column customer_number ), and this is what domains expose in their canonical contracts so downstream consumers see a consistent business identifier rather than the surrogate key. Examples: Applying the \"missing name\" rule (when creating new identifiers): Primary Keys: Customer ID (PK) \u2014 natural business identifier (when no source name exists) Customer Key (PK) \u2014 surrogate warehouse key (e.g., auto-increment, UUID) Order ID (PK) \u2014 natural business identifier (when no source name exists) Applying the \"default\" rule (preserving source system names): Primary Keys from Source: Customer Number (PK) \u2014 natural business identifier from source system Account Code (PK) \u2014 natural business identifier from source system Foreign Keys: FKs can point to either natural or surrogate PKs and that the FK naming follows the target\u2019s business term where possible: Patient ID (FK) \u2014 references Patient ID from another entity (the original key may be natural or surrogate) Customer Order Key (FK) \u2014 references the surrogate composite key Warehouse Dimension Keys: Customer Key \u2014 surrogate key for dimensional models (used for joins in the warehouse) Physical mapping: customer_key Other unique identifiers named according to business meaning: Invoice Number (PK) \u2014 natural business reference number for an invoice Medicare Number (AK) \u2014 natural alternate key for a patient National Health Identifier (AK) \u2014 natural alternate key for a patient Composite Keys: Our preference is to give composite keys their own distinct name if the modelling tools allow for it. Each component follows the same naming conventions (Default or Missing name rules apply) If the composite is a natural key, use ID suffix; if any single component is surrogate use Key suffix Examples: Flight ID (Natural PK) = Flight Number + Departure Date \u2014 logical name for the natural composite (all components natural) Enrollment ID (Natural PK) = Student Number + Course Number \u2014 logical name for natural composite (all components natural) Daily Product Sales Key (Surrogate PK) = Product ID + Date Key (from DW) + Transaction Key (from DW) \u2014 logical name for surrogate composite (at least one surrogate component) Customer Order Key (Surrogate PK) = Customer ID + Order Key (from DW) \u2014 logical name for surrogate composite (mixed: surrogate + natural) Diagrammatic Representation (Logical) Consistent with conceptual models, with additional details to show attributes, key types Show relationships as lines and cardinality as crows foot or multiplicity notation. Physical Models For physical modelling framework and common formats, see Physical Models in the Modelling Framework. Physical models represent implemented storage structures and must follow deterministic, platform-safe naming conventions. Singular vs Plural Naming: Default : Use singular nouns for table names (e.g., customer , invoice , payment ) Exception - Dimensional Facts : Use plural nouns for fact tables (e.g., fact_payments , fact_orders ) Dimensional Dimensions : Use singular nouns (e.g., dim_customer , dim_product ) Relational Structures Naming All physical objects must use lowercase with underscores ( snake_case ). General Rules: No mixed case No spaces Use singular table names (except for dimensional model facts - see below) Prefer clarity over brevity Avoid source-system naming where possible Tables: Express business meaning Use prefixes where appropriate (e.g., fact_ , dim_ ) Examples: customer , invoice , fact_orders (dimensional fact), dim_patient (dimensional dimension) Columns: Map directly from logical attributes Use semantic names Do not encode data types or storage formats There are two options for handling units: A: embed the unit in the column name B: separate value and unit type as separate columns Examples: customer_id , invoice_number , order_datetime , total_amount , is_active Diagrammatic Representation Consistent with logical models Show relationships as lines and cardinality as crows foot or multiplicity notation. Databricks Conventions For detailed conventions and examples see Databricks Catalogs: Catalogs represent domain-level scope (e.g., engineering__dev , corporate__dev ) May optionally include data stage, zone, or subdomain in the catalog name depending on desired granularity for access and sharing controls Naming is lowercase, underscore-separated, and meaningful Pattern: {domain}{__env} (e.g., corporate__dev ) Schemas: Schemas represent layers and source systems within a catalog Use double underscores ( __ ) as separators for schema components Optional channel suffix as an additional __ -separated component (avoid special characters in identifiers) Pattern: {layer}{__source_system}{__source_schema}{__channel} (e.g., ods__fhirhouse__dbo__lakeflow ) Full example: engineering__dev.ods__fhirhouse__dbo__lakeflow.encounter Tables and Views: Tables for persisted data Views for logical abstraction Names reflect business meaning, not source-system terminology Columns: Use lowercase snake_case No abbreviations unless standard Dimensional Models For dimensional modelling framework, conformed dimensions, and dimensional bus matrix, see Dimensional (Kimball) Model in the Modelling Framework. For detailed conventions and examples including staging models see the Information Marts sections of Schema and Object Conventions Naming: Fact tables prefixed with fact_ and use plural entity names (e.g., fact_payments , fact_orders ) (pluralisation here is an exception in naming) Dimension tables prefixed with dim_ and use singular entity names (e.g., dim_customer , dim_date ) Warehouse dimension keys: <entity>_key (the surrogate key used for joins in the warehouse) SCD Type 2 Columns: Column Description effective_from_datetime Timestamp when this record version became effective effective_to_datetime Timestamp when this record version expired (NULL for current records) updated_datetime Timestamp from the source system indicating when the record was last modified Default dbt Snapshot Columns: Column Description dbt_scd_id Unique identifier for each snapshot record (surrogate key) dbt_valid_from Timestamp when this record version became effective dbt_valid_to Timestamp when this record version expired (NULL for current records) dbt_updated_at Timestamp from the source system indicating when the record was last modified dbt_deleted (Optional) Boolean flag indicating if a record has been deleted from the source system Measures and Metrics: Align to logical model naming but apply lowercase snake_case (e.g., invoice_amount) Data Warehouse Keys Key resolution Mappings of keys are often required to resolve and integrate heterogeneous identifiers originating from multiple source systems into a consistent warehouse-wide identifier. Key mapping tables support integration by translating source-system identifiers into a common business key and/or surrogate key used by downstream dimensional models. Example: - corporate__prod.edw.keys_employee (base mart keyset conforming SAP and Workday employee identifiers into a single enterprise employee key) Business Keys Business keys (BKs) represent the real-world identifier of an entity and are sourced from operational systems. Business keys may be: Single-column or composite Source-specific or enterprise-conformed The basis of our conformed BK pattern is: source_system | source_identifier in string format Usage guidance: Business keys may be used directly as dimension primary keys (and fact foreign keys) for simple Type 1 dimensions, however this creates inconsistency if other dimensions use surrogate keys for Type 2 tracking. Business keys alone are insufficient for Type 2 dimensions because they do not distinguish historical versions of the same entity. Hence the need for Surrogate Keys. Surrogate Keys Surrogate keys (SKs) are system-generated identifiers that uniquely represent a specific version of a dimensional entity. They are required for: Type 2 (and higher) SCD dimensions Stable fact-to-dimension joins Decoupling fact tables from volatile or composite business keys For Type 2 dimensions, the logical version grain is: business_key + effective_from_datetime The surrogate key is the physical primary key representing this grain and is used by fact tables as the foreign key. Type 2 Fact Resolution For Type 2 behaviour, facts must resolve the correct dimensional version using an effective-date window. fact.business_key = dim.business_key AND fact.event_timestamp >= dim.effective_from_datetime AND fact.event_timestamp < COALESCE(dim.effective_to_datetime, TIMESTAMP '9999-12-31 00:00:00') Databricks Surrogate Keys In Databricks, there are multiple approaches to creating surrogate keys. IDENTITY Databricks recommended approach here : BIGINT GENERATED ALWAYS AS IDENTITY Sequence- or merge-based incrementing keys Characteristics: High performance for joins and aggregations No collision risk Environment-specific values (acceptable in most warehouse designs) Limitations: Not compatible with dbt-managed tables Concurrent writes to the table are not supported Other valid approaches (depending on downstream requirements): Natural key only (BK as PK/FK) : valid for Type 1 dimensions; simplest, but typically wider keys and slower joins. Type 2 composite key (string) : store business_key | effective_from_datetime as a single concatenated key; simple and human readable, but increases key width and join cost. Deterministic hash key : hash (business_key + effective_from_datetime) into a fixed-width value (string or numeric) for smaller keys and cross-environment stability; manage collision risk by choosing an appropriate hash and store BK/effective_from_datetime alongside for traceability. dbt Surrogate Keys dbt supports surrogate hash key generation out of the box: dbt_utils.surrogate_key(...) (MD5 string hash) Platform-specific numeric hashes (e.g. xxhash64 ) cast to BIGINT Usage guidance Hash-based SKs should include: Business key effective_from_datetime (for Type 2 dimensions) Hash-based SKs enable deterministic keys across environments: MD5 used by the dbt default macro returns a 32-character hex string. Binary is more performant but it requires tweaking. Consider SHA256 if hash collision risk is a concern (it is unlikely for dimensions however) Key Design Summary Numeric (BIGINT) keys offer better join performance Prefer numeric SKs for performance (especially with PowerBI); use deterministic SKs when cross-environment stability is required Date dimension tip : Use integer with YYYYMMDD format as the surrogate key for optimal performance Create keys at the layer that owns their grain and semantics i.e where are business keys resolved and where is Type-2 first defined? In most architectures, this will be the Silver / EDW layer.However, if Type 2 semantics are applied later (e.g. in Gold), then surrogate keys must be created there instead. Reference Data For reference data and master data framework, logical representation, and physical implementation patterns, see Master Data and Reference Data in the Modelling Framework. Reference data plays a critical role in conformance by providing standardised values that enable mapping of source-specific codes to canonical enterprise definitions. Logical Modelling Reference entities contain: Natural keys (e.g., Country Code, Product Type Code) Code and description attributes Optional: effective datetimes, display sequences, parent references (hierarchies), business metadata One-to-many relationships to domain entities Example: Country Code Reference Entity country_code country_name iso_code_3 display_sequence effective_from_datetime effective_to_datetime AU Australia AUS 10 2020-01-01 00:00:00 9999-12-31 00:00:00 GB United Kingdom GBR 20 2020-01-01 00:00:00 9999-12-31 00:00:00 US United States USA 30 2020-01-01 00:00:00 9999-12-31 00:00:00 NZ New Zealand NZL 40 2020-01-01 00:00:00 9999-12-31 00:00:00 CA Canada CAN 50 2020-01-01 00:00:00 9999-12-31 00:00:00 Note: Additional standard attributes (audit columns: created_timestamp, created_by, modified_timestamp, modified_by; optional: parent_code, version, source_system_id) would be included in the physical implementation. Physical Implementation Raw Reference Data sourced from upstream systems may require their own staging and transformation pipelines in order to conform them to standard, preserve change history and capture required metadata. Cleansed reference tables are stored in silver/edw layer for wide availability following the reference data naming standard : Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference_data_set_name}{optional:__source_system}{optional:__source_channel} e.g.: corporate__dev.ref.account_code Effectivity: effective_from_date , effective_to_date , ( is_active is derivable) Audit: created_datetime , created_by , updated_datetime , updated_by Hierarchy: parent_code Business: description Technical: version , source_system_id Usage: Mapping logic is applied in Silver/EDW layer staging models during transformation for domain/enterprise-wide application. Consumption: Post-mapped data are exposed in marts in Silver or indirectly in Gold (having passed through Silver). As dimension attributes: Reference values embedded directly in dimension tables (e.g., Product Type Code/Description in Product dimension) for filtering and grouping. Change tracking: Implement Type 1, 2, 4, or 6 slowly changing dimension strategies based on business requirements for point-in-time accuracy. This is especially important when reference data changes frequently or has many attributes. Consider using mini-dimensions/outriggers\u2014separate dimension tables linked via foreign keys\u2014to efficiently track history without excessive row growth in the main dimension. Recommended Practices: Store codes and descriptions in fact tables only when necessary for performance Prefer dimension lookups to maintain single source of truth Standard Suffix and Prefix Inventory Identity & Keys Suffix Meaning Logical/Conceptual Physical ID Primary business identifier Customer ID customer_id Key Warehouse dimension key (dimensional models only) Customer Key customer_key Number Business reference number Invoice Number invoice_number Code Coded business value Diagnosis Code diagnosis_code Reference External reference External Reference external_reference Identifier Explicit identifier (potentially many identifiers) National Identifier national_identifier Temporal Concepts Suffix Meaning Logical/Conceptual Physical Date Calendar date only Admission Date admission_date DateTime Timestamp (assumed UTC) Order DateTime order_datetime DateTime <Timezone> Timestamp in explicit timezone Order DateTime AEST order_datetime_aest Time Time only Appointment Time appointment_time From Date Validity start Policy From Date policy_from_date To Date Validity end Policy To Date policy_to_date Example: Business Name: Order DateTime Physical Name (UTC, implied): order_datetime Physical Name (Local timezone explicit): order_datetime_aest State & Classification Suffix Meaning Logical/Conceptual Physical Status Lifecycle state Order Status order_status Type Classification Customer Type customer_type Category Grouping Product Category product_category Class Structural grouping Asset Class asset_class Quantities & Measures Suffix Meaning Logical/Conceptual Physical Count Quantity Item Count item_count Amount Monetary value Invoice Amount invoice_amount Value General numeric Score Value score_value Rate Rate Interest Rate interest_rate Ratio Proportion Utilisation Ratio utilisation_ratio Percentage Percentage Discount Percentage discount_percentage Boolean Indicators (Prefix Convention) Prefix Meaning Logical/Conceptual Physical Is State check Is Active is_active Has Ownership Has Consent has_consent Can Capability Can Transact can_transact Audit & Control Suffix Meaning Logical/Conceptual Physical Created DateTime Creation timestamp Created DateTime created_datetime Updated DateTime Last update timestamp Updated DateTime updated_datetime Deleted DateTime Soft deletion Deleted DateTime deleted_datetime Effective From Date Valid from date Effective From Date effective_from_date Effective To Date Valid to date Effective To Date effective_to_date Summary of Naming Rules Suffixes encode meaning , not technical data type or implementation details. Rule Logical/Conceptual Physical Primary business identifier Customer ID customer_id Warehouse dimension key Customer Key customer_key Business reference Invoice Number , Diagnosis Code invoice_number , diagnosis_code Temporal meaning Order DateTime (assumed UTC) order_datetime Classification Order Status , Customer Type order_status , customer_type Measures Invoice Amount , Interest Rate invoice_amount , interest_rate Boolean indicators Is Active , Has Consent is_active , has_consent","title":"Intuitas Data Modelling Standards and Conventions"},{"location":"modelling_standards_and_conventions/#intuitas-data-modelling-standards-and-conventions","text":"Return to home Updated 5/12/2025 Related Documents: This document focuses on naming conventions and standards. For platform-specific implementation patterns and architectural details, see Standards and Conventions . For conceptual framework and model types, see Modelling Framework .","title":"Intuitas Data Modelling Standards and Conventions"},{"location":"modelling_standards_and_conventions/#table-of-contents","text":"General Naming Conventions Entity Naming (General Rules) Business Names vs Physical Names Events and Transactions Relationship Naming Conventions Relationship Directionality Relationship Cardinality Conceptual Models Entities Diagrammatic Representation Logical Models Entities Attributes Measures and Metrics Keys (Logical Models) Diagrammatic Representation Physical Models Relational Structures Naming Diagrammatic Representation Databricks Conventions Dimensional Models Data Warehouse Keys Key resolution Business Keys Surrogate Keys Type 2 Fact Resolution Databricks Surrogate Keys dbt Surrogate Keys Reference Data Logical Modelling Physical Implementation Standard Suffix and Prefix Inventory Identity & Keys Temporal Concepts State & Classification Quantities & Measures Boolean Indicators (Prefix Convention) Audit & Control Summary of Naming Rules","title":"Table of Contents"},{"location":"modelling_standards_and_conventions/#general-naming-conventions","text":"","title":"General Naming Conventions"},{"location":"modelling_standards_and_conventions/#entity-naming-general-rules","text":"Use clear business language Use terminology familiar to business stakeholders and consistent with the business glossary In domain models, use terms appropriate to the business context and document synonyms in the business glossary Canonical entities must use terms that are formally agreed and recognised across domains Use singular nouns for entities (e.g., Customer , not Customers ) Plurality is allowed only in dimensional fact tables (e.g., fact_payments ). Dimension tables use singular (e.g., dim_customer ) Avoid system-specific or technical terminology Avoid abbreviations unless universally understood Be specific: add context where needed (e.g., Admin User vs User )","title":"Entity Naming (General Rules)"},{"location":"modelling_standards_and_conventions/#business-names-vs-physical-names","text":"Business names use natural language with spaces (e.g., Order Placed ) Business names are authoritative for semantic meaning Machine-safe identifiers (e.g., GUIDs) are internal metadata only Physical names follow platform-specific conventions (e.g., order_placed ) The business glossary is the semantic source of truth Example: Business Name: Order Placed Internal Identifier: GUID-1234-5678 Physical Name: order_placed","title":"Business Names vs Physical Names"},{"location":"modelling_standards_and_conventions/#events-and-transactions","text":"Business events and transactions may be modelled in two valid ways: Model the entity itself, treating its lifecycle state as an attribute (e.g., Order with Order Status as an attribute) Model the entity in a state-specific form where the status is intrinsic to the concept (e.g., Order Placed , Invoice Issued ) Choose the approach that best represents how the business actually thinks and works with the concept.","title":"Events and Transactions"},{"location":"modelling_standards_and_conventions/#relationship-naming-conventions","text":"Use business verbs, not technical terms Names must read as a business sentence Use present tense and active voice Avoid vague terms (e.g., \"related to\", \"linked to\") Choose precise verbs that express meaning Reflect ownership or composition explicitly where applicable Use domain-specific language Direction should be clear Examples: Customer places Order Policy covers Claim Doctor refers Patient Employer employs Worker Patient receives Treatment Supplier delivers Product Avoid: has_fk references linked to related to","title":"Relationship Naming Conventions"},{"location":"modelling_standards_and_conventions/#relationship-directionality","text":"Every relationship has one authoritative direction and name Model one semantic relationship with one verb. Allow alternate read-paths in tooling or documentation, not duplicate relationships Do not create passive inversions (e.g., \"is placed by\" instead of \"places\") If you need a different semantic perspective, model it as a separate relationship","title":"Relationship Directionality"},{"location":"modelling_standards_and_conventions/#relationship-cardinality","text":"There are two options: Use crow's foot notation (shows 'one' with a line, 'many' with a branching crow's foot; commonly denotes relationships such as one-to-many, many-to-many, etc.) Use multiplicity notation (explicitly labels relationship ends with numbers or ranges, e.g., '0..1', '1', '0.. ', '1.. ', indicating how many entities participate in the relationship) Single direction (preferred): Customer places Order \u2713 Avoid passive inversion (same relationship, just reversed grammar): Order is placed by Customer \u2717 Separate relationships (different business concepts): Customer places Order (ordering action) Customer is billed for Order (financial relationship)","title":"Relationship Cardinality"},{"location":"modelling_standards_and_conventions/#conceptual-models","text":"For conceptual modelling framework and principles, see Conceptual Models in the Modelling Framework.","title":"Conceptual Models"},{"location":"modelling_standards_and_conventions/#entities-conceptual","text":"Business-Facing format with capitalised first letters and spaces (e.g., Order Placed , Customer , Patient Encounter ) Optimised for clarity and business communication","title":"Entities (Conceptual)"},{"location":"modelling_standards_and_conventions/#diagrammatic-representation-conceptual","text":"Boxes represent business concepts (entities) Lines represent relationships Show: Core concepts Key relationships Major business rules Do not show: Keys Data types Technical constraints Detailed cardinality beyond simple one-to-many","title":"Diagrammatic Representation (Conceptual)"},{"location":"modelling_standards_and_conventions/#logical-models","text":"For logical modelling framework and principles, see Logical Models in the Modelling Framework.","title":"Logical Models"},{"location":"modelling_standards_and_conventions/#entities-logical","text":"Use the same format as conceptual models Business-Facing format with capitalised first letters and spaces (e.g., Order Placed , Customer , Patient Encounter ) Optimised for clarity and business communication","title":"Entities (Logical)"},{"location":"modelling_standards_and_conventions/#attributes","text":"Use Business-Facing format with capitalised first letters and spaces Names must be business-meaningful Avoid embedding data types in names Prefer semantic names over technical ones","title":"Attributes"},{"location":"modelling_standards_and_conventions/#measures-and-metrics","text":"For measures and metrics framework, governance, and placement principles, see Measures and Metrics in the Modelling Framework. Use Business-Facing format with capitalised first letters and spaces (e.g., Invoice Amount , Item Count ) Make names consistent with related entities and attributes Use semantic suffixes Quantities & Measures in the Standard Suffix Inventory. Example: Measure: Total Sales Amount \u2014 aggregated sum of sales transactions Metric: Average Order Value \u2014 calculated as Total Sales Amount / Order Count Important: Keep metric names dimension-agnostic : Avoid including dimension references like \"by Product\" or \"by Region\" in metric names\u2014the dimensional model handles slicing (e.g., use Sales Amount not Sales Amount by Product by Time ). Exception\u2014Intrinsic dimensions : Include dimensional qualifiers only when the dimension is intrinsic to the calculation logic and defines how the metric works (e.g., Customer Lifetime Value is calculated at customer grain by definition; Monthly Revenue Growth Rate compares month-to-month by definition). Temporal metrics : Include time window qualifiers when the period defines the calculation logic (e.g., Year-to-Date Sales Amount accumulates from year start; 90-Day Rolling Average uses a 90-day window). Aggregation behavior : Averages and ratios cannot be summed across periods; they must be recalculated at each grain. BI tools like Power BI default to SUM, producing incorrect results for non-additive metrics\u2014define these explicitly with appropriate DAX measures (AVERAGE, DIVIDE) rather than column aggregations. Document additivity to prevent incorrect rollups.","title":"Measures and Metrics"},{"location":"modelling_standards_and_conventions/#keys-logical-models","text":"","title":"Keys (Logical Models)"},{"location":"modelling_standards_and_conventions/#key-types","text":"Natural or Surrogate (implementation characteristic): Natural Key : Uses a real-world business identifier (e.g., Invoice Number , Medicare Number ) Surrogate Key : System-generated identifier (e.g., auto-increment, UUID) This is a metadata property, not encoded in the name Primary or Alternate (role in the model): Primary Key (PK) : The chosen unique identifier for the entity Alternate Key (AK) : Any other unique identifier Tag primary keys as (PK) in logical models All other unique keys are considered alternate keys","title":"Key Types"},{"location":"modelling_standards_and_conventions/#key-naming-pattern","text":"Naming follows these patterns for readability, but metadata remains the source of truth for key roles (i.e., metadata\u2014not naming\u2014should be used to identify Primary Keys, Alternate Keys, and Foreign Keys). Default: When source systems or business glossaries explicitly define key names, preserve their terminology even if it deviates from this convention. Whether a key is natural or surrogate, primary or alternate is captured as metadata Missing name: Where a name hasn't been provided, then preference ID suffix with natural/business keys and Key suffix with surrogate warehouse keys. Canonical Models Rule: In canonical/enterprise views, fields named \\<Entity> ID must represent business-meaningful identifiers recognizable across domains, not warehouse-generated surrogates. Use \\<Entity> Key for warehouse surrogate keys. Scenario Example: Source system (CRM): Provides a natural business identifier for the customer customer_number . Logical model: The identifier is named Customer Number . Physical staging layer: The identifier is stored as the column customer_number to reflect source system (CRM). Canonical view: Exposes Customer Number with metadata indicating it is a natural primary key (NPK). Dimensional model (Customer dimension): Introduces a warehouse surrogate key called Customer Key (physical column customer_key ) to support SCD Type 2. Fact tables (e.g. Sales fact): Store a foreign key column customer_key that references the Customer dimension. Domain contracts (other marts using the conformed dimension): A Sales mart and a Billing mart both join their fact tables to the conformed Customer dimension via customer_key internally. The dimension still carries the natural identifier as Customer Number (physical column customer_number ), and this is what domains expose in their canonical contracts so downstream consumers see a consistent business identifier rather than the surrogate key. Examples: Applying the \"missing name\" rule (when creating new identifiers): Primary Keys: Customer ID (PK) \u2014 natural business identifier (when no source name exists) Customer Key (PK) \u2014 surrogate warehouse key (e.g., auto-increment, UUID) Order ID (PK) \u2014 natural business identifier (when no source name exists) Applying the \"default\" rule (preserving source system names): Primary Keys from Source: Customer Number (PK) \u2014 natural business identifier from source system Account Code (PK) \u2014 natural business identifier from source system Foreign Keys: FKs can point to either natural or surrogate PKs and that the FK naming follows the target\u2019s business term where possible: Patient ID (FK) \u2014 references Patient ID from another entity (the original key may be natural or surrogate) Customer Order Key (FK) \u2014 references the surrogate composite key Warehouse Dimension Keys: Customer Key \u2014 surrogate key for dimensional models (used for joins in the warehouse) Physical mapping: customer_key Other unique identifiers named according to business meaning: Invoice Number (PK) \u2014 natural business reference number for an invoice Medicare Number (AK) \u2014 natural alternate key for a patient National Health Identifier (AK) \u2014 natural alternate key for a patient Composite Keys: Our preference is to give composite keys their own distinct name if the modelling tools allow for it. Each component follows the same naming conventions (Default or Missing name rules apply) If the composite is a natural key, use ID suffix; if any single component is surrogate use Key suffix Examples: Flight ID (Natural PK) = Flight Number + Departure Date \u2014 logical name for the natural composite (all components natural) Enrollment ID (Natural PK) = Student Number + Course Number \u2014 logical name for natural composite (all components natural) Daily Product Sales Key (Surrogate PK) = Product ID + Date Key (from DW) + Transaction Key (from DW) \u2014 logical name for surrogate composite (at least one surrogate component) Customer Order Key (Surrogate PK) = Customer ID + Order Key (from DW) \u2014 logical name for surrogate composite (mixed: surrogate + natural)","title":"Key Naming Pattern"},{"location":"modelling_standards_and_conventions/#diagrammatic-representation-logical","text":"Consistent with conceptual models, with additional details to show attributes, key types Show relationships as lines and cardinality as crows foot or multiplicity notation.","title":"Diagrammatic Representation (Logical)"},{"location":"modelling_standards_and_conventions/#physical-models","text":"For physical modelling framework and common formats, see Physical Models in the Modelling Framework. Physical models represent implemented storage structures and must follow deterministic, platform-safe naming conventions. Singular vs Plural Naming: Default : Use singular nouns for table names (e.g., customer , invoice , payment ) Exception - Dimensional Facts : Use plural nouns for fact tables (e.g., fact_payments , fact_orders ) Dimensional Dimensions : Use singular nouns (e.g., dim_customer , dim_product )","title":"Physical Models"},{"location":"modelling_standards_and_conventions/#relational-structures","text":"","title":"Relational Structures"},{"location":"modelling_standards_and_conventions/#naming","text":"All physical objects must use lowercase with underscores ( snake_case ). General Rules: No mixed case No spaces Use singular table names (except for dimensional model facts - see below) Prefer clarity over brevity Avoid source-system naming where possible Tables: Express business meaning Use prefixes where appropriate (e.g., fact_ , dim_ ) Examples: customer , invoice , fact_orders (dimensional fact), dim_patient (dimensional dimension) Columns: Map directly from logical attributes Use semantic names Do not encode data types or storage formats There are two options for handling units: A: embed the unit in the column name B: separate value and unit type as separate columns Examples: customer_id , invoice_number , order_datetime , total_amount , is_active","title":"Naming"},{"location":"modelling_standards_and_conventions/#diagrammatic-representation","text":"Consistent with logical models Show relationships as lines and cardinality as crows foot or multiplicity notation.","title":"Diagrammatic Representation"},{"location":"modelling_standards_and_conventions/#databricks-conventions","text":"For detailed conventions and examples see Databricks Catalogs: Catalogs represent domain-level scope (e.g., engineering__dev , corporate__dev ) May optionally include data stage, zone, or subdomain in the catalog name depending on desired granularity for access and sharing controls Naming is lowercase, underscore-separated, and meaningful Pattern: {domain}{__env} (e.g., corporate__dev ) Schemas: Schemas represent layers and source systems within a catalog Use double underscores ( __ ) as separators for schema components Optional channel suffix as an additional __ -separated component (avoid special characters in identifiers) Pattern: {layer}{__source_system}{__source_schema}{__channel} (e.g., ods__fhirhouse__dbo__lakeflow ) Full example: engineering__dev.ods__fhirhouse__dbo__lakeflow.encounter Tables and Views: Tables for persisted data Views for logical abstraction Names reflect business meaning, not source-system terminology Columns: Use lowercase snake_case No abbreviations unless standard","title":"Databricks Conventions"},{"location":"modelling_standards_and_conventions/#dimensional-models","text":"For dimensional modelling framework, conformed dimensions, and dimensional bus matrix, see Dimensional (Kimball) Model in the Modelling Framework. For detailed conventions and examples including staging models see the Information Marts sections of Schema and Object Conventions Naming: Fact tables prefixed with fact_ and use plural entity names (e.g., fact_payments , fact_orders ) (pluralisation here is an exception in naming) Dimension tables prefixed with dim_ and use singular entity names (e.g., dim_customer , dim_date ) Warehouse dimension keys: <entity>_key (the surrogate key used for joins in the warehouse) SCD Type 2 Columns: Column Description effective_from_datetime Timestamp when this record version became effective effective_to_datetime Timestamp when this record version expired (NULL for current records) updated_datetime Timestamp from the source system indicating when the record was last modified Default dbt Snapshot Columns: Column Description dbt_scd_id Unique identifier for each snapshot record (surrogate key) dbt_valid_from Timestamp when this record version became effective dbt_valid_to Timestamp when this record version expired (NULL for current records) dbt_updated_at Timestamp from the source system indicating when the record was last modified dbt_deleted (Optional) Boolean flag indicating if a record has been deleted from the source system Measures and Metrics: Align to logical model naming but apply lowercase snake_case (e.g., invoice_amount)","title":"Dimensional Models"},{"location":"modelling_standards_and_conventions/#data-warehouse-keys","text":"","title":"Data Warehouse Keys"},{"location":"modelling_standards_and_conventions/#key-resolution","text":"Mappings of keys are often required to resolve and integrate heterogeneous identifiers originating from multiple source systems into a consistent warehouse-wide identifier. Key mapping tables support integration by translating source-system identifiers into a common business key and/or surrogate key used by downstream dimensional models. Example: - corporate__prod.edw.keys_employee (base mart keyset conforming SAP and Workday employee identifiers into a single enterprise employee key)","title":"Key resolution"},{"location":"modelling_standards_and_conventions/#business-keys","text":"Business keys (BKs) represent the real-world identifier of an entity and are sourced from operational systems. Business keys may be: Single-column or composite Source-specific or enterprise-conformed The basis of our conformed BK pattern is: source_system | source_identifier in string format Usage guidance: Business keys may be used directly as dimension primary keys (and fact foreign keys) for simple Type 1 dimensions, however this creates inconsistency if other dimensions use surrogate keys for Type 2 tracking. Business keys alone are insufficient for Type 2 dimensions because they do not distinguish historical versions of the same entity. Hence the need for Surrogate Keys.","title":"Business Keys"},{"location":"modelling_standards_and_conventions/#surrogate-keys","text":"Surrogate keys (SKs) are system-generated identifiers that uniquely represent a specific version of a dimensional entity. They are required for: Type 2 (and higher) SCD dimensions Stable fact-to-dimension joins Decoupling fact tables from volatile or composite business keys For Type 2 dimensions, the logical version grain is: business_key + effective_from_datetime The surrogate key is the physical primary key representing this grain and is used by fact tables as the foreign key.","title":"Surrogate Keys"},{"location":"modelling_standards_and_conventions/#type-2-fact-resolution","text":"For Type 2 behaviour, facts must resolve the correct dimensional version using an effective-date window. fact.business_key = dim.business_key AND fact.event_timestamp >= dim.effective_from_datetime AND fact.event_timestamp < COALESCE(dim.effective_to_datetime, TIMESTAMP '9999-12-31 00:00:00')","title":"Type 2 Fact Resolution"},{"location":"modelling_standards_and_conventions/#databricks-surrogate-keys","text":"In Databricks, there are multiple approaches to creating surrogate keys. IDENTITY Databricks recommended approach here : BIGINT GENERATED ALWAYS AS IDENTITY Sequence- or merge-based incrementing keys Characteristics: High performance for joins and aggregations No collision risk Environment-specific values (acceptable in most warehouse designs) Limitations: Not compatible with dbt-managed tables Concurrent writes to the table are not supported Other valid approaches (depending on downstream requirements): Natural key only (BK as PK/FK) : valid for Type 1 dimensions; simplest, but typically wider keys and slower joins. Type 2 composite key (string) : store business_key | effective_from_datetime as a single concatenated key; simple and human readable, but increases key width and join cost. Deterministic hash key : hash (business_key + effective_from_datetime) into a fixed-width value (string or numeric) for smaller keys and cross-environment stability; manage collision risk by choosing an appropriate hash and store BK/effective_from_datetime alongside for traceability.","title":"Databricks Surrogate Keys"},{"location":"modelling_standards_and_conventions/#dbt-surrogate-keys","text":"dbt supports surrogate hash key generation out of the box: dbt_utils.surrogate_key(...) (MD5 string hash) Platform-specific numeric hashes (e.g. xxhash64 ) cast to BIGINT Usage guidance Hash-based SKs should include: Business key effective_from_datetime (for Type 2 dimensions) Hash-based SKs enable deterministic keys across environments: MD5 used by the dbt default macro returns a 32-character hex string. Binary is more performant but it requires tweaking. Consider SHA256 if hash collision risk is a concern (it is unlikely for dimensions however)","title":"dbt Surrogate Keys"},{"location":"modelling_standards_and_conventions/#key-design-summary","text":"Numeric (BIGINT) keys offer better join performance Prefer numeric SKs for performance (especially with PowerBI); use deterministic SKs when cross-environment stability is required Date dimension tip : Use integer with YYYYMMDD format as the surrogate key for optimal performance Create keys at the layer that owns their grain and semantics i.e where are business keys resolved and where is Type-2 first defined? In most architectures, this will be the Silver / EDW layer.However, if Type 2 semantics are applied later (e.g. in Gold), then surrogate keys must be created there instead.","title":"Key Design Summary"},{"location":"modelling_standards_and_conventions/#reference-data","text":"For reference data and master data framework, logical representation, and physical implementation patterns, see Master Data and Reference Data in the Modelling Framework. Reference data plays a critical role in conformance by providing standardised values that enable mapping of source-specific codes to canonical enterprise definitions.","title":"Reference Data"},{"location":"modelling_standards_and_conventions/#logical-modelling","text":"Reference entities contain: Natural keys (e.g., Country Code, Product Type Code) Code and description attributes Optional: effective datetimes, display sequences, parent references (hierarchies), business metadata One-to-many relationships to domain entities Example: Country Code Reference Entity country_code country_name iso_code_3 display_sequence effective_from_datetime effective_to_datetime AU Australia AUS 10 2020-01-01 00:00:00 9999-12-31 00:00:00 GB United Kingdom GBR 20 2020-01-01 00:00:00 9999-12-31 00:00:00 US United States USA 30 2020-01-01 00:00:00 9999-12-31 00:00:00 NZ New Zealand NZL 40 2020-01-01 00:00:00 9999-12-31 00:00:00 CA Canada CAN 50 2020-01-01 00:00:00 9999-12-31 00:00:00 Note: Additional standard attributes (audit columns: created_timestamp, created_by, modified_timestamp, modified_by; optional: parent_code, version, source_system_id) would be included in the physical implementation.","title":"Logical Modelling"},{"location":"modelling_standards_and_conventions/#physical-implementation","text":"Raw Reference Data sourced from upstream systems may require their own staging and transformation pipelines in order to conform them to standard, preserve change history and capture required metadata. Cleansed reference tables are stored in silver/edw layer for wide availability following the reference data naming standard : Schema naming convention: ref{optional: __domain name}{optional: __subdomain name(s)} Object naming convention: {reference_data_set_name}{optional:__source_system}{optional:__source_channel} e.g.: corporate__dev.ref.account_code Effectivity: effective_from_date , effective_to_date , ( is_active is derivable) Audit: created_datetime , created_by , updated_datetime , updated_by Hierarchy: parent_code Business: description Technical: version , source_system_id Usage: Mapping logic is applied in Silver/EDW layer staging models during transformation for domain/enterprise-wide application. Consumption: Post-mapped data are exposed in marts in Silver or indirectly in Gold (having passed through Silver). As dimension attributes: Reference values embedded directly in dimension tables (e.g., Product Type Code/Description in Product dimension) for filtering and grouping. Change tracking: Implement Type 1, 2, 4, or 6 slowly changing dimension strategies based on business requirements for point-in-time accuracy. This is especially important when reference data changes frequently or has many attributes. Consider using mini-dimensions/outriggers\u2014separate dimension tables linked via foreign keys\u2014to efficiently track history without excessive row growth in the main dimension. Recommended Practices: Store codes and descriptions in fact tables only when necessary for performance Prefer dimension lookups to maintain single source of truth","title":"Physical Implementation"},{"location":"modelling_standards_and_conventions/#standard-suffix-and-prefix-inventory","text":"","title":"Standard Suffix and Prefix Inventory"},{"location":"modelling_standards_and_conventions/#identity-keys","text":"Suffix Meaning Logical/Conceptual Physical ID Primary business identifier Customer ID customer_id Key Warehouse dimension key (dimensional models only) Customer Key customer_key Number Business reference number Invoice Number invoice_number Code Coded business value Diagnosis Code diagnosis_code Reference External reference External Reference external_reference Identifier Explicit identifier (potentially many identifiers) National Identifier national_identifier","title":"Identity &amp; Keys"},{"location":"modelling_standards_and_conventions/#temporal-concepts","text":"Suffix Meaning Logical/Conceptual Physical Date Calendar date only Admission Date admission_date DateTime Timestamp (assumed UTC) Order DateTime order_datetime DateTime <Timezone> Timestamp in explicit timezone Order DateTime AEST order_datetime_aest Time Time only Appointment Time appointment_time From Date Validity start Policy From Date policy_from_date To Date Validity end Policy To Date policy_to_date Example: Business Name: Order DateTime Physical Name (UTC, implied): order_datetime Physical Name (Local timezone explicit): order_datetime_aest","title":"Temporal Concepts"},{"location":"modelling_standards_and_conventions/#state-classification","text":"Suffix Meaning Logical/Conceptual Physical Status Lifecycle state Order Status order_status Type Classification Customer Type customer_type Category Grouping Product Category product_category Class Structural grouping Asset Class asset_class","title":"State &amp; Classification"},{"location":"modelling_standards_and_conventions/#quantities-measures","text":"Suffix Meaning Logical/Conceptual Physical Count Quantity Item Count item_count Amount Monetary value Invoice Amount invoice_amount Value General numeric Score Value score_value Rate Rate Interest Rate interest_rate Ratio Proportion Utilisation Ratio utilisation_ratio Percentage Percentage Discount Percentage discount_percentage","title":"Quantities &amp; Measures"},{"location":"modelling_standards_and_conventions/#boolean-indicators-prefix-convention","text":"Prefix Meaning Logical/Conceptual Physical Is State check Is Active is_active Has Ownership Has Consent has_consent Can Capability Can Transact can_transact","title":"Boolean Indicators (Prefix Convention)"},{"location":"modelling_standards_and_conventions/#audit-control","text":"Suffix Meaning Logical/Conceptual Physical Created DateTime Creation timestamp Created DateTime created_datetime Updated DateTime Last update timestamp Updated DateTime updated_datetime Deleted DateTime Soft deletion Deleted DateTime deleted_datetime Effective From Date Valid from date Effective From Date effective_from_date Effective To Date Valid to date Effective To Date effective_to_date","title":"Audit &amp; Control"},{"location":"modelling_standards_and_conventions/#summary-of-naming-rules","text":"Suffixes encode meaning , not technical data type or implementation details. Rule Logical/Conceptual Physical Primary business identifier Customer ID customer_id Warehouse dimension key Customer Key customer_key Business reference Invoice Number , Diagnosis Code invoice_number , diagnosis_code Temporal meaning Order DateTime (assumed UTC) order_datetime Classification Order Status , Customer Type order_status , customer_type Measures Invoice Amount , Interest Rate invoice_amount , interest_rate Boolean indicators Is Active , Has Consent is_active , has_consent","title":"Summary of Naming Rules"},{"location":"standards_and_conventions/","text":"Standards and Conventions Return to home These standards are opinionated and designed to ensure consistency, governance, and automation across an organisation. They largely reflect an Azure Databricks environment, however can be adapted to other platforms. Organisations should adapt these standards to fit their existing internal conventions. Table of Contents Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (standards_and_conventions.mdADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Policies Frameworks Mesh Domain Names All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g: intuitas_corporate Platform Environment Environment name: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred) VNET Name: vn-{organisation_name}-{domain_name} = e.g: vn-intuitas-corporate Resource Groups Name: rg-{organisation_name}-{domain_name} e.g: rg-intuitas-corporate Databricks workspace Name: ws-{organisation_name}-{domain_name} e.g: ws-intuitas-corporate Key vault Name: kv-{organisation_name}-{domain_name} e.g: kv-intuitas-corporate Secrets Name: {secret_name} Entra Group Names Name: eg-{organisation_name}-{domain_name} = e.g: eg-intuitas-corporate Azure Data Factory (ADF) Name: adf-{organisation_name}-{domain_name} e.g: adf-intuitas-corporate SQL Server Name: sql-{organisation_name}-{domain_name} e.g: sql-intuitas-corporate SQL Database Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g: sqldb-metadata-intuitas-corporate Storage The section describes naming standards and conventions for cloud storage resources. Lakehouse storage Lakehouse storage account name: dl{organisation_name}{domain_name} Lakehouse storage containers Name: {environment} (dev/test/preprod/prod) Lakehouse storage folders Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} e.g: bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog Generic Blob storage Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod Generic Blob files and folders No standard naming conventions for files and folders. Databricks This section provides naming standards and conventions for Databricks. Workspace and cluster names Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name} Jobs and Pipelines Job names Job names: {domain}__{layer}__{purpose}__{source}{optional: __target}{optional: __schedule}{optional: __version}__{env} e.g. clinical__bronze__ingest__fhircdr__dev For Delta Live Table (DLT) Pipelines Pipeline names: {domain_name}__{layer}__pipeline__{dataset}{optional: __schedule}{optional: __version}__{env} e.g. clinical__bronze__pipeline__fhircdr__dev e.g. supplychain__gold__pipeline__inventorymart__prod Note on {layer}: If the 'business outcome' is Gold, you call it Gold, even if it produces Bronze + Silver on the way. i.e \"This is the production DLT pipeline in the supply chain domain, which builds and maintains the curated gold-layer dataset called Inventory Mart\" Include pipeline so it\u2019s distinguishable from ad hoc jobs. Dataset can be a logical grouping (e.g., patient, encounter, claims). Orchestration job names Orchestration job names: {domain}__orchestration__{workflow-name}{optional: __schedule}{optional: __version}__{env} e.g. clinical__orchestrate__fhirworkflow__daily__dev which then orchestrates: clinical__bronze__pipeline__fhircdr__prod clinical__silver__pipeline__fhirclean__prod clinical__gold__pipeline__clinicalmart__prod Job logging event log catalog: {domain}__audit__{env} event log schema: audit__event_log Optional Versioning (if needed): add v1, v2 if a job is redesigned but old one stays around. Scheduling frequency (optional): suffix with _hourly, _daily, _weekly if relevant. Catalog naming and conventions Refer to Data layers and stages for further context and definitions applicable to this section. Catalog name: The choice of granularity depends on domain topology, stage/zone convention and desired level of segregation for access and sharing controls (i.e. catalog or schema level) Minimum granularity (domain level): {domain_name}{__environment (dev/test/pat/prod)} (prod is implied optional) e.g: corporate__dev Optional granularity (domain-data stage level): {domain_name}{_data_stage: (bronze/silver/gold)}{__environment (dev/test/pat/prod)} e.g: corporate_bronze__dev Optional granularity (domain-data stage and zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/mart)}{__environment (dev/test/pat/prod)} e.g: corporate_bronze_ods__dev Optional granularity (domain-data zone level): {domain_name}{_data_zone: (ods/pds/edw/im)}{__environment (dev/test/pat/prod)} e.g: corporate_ods__dev Optional granularity (subdomain-data stage level): {domain_name}{_descriptor (subdomain/subject/project*)}{_data_stage: (bronze/silver/gold)}{__environment (dev/test/pat/prod)} e.g: corporate_finance_bronze__dev In the examples provided - we have opted for domain level - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.ods__fhirhouse__dbo(lakeflow).encounter Note that projects are temporary constructs, and hence are not recommended for naming Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog Externally mounted (lakehouse federation) Catalog Names Foreign Catalog name: {domain_name (owner)}__fc__{source_system}{optional:__other_useful_descriptors e.g:__environment} e.g: corporate__fc__sqlonpremsource e.g: corporate__fc__sqlonpremsource__prod Catalog Metadata tags: The following metadata should be added when creating a catalog: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g: if delegating to engineering domain Schema and object conventions Refer to Data layers and stages for further context and definitions applicable to this section. Refer to Physical Models for the standard relating to column naming, types, and conventions. Schema level external storage locations Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g: abfss://dev@dlintutiasengineering.dfs.core.windows.net/engineering__dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations. Metadata Schemas and Objects Refer to Data layers and stages for further context and definitions applicable to this section. Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta {optional: __function} Naming convention: {function/descriptor} e.g: corporate__dev.meta.ingestion_control e.g: corporate__dev.meta__ingestion.ingestion_control Bronze (Raw data according to systems) The Bronze layer stores raw, immutable data as it is ingested from source systems. See Data layers and stages for definitions and context. All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. bronze__ In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.ods__fhirhouse__dbo__lakeflow.encounter Persistent Landing : Persistent Landing uses Unity Catalog Volumes for storing raw files and unstructured data as they arrive from source systems. Volume naming convention: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] [domain][__env] : Domain and environment identifier (e.g., corporate_dev , engineering_prod ) [layer] : Data layer identifier (e.g., landing ) [__source_system] : Source system identifier (e.g., workdayapi , saphr ) [__source_schema] : Optional source schema or subsystem identifier (channel) : Ingestion channel/method (e.g., adf , fivetran , api ) [source_object/volume_name] : Descriptive volume name or source object identifier e.g: corporate__dev.landing__workdayapi.schedule_volume e.g: engineering__prod.landing__fhirapi__patients.raw_data Operational Data Store (ODS) : The objective of raw layer conventions is to provide clarity over which zone and stage it belongs, what the data relates to, where it was sourced from, and via what channel it arrived (as there may be nuances in data depending on its channel). ODS can be replicated from source systems, or prepared for use from semi/unstructured data via hard-transformation and hence will have these associated conventions: Database replicated ODS (structured sources like SQL Server): - Format: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., ods ) - [__source_system] : Source system identifier - [__source_schema] : Source schema (if applicable, e.g., dbo , reporting ) - (channel) : Optional ingestion channel (e.g., adf , fivetran , lakeflow ) - [source_object/volume_name] : Table name as per source - e.g: clinical__dev.ods__patientflowmanager01__reporting.patients - e.g: clinical__dev.ods__fhirhouse__dbo(adf).encounter Prepped semi/unstructured ODS data: - Format: [domain][__env].[layer][__source_system][__source_descriptor](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., ods ) - [__source_system] : Source system identifier - [__source_descriptor] : Source descriptor or subsystem identifier - (channel) : Optional ingestion channel (e.g., kafka , databricks , api ) - [source_object/volume_name] : Named as per source or unique assigned name (e.g., topic/folder name) - e.g: clinical__dev.ods__ambosim__confluent(kafka).encounter - e.g: corporate__dev.ods__workdayapi__employees.raw_data Persistent Data Store (PDS) : PDS conventions will mirror ODS conventions: Database replicated PDS (structured sources like SQL Server): - Format: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., pds ) - [__source_system] : Source system identifier - [__source_schema] : Source schema (if applicable, e.g., dbo , reporting ) - (channel) : Optional ingestion channel (e.g., adf , fivetran , lakeflow ) - [source_object/volume_name] : Table name as per source - e.g: clinical__dev.pds__patientflowmanager01__reporting.patients - e.g: clinical__dev.pds__fhirhouse__dbo(adf).encounter Prepped semi/unstructured PDS data: - Format: [domain][__env].[layer][__source_system][__source_descriptor](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., pds ) - [__source_system] : Source system identifier - [__source_descriptor] : Source descriptor or subsystem identifier - (channel) : Optional ingestion channel (e.g., kafka , databricks , api ) - [source_object/volume_name] : Named as per source or unique assigned name (e.g., topic/folder name) - e.g: clinical__dev.pds__ambosim__confluent(kafka).encounter - e.g: corporate__dev.pds__workdayapi__employees.raw_data Silver/EDW (Data according to business entities) The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. Refer to Data layers and stages for further context and definitions applicable to this section. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.edw.dim_customer All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. silver__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver)/EDW Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts (base marts in the EDW layer) and reference data (in the REF layer). Naming Convention: [domain][__env] . [layer] . [object_type][_entity/descriptor](__source)(__channel)(__transformation) Where: - [domain][__env] - Domain and environment (e.g., corporate__prod , corporate__dev ) - [layer] - Data layer (e.g., ref , edw ) - [object_type] - Object type prefix (e.g., stg_ , ref_ , dim_ , fact_ , keys_ ) - [_entity/descriptor] - Entity or object name - (__source) - Optional source system identifier (e.g., __sap , __workday , __finance_system ) - (__channel) - Optional source channel (e.g., __adf , __api ) - (__transformation) - Optional transformation step (e.g., __01_typed , __02_cleaned ) Reference Data Staging: Source-specific staging (with transformations): e.g: corporate__prod.ref.stg_facility__sap__01_filtered e.g: corporate__prod.ref.stg_facility__sap__02_typed e.g: corporate__prod.ref.stg_account_code__finance_system__adf__01_renamed e.g: corporate__prod.ref.stg_location__workday__01_typed Final reference data (cleaned and conformed): e.g: corporate__prod.ref.ref_facility e.g: corporate__prod.ref.ref_location e.g: corporate__prod.ref.ref_account_code e.g: corporate__dev.ref.ref_country_codes Base Mart (EDW) Staging: Source-specific staging (with transformations): e.g: corporate__prod.edw.stg_employee__sap__01_typed e.g: corporate__prod.edw.stg_employee__workday__01_typed e.g: corporate__prod.edw.stg_employee__workday__02_cleaned e.g: corporate__prod.edw.stg_customer__crm__adf__01_renamed e.g: corporate__prod.edw.stg_customer__crm__adf__02_cleaned e.g: corporate__prod.edw.stg_payment__new_finance_system__adf__01_typed e.g: corporate__prod.edw.stg_payment__old_finance_system__adf__01_typed e.g: corporate__prod.edw.stg_account__finance_system__api__01_renamed Non-source specific staging (after combining/deduplicating): e.g: corporate__prod.edw.stg_employee__01_deduped e.g: corporate__prod.edw.stg_employee__02_validated e.g: corporate__prod.edw.stg_payment__01_unified Base Mart Keysets: Conforming/deduplicating keysets across multiple sources: e.g: corporate__prod.edw.keys_employee (conforms sap and workday) e.g: corporate__prod.edw.keys_customer (conforms multiple crm sources) e.g: corporate__prod.edw.keys_account (conforms finance systems) e.g: health__prod.edw.keys_patient (conforms clinical systems) Base Marts (Final Dimensional Models): Source-specific dimensions (preserving source identity): e.g: corporate__prod.edw.dim_employee__sap e.g: corporate__prod.edw.dim_employee__workday e.g: corporate__prod.edw.dim_account__old_finance_system e.g: corporate__prod.edw.dim_account__new_finance_system Conformed dimensions (unified, non-source specific): e.g: corporate__prod.edw.dim_employee (type 1 SCD implied) e.g: corporate__prod.edw.dim_employee__type2 (with history) e.g: corporate__prod.edw.dim_customer e.g: corporate__prod.edw.dim_account (unified across finance systems) Fact tables: e.g: corporate__prod.edw.fact_payment e.g: corporate__prod.edw.fact_transaction e.g: health__prod.edw.fact_encounter Common Transformation Suffixes: __01_renamed or __01_typed __02_cleaned __03_deduped __04_filtered __05_split __06_validated __07_desensitised Additional Notes: For further context on reference data modeling, refer to Reference Data Standards and Conventions . Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g: corporate__dev.edw_rv.hs_payments__finance_system__adf Business Vault : Optional warehousing construct. Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g: corporate__dev.edw_bv.hs_late_payments__finance_system__adf Gold (Data according to requirements) The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists when source-specific views are needed. Refer to Data layers and stages for further context and definitions applicable to this section. Naming Convention: [domain][__env] . mart . [object_type][product_name/descriptor](__source)(__transformation) Where: - [domain][__env] - Domain and environment (e.g., corporate__prod , health__dev ) - mart - Gold layer schema (product marts) - [object_type] - Object type prefix (e.g., stg_ , fact_ , dim_ , obt_ ) - [product_name/descriptor] - Product or business-aligned name - (__source) - Optional source system identifier (only when source-specific view is needed) - (__transformation) - Optional transformation step (for staging only) Naming Guidelines: - All entity names which align to facts should be named in plural - All entity names which align to dimensions should be named in singular - Product marts are business/requirement-aligned, not technical constructs (Gold)/Product Mart Staging Models : Staging models serve as intermediary models that transform base marts (EDW) into requirement-aligned product marts. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold product marts with business-specific transformations: Common Transformations: - Pivoting - Aggregation - Joining across multiple base marts - Business rule application - Desensitization - Complex calculations Examples: e.g: corporate__prod.mart.stg_downtime_by_region__01_pivoted e.g: corporate__prod.mart.stg_downtime_by_region__02_desensitised e.g: corporate__prod.mart.stg_late_payments__01_aggregated e.g: corporate__prod.mart.stg_late_payments__02_joined e.g: corporate__prod.mart.stg_fte_calculations__01_pivoted e.g: corporate__prod.mart.stg_fte_calculations__02_aggregated e.g: health__prod.mart.stg_patient_outcomes__01_joined e.g: health__prod.mart.stg_patient_outcomes__02_desensitised (Gold)/Product Marts (Final Products) : Final business-aligned data products ready for consumption by analytics, reporting, and BI tools. Examples: Fact tables (aggregated/business-focused): e.g: corporate__prod.mart.fact_downtime_by_region e.g: corporate__prod.mart.fact_late_payments e.g: corporate__prod.mart.fact_regional_account_payments e.g: health__prod.mart.fact_patient_outcomes Dimension tables (product-specific): e.g: corporate__prod.mart.dim_region e.g: corporate__prod.mart.dim_payment_category One Big Tables (OBT) - Denormalized aggregates: e.g: corporate__prod.mart.obt_fte_calculations e.g: corporate__prod.mart.obt_financial_summary e.g: corporate__prod.mart.obt_executive_dashboard Source-specific products (when needed): e.g: corporate__prod.mart.fact_account_payments__old_finance_system e.g: corporate__prod.mart.fact_account_payments__new_finance_system e.g: corporate__prod.mart.obt_employee_metrics__sap e.g: corporate__prod.mart.obt_employee_metrics__workday Delta Sharing Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g: intuitas_corporate__finance__reporting__account_payments__payments Azure Data Factory Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g: ls_financedb_intuitas_corporate Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency} Streaming Cluster name: {domain_name}__cluster__{optional:environment} Topic names: {domain_name}__{object/entity?}__{optional:source_system}___{optional:source_channel}__{optional:environment} Consumer group names: {domain_name}__{unique_group_name}__{optional:environment} dbt The following standards and conventions relate to dbt projects. Documentation and model metadata Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml Sources Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g: ods__ambo_sim__kafka__sources.yml Model and Folder Names dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards. Bronze Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/ Silver Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} *e.g:* - *silver\\new_finance_system__adf\\stg\\corporate__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - or *silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - materialises to: *corporate__dev.edw.stg_account__new_finance_system__adf__01_renamed* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:dim_/fact_}{__entity /_object_description} __{ordinal}_{transformation description} e.g: corporate__dev.edw.stg_account__01_deduped e.g: corporate__dev.edw.stg_account__02_validated *e.g:* - *silver\\mart\\accounts\\stg\\corporate__silver__stg__accounts__01_deduped.sql* - or *silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql* - materialises to: *e.g: corporate__dev.edw.stg_account__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim_/fact_}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim_/fact_}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:dim_/fact_} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention:** Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards} Gold Staging:** Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} Dimensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__dim_{__entity / product description} (optional: __{source_system} __{source_channel}) Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__fact_{__entity / product description} (optional: __{source_system} __{source_channel}) Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}} Example dbt model structure: The model structure below reflects a single catalog for domain+environment and schema separation for layers and stages: {{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__dim_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__dim_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__dim_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__dim_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g: finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql dbt_project.yml The yml structure below reflects a single catalog for domain+environment and schema separation for layers and stages: models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: ods: +schema: ods pds: +schema: pds silver: ref: +description: \"Reference data\" +schema: ref +materialized: table edw: +description: \"Enterprise data warehouse base marts\" +schema: edw +materialized: table staging: +materialized: view gold: +materialized: view # default for speed mart: +description: \"Product marts\" +schema: mart +materialized: table CI/CD The following standards and conventions relate to Continuous Improvement and Continuous Delivery constructs. Repository naming All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-corporate-dbt - intuitas-corporate-ingestion-framework - intuitas-corporate-cicd-templates - intuitas-corporate-infrastructure Branch naming All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests Branch lifecycle Simple branch lifecycle: main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master Comprehensive team branch lifecycle: Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes Databricks Asset Bundles Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name}` e.g: if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g: yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com Security Security standards and conventioned provided here provide a starter set, however existing organisational and applicable industry standards should take precedence. Consult with your cybersecurity advisor. Entra Under development. (Contact us to know more). Most organisations will already have an established set of groups and conventions. Where there are gaps, the following can still be considered. Recommended areas to align to organisational governance and cyber requirements: Naming conventions for admin, service, and user groups Role-based access alignment (least privilege, separation of duties) Alignment to domains - Cross-domain vs. domain-specific group patterns Entra Group Names : Pattern: grp-<org>-<domain>-<plat>-<scope>-<role>-<env>[-<region>][-ext-<partner>][-idx] Lowercase, hyphen-separated; no spaces. Keep to \u2264 120 chars total. No PII in names. Use Security groups (not M365) for RBAC; enable PIM where appropriate e.g. Admins. role : owner \u2014 full control of the named scope admin \u2014 administrative (non-ownership) rights contrib \u2014 create/modify within scope editor \u2014 modify data/artifacts, not permissions reader \u2014 read-only steward \u2014 governance/metadata rights custodian \u2014 key/secret/storage control operator \u2014 run/ops rights (pipelines, jobs) viewer \u2014 read dashboards/reports plat: dbx (Databricks), uc (Unity Catalog), pbi (Power BI), adf (Data Factory), dlk (Data Lake), sql (Azure SQL), kva (Key Vault), syn (Synapse) scope (or object): Databricks Workspace: ws- Unity Catalog: uc-meta (metastore), uc-cat- , uc-sch- . , uc-obj- . . Power BI: pbi-ws- Data Lake: dlk-path-/datalake/ / Examples : GRP-INTUITAS-CLIN-DBX-WS-Analytics-ADMIN-PRD GRP-INTUITAS-CLIN-UC-UC-CAT-Claims-OWNER-PRD GRP-INTUITAS-CLIN-UC-UC-SCH-Claims.Curated-READER-UAT GRP-INTUITAS-FIN-PBI-PBI-WS-ExecDash-VIEWER-PRD GRP-INTUITAS-ENT-KVA-KVA-Keys-CUSTODIAN-PRD GRP-INTUITAS-CLIN-DLK-DLK-PATH-/curated/claims/READER-PRD-AUE GRP-INTUITAS-CLIN-DBX-WS-PartnerLake-READER-PRD-EXT-ACME Policies Under development. (Contact us to know more). Recommended areas to align to non-functional requirements: Data retention (duration, archival, legal hold) Key retention and rotation cycles Backup and recovery standards Incident response and escalation procedures Access review and recertification Frameworks Under development. (Contact us to know more). Recommended areas to align to industry and cyber compliance: Engineering standards (e.g., code repositories, CI/CD security, IaC policies) Security frameworks (e.g., NIST, ISO 27001, CIS Benchmarks, Zero Trust) Compliance mappings (HIPAA, GDPR, SOC2, local regulatory obligations)","title":"Standards"},{"location":"standards_and_conventions/#standards-and-conventions","text":"Return to home These standards are opinionated and designed to ensure consistency, governance, and automation across an organisation. They largely reflect an Azure Databricks environment, however can be adapted to other platforms. Organisations should adapt these standards to fit their existing internal conventions.","title":"Standards and Conventions"},{"location":"standards_and_conventions/#table-of-contents","text":"Mesh Domain Names Platform Environment VNET Resource Groups Databricks workspace Key vault Secrets Entra Group Names Azure Data Factory (standards_and_conventions.mdADF) SQL Server SQL Database Storage Lakehouse Storage Lakehouse Storage Containers Lakehouse Storage Folders Generic Blob Storage Generic Blob Files and Folders Databricks Workspace and Cluster Names Catalog Schema and Object Conventions Delta Sharing Azure Data Factory Streaming dbt Documentation and Model Metadata Sources Model and Folder Names dbt_project.yml CI/CD Repository Naming Branch Naming Branch Lifecycle Databricks Asset Bundles Security Entra Policies Frameworks","title":"Table of Contents"},{"location":"standards_and_conventions/#mesh","text":"","title":"Mesh"},{"location":"standards_and_conventions/#domain-names","text":"All lower case: {optional:organisation_}{functional area/domain}_{subdomain} e.g: intuitas_corporate","title":"Domain Names"},{"location":"standards_and_conventions/#platform","text":"","title":"Platform"},{"location":"standards_and_conventions/#environment","text":"Environment name: dev/test/prod/sandbox/poc (pat - production acceptance testing is optional as prepred)","title":"Environment"},{"location":"standards_and_conventions/#vnet","text":"Name: vn-{organisation_name}-{domain_name} = e.g: vn-intuitas-corporate","title":"VNET"},{"location":"standards_and_conventions/#resource-groups","text":"Name: rg-{organisation_name}-{domain_name} e.g: rg-intuitas-corporate","title":"Resource Groups"},{"location":"standards_and_conventions/#databricks-workspace","text":"Name: ws-{organisation_name}-{domain_name} e.g: ws-intuitas-corporate","title":"Databricks workspace"},{"location":"standards_and_conventions/#key-vault","text":"Name: kv-{organisation_name}-{domain_name} e.g: kv-intuitas-corporate","title":"Key vault"},{"location":"standards_and_conventions/#secrets","text":"Name: {secret_name}","title":"Secrets"},{"location":"standards_and_conventions/#entra-group-names","text":"Name: eg-{organisation_name}-{domain_name} = e.g: eg-intuitas-corporate","title":"Entra Group Names"},{"location":"standards_and_conventions/#azure-data-factory-adf","text":"Name: adf-{organisation_name}-{domain_name} e.g: adf-intuitas-corporate","title":"Azure Data Factory (ADF)"},{"location":"standards_and_conventions/#sql-server","text":"Name: sql-{organisation_name}-{domain_name} e.g: sql-intuitas-corporate","title":"SQL Server"},{"location":"standards_and_conventions/#sql-database","text":"Name: sqldb-{purpose}-{organisation_name}-{domain_name}-{optional:environment} e.g: sqldb-metadata-intuitas-corporate","title":"SQL Database"},{"location":"standards_and_conventions/#storage","text":"The section describes naming standards and conventions for cloud storage resources.","title":"Storage"},{"location":"standards_and_conventions/#lakehouse-storage","text":"Lakehouse storage account name: dl{organisation_name}{domain_name}","title":"Lakehouse storage"},{"location":"standards_and_conventions/#lakehouse-storage-containers","text":"Name: {environment} (dev/test/preprod/prod)","title":"Lakehouse storage containers"},{"location":"standards_and_conventions/#lakehouse-storage-folders","text":"Level 1 Name: {layer} (bronze/silver/gold) // if using medallion approach Level 2 Name: {stage_name} e.g: bronze/landing bronze/ods bronze/pds bronze/schema (for autoloader metadata) bronze/checkpoints (for autoloader metadata) silver/automatically determined by unity catalog gold/automatically determined by unity catalog","title":"Lakehouse storage folders"},{"location":"standards_and_conventions/#generic-blob-storage","text":"Generic Blob storage can be used for all non-lakehouse data; or alternatively within the lakehouse storage account in the appropriate container and folder. Resource: ADLSGen2 Generic storage account name: sa{organisation_name}{domain_name}{functional_description} Tier: Standard/Premium (depends on workload) Redundancy: Minimum ZRS or GRS for prod Minimum LRS for poc, dev, test and preprod","title":"Generic Blob storage"},{"location":"standards_and_conventions/#generic-blob-files-and-folders","text":"No standard naming conventions for files and folders.","title":"Generic Blob files and folders"},{"location":"standards_and_conventions/#databricks","text":"This section provides naming standards and conventions for Databricks.","title":"Databricks"},{"location":"standards_and_conventions/#workspace-and-cluster-names","text":"Workspace name: ws-{organisation_name}_{domain_name} Cluster name: {personal_name/shared_name} Cluster Workflow name: {dev/test} {workflow_name}","title":"Workspace and cluster names"},{"location":"standards_and_conventions/#jobs-and-pipelines","text":"","title":"Jobs and Pipelines"},{"location":"standards_and_conventions/#job-names","text":"Job names: {domain}__{layer}__{purpose}__{source}{optional: __target}{optional: __schedule}{optional: __version}__{env} e.g. clinical__bronze__ingest__fhircdr__dev","title":"Job names"},{"location":"standards_and_conventions/#for-delta-live-table-dlt-pipelines","text":"Pipeline names: {domain_name}__{layer}__pipeline__{dataset}{optional: __schedule}{optional: __version}__{env} e.g. clinical__bronze__pipeline__fhircdr__dev e.g. supplychain__gold__pipeline__inventorymart__prod Note on {layer}: If the 'business outcome' is Gold, you call it Gold, even if it produces Bronze + Silver on the way. i.e \"This is the production DLT pipeline in the supply chain domain, which builds and maintains the curated gold-layer dataset called Inventory Mart\" Include pipeline so it\u2019s distinguishable from ad hoc jobs. Dataset can be a logical grouping (e.g., patient, encounter, claims).","title":"For Delta Live Table (DLT) Pipelines"},{"location":"standards_and_conventions/#orchestration-job-names","text":"Orchestration job names: {domain}__orchestration__{workflow-name}{optional: __schedule}{optional: __version}__{env} e.g. clinical__orchestrate__fhirworkflow__daily__dev which then orchestrates: clinical__bronze__pipeline__fhircdr__prod clinical__silver__pipeline__fhirclean__prod clinical__gold__pipeline__clinicalmart__prod","title":"Orchestration job names"},{"location":"standards_and_conventions/#job-logging","text":"event log catalog: {domain}__audit__{env} event log schema: audit__event_log","title":"Job logging"},{"location":"standards_and_conventions/#optional","text":"Versioning (if needed): add v1, v2 if a job is redesigned but old one stays around. Scheduling frequency (optional): suffix with _hourly, _daily, _weekly if relevant.","title":"Optional"},{"location":"standards_and_conventions/#catalog-naming-and-conventions","text":"Refer to Data layers and stages for further context and definitions applicable to this section. Catalog name: The choice of granularity depends on domain topology, stage/zone convention and desired level of segregation for access and sharing controls (i.e. catalog or schema level) Minimum granularity (domain level): {domain_name}{__environment (dev/test/pat/prod)} (prod is implied optional) e.g: corporate__dev Optional granularity (domain-data stage level): {domain_name}{_data_stage: (bronze/silver/gold)}{__environment (dev/test/pat/prod)} e.g: corporate_bronze__dev Optional granularity (domain-data stage and zone level): {domain_name}{_data_stage: (bronze/silver/gold)}{_data_zone: (ods/pds/edw/mart)}{__environment (dev/test/pat/prod)} e.g: corporate_bronze_ods__dev Optional granularity (domain-data zone level): {domain_name}{_data_zone: (ods/pds/edw/im)}{__environment (dev/test/pat/prod)} e.g: corporate_ods__dev Optional granularity (subdomain-data stage level): {domain_name}{_descriptor (subdomain/subject/project*)}{_data_stage: (bronze/silver/gold)}{__environment (dev/test/pat/prod)} e.g: corporate_finance_bronze__dev In the examples provided - we have opted for domain level - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.ods__fhirhouse__dbo(lakeflow).encounter Note that projects are temporary constructs, and hence are not recommended for naming Catalog storage root: abfss://{environment}@dl{organisation_name}{domain_name}.dfs.core.windows.net/{domain_name}_{environment}_catalog","title":"Catalog naming and conventions"},{"location":"standards_and_conventions/#externally-mounted-lakehouse-federation-catalog-names","text":"Foreign Catalog name: {domain_name (owner)}__fc__{source_system}{optional:__other_useful_descriptors e.g:__environment} e.g: corporate__fc__sqlonpremsource e.g: corporate__fc__sqlonpremsource__prod","title":"Externally mounted (lakehouse federation) Catalog Names"},{"location":"standards_and_conventions/#catalog-metadata-tags","text":"The following metadata should be added when creating a catalog: Key: domain (owner): {domain_name} Key: environment: {environment} Key: managing_domain: {domain_name} e.g: if delegating to engineering domain","title":"Catalog Metadata tags:"},{"location":"standards_and_conventions/#schema-and-object-conventions","text":"Refer to Data layers and stages for further context and definitions applicable to this section. Refer to Physical Models for the standard relating to column naming, types, and conventions.","title":"Schema and object conventions"},{"location":"standards_and_conventions/#schema-level-external-storage-locations","text":"Recommendations: For managed tables (default): do nothing. Let dbt create schemas without additional configuration. Databricks will manage storage and metadata.Objects will then be stored in the catalog storage root. e.g: abfss://dev@dlintutiasengineering.dfs.core.windows.net/engineering__dev_catalog/__unitystorage/catalogs/catalog-guid/tables/object-guid For granular control over schema-level storage locations: Pre-create schemas with LOCATION mapped to external paths or configure the catalog-level location. Ensure dbt's dbt_project.yml and environment variables align with storage locations.","title":"Schema level external storage locations"},{"location":"standards_and_conventions/#metadata-schemas-and-objects","text":"Refer to Data layers and stages for further context and definitions applicable to this section. Contains metadata that supports engineering and governance. This will vary depending on engineering and governance toolsets Engineering - ingestion framework : Schema naming convention: meta {optional: __function} Naming convention: {function/descriptor} e.g: corporate__dev.meta.ingestion_control e.g: corporate__dev.meta__ingestion.ingestion_control","title":"Metadata Schemas and Objects"},{"location":"standards_and_conventions/#bronze-raw-data-according-to-systems","text":"The Bronze layer stores raw, immutable data as it is ingested from source systems. See Data layers and stages for definitions and context. All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. bronze__ In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.ods__fhirhouse__dbo__lakeflow.encounter Persistent Landing : Persistent Landing uses Unity Catalog Volumes for storing raw files and unstructured data as they arrive from source systems. Volume naming convention: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] [domain][__env] : Domain and environment identifier (e.g., corporate_dev , engineering_prod ) [layer] : Data layer identifier (e.g., landing ) [__source_system] : Source system identifier (e.g., workdayapi , saphr ) [__source_schema] : Optional source schema or subsystem identifier (channel) : Ingestion channel/method (e.g., adf , fivetran , api ) [source_object/volume_name] : Descriptive volume name or source object identifier e.g: corporate__dev.landing__workdayapi.schedule_volume e.g: engineering__prod.landing__fhirapi__patients.raw_data Operational Data Store (ODS) : The objective of raw layer conventions is to provide clarity over which zone and stage it belongs, what the data relates to, where it was sourced from, and via what channel it arrived (as there may be nuances in data depending on its channel). ODS can be replicated from source systems, or prepared for use from semi/unstructured data via hard-transformation and hence will have these associated conventions: Database replicated ODS (structured sources like SQL Server): - Format: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., ods ) - [__source_system] : Source system identifier - [__source_schema] : Source schema (if applicable, e.g., dbo , reporting ) - (channel) : Optional ingestion channel (e.g., adf , fivetran , lakeflow ) - [source_object/volume_name] : Table name as per source - e.g: clinical__dev.ods__patientflowmanager01__reporting.patients - e.g: clinical__dev.ods__fhirhouse__dbo(adf).encounter Prepped semi/unstructured ODS data: - Format: [domain][__env].[layer][__source_system][__source_descriptor](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., ods ) - [__source_system] : Source system identifier - [__source_descriptor] : Source descriptor or subsystem identifier - (channel) : Optional ingestion channel (e.g., kafka , databricks , api ) - [source_object/volume_name] : Named as per source or unique assigned name (e.g., topic/folder name) - e.g: clinical__dev.ods__ambosim__confluent(kafka).encounter - e.g: corporate__dev.ods__workdayapi__employees.raw_data Persistent Data Store (PDS) : PDS conventions will mirror ODS conventions: Database replicated PDS (structured sources like SQL Server): - Format: [domain][__env].[layer][__source_system][__source_schema](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., pds ) - [__source_system] : Source system identifier - [__source_schema] : Source schema (if applicable, e.g., dbo , reporting ) - (channel) : Optional ingestion channel (e.g., adf , fivetran , lakeflow ) - [source_object/volume_name] : Table name as per source - e.g: clinical__dev.pds__patientflowmanager01__reporting.patients - e.g: clinical__dev.pds__fhirhouse__dbo(adf).encounter Prepped semi/unstructured PDS data: - Format: [domain][__env].[layer][__source_system][__source_descriptor](channel).[source_object/volume_name] - [domain][__env] : Domain and environment (e.g., clinical__dev ) - [layer] : Data layer (e.g., pds ) - [__source_system] : Source system identifier - [__source_descriptor] : Source descriptor or subsystem identifier - (channel) : Optional ingestion channel (e.g., kafka , databricks , api ) - [source_object/volume_name] : Named as per source or unique assigned name (e.g., topic/folder name) - e.g: clinical__dev.pds__ambosim__confluent(kafka).encounter - e.g: corporate__dev.pds__workdayapi__employees.raw_data","title":"Bronze (Raw data according to systems)"},{"location":"standards_and_conventions/#silveredw-data-according-to-business-entities","text":"The Silver layer focuses on transforming raw data into cleaned, enriched, and validated datasets that are the building blocks for downstream consumption and analysis. Refer to Data layers and stages for further context and definitions applicable to this section. These marts are objects that are aligned to business entities and broad requirements, hence they must contain source-specific objects at the lowest grain. There may be further enrichment and joins applied across sources. In the examples provided - we have opted for domain level catalogs - with schema separation for the lower levels of grain via prefixes. i.e engineering__dev.edw.dim_customer All schemas may be optionally prefixed with data stage if not already decomposed at domain-level i.e. silver__ All entity names which align to facts should be named in plural. All entity names which align to dims should be named in singular. (Silver)/EDW Staging Objects : Staging models serve as intermediary models that transform source data into the target silver model. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage silver marts (base marts in the EDW layer) and reference data (in the REF layer). Naming Convention: [domain][__env] . [layer] . [object_type][_entity/descriptor](__source)(__channel)(__transformation) Where: - [domain][__env] - Domain and environment (e.g., corporate__prod , corporate__dev ) - [layer] - Data layer (e.g., ref , edw ) - [object_type] - Object type prefix (e.g., stg_ , ref_ , dim_ , fact_ , keys_ ) - [_entity/descriptor] - Entity or object name - (__source) - Optional source system identifier (e.g., __sap , __workday , __finance_system ) - (__channel) - Optional source channel (e.g., __adf , __api ) - (__transformation) - Optional transformation step (e.g., __01_typed , __02_cleaned ) Reference Data Staging: Source-specific staging (with transformations): e.g: corporate__prod.ref.stg_facility__sap__01_filtered e.g: corporate__prod.ref.stg_facility__sap__02_typed e.g: corporate__prod.ref.stg_account_code__finance_system__adf__01_renamed e.g: corporate__prod.ref.stg_location__workday__01_typed Final reference data (cleaned and conformed): e.g: corporate__prod.ref.ref_facility e.g: corporate__prod.ref.ref_location e.g: corporate__prod.ref.ref_account_code e.g: corporate__dev.ref.ref_country_codes Base Mart (EDW) Staging: Source-specific staging (with transformations): e.g: corporate__prod.edw.stg_employee__sap__01_typed e.g: corporate__prod.edw.stg_employee__workday__01_typed e.g: corporate__prod.edw.stg_employee__workday__02_cleaned e.g: corporate__prod.edw.stg_customer__crm__adf__01_renamed e.g: corporate__prod.edw.stg_customer__crm__adf__02_cleaned e.g: corporate__prod.edw.stg_payment__new_finance_system__adf__01_typed e.g: corporate__prod.edw.stg_payment__old_finance_system__adf__01_typed e.g: corporate__prod.edw.stg_account__finance_system__api__01_renamed Non-source specific staging (after combining/deduplicating): e.g: corporate__prod.edw.stg_employee__01_deduped e.g: corporate__prod.edw.stg_employee__02_validated e.g: corporate__prod.edw.stg_payment__01_unified Base Mart Keysets: Conforming/deduplicating keysets across multiple sources: e.g: corporate__prod.edw.keys_employee (conforms sap and workday) e.g: corporate__prod.edw.keys_customer (conforms multiple crm sources) e.g: corporate__prod.edw.keys_account (conforms finance systems) e.g: health__prod.edw.keys_patient (conforms clinical systems) Base Marts (Final Dimensional Models): Source-specific dimensions (preserving source identity): e.g: corporate__prod.edw.dim_employee__sap e.g: corporate__prod.edw.dim_employee__workday e.g: corporate__prod.edw.dim_account__old_finance_system e.g: corporate__prod.edw.dim_account__new_finance_system Conformed dimensions (unified, non-source specific): e.g: corporate__prod.edw.dim_employee (type 1 SCD implied) e.g: corporate__prod.edw.dim_employee__type2 (with history) e.g: corporate__prod.edw.dim_customer e.g: corporate__prod.edw.dim_account (unified across finance systems) Fact tables: e.g: corporate__prod.edw.fact_payment e.g: corporate__prod.edw.fact_transaction e.g: health__prod.edw.fact_encounter Common Transformation Suffixes: __01_renamed or __01_typed __02_cleaned __03_deduped __04_filtered __05_split __06_validated __07_desensitised Additional Notes: For further context on reference data modeling, refer to Reference Data Standards and Conventions . Raw Vault : Optional warehousing construct. Schema naming convention: edw_rv Object naming convention: {vault object named as per data vault standards} e.g: corporate__dev.edw_rv.hs_payments__finance_system__adf Business Vault : Optional warehousing construct. Schema naming convention: edw_bv Object naming convention: {vault object named as per data vault standards} e.g: corporate__dev.edw_bv.hs_late_payments__finance_system__adf","title":"Silver/EDW (Data according to business entities)"},{"location":"standards_and_conventions/#gold-data-according-to-requirements","text":"The Gold layer focuses on requirement-aligned products (datasets, aggregations, and reporting structures). Products are predominantly source agnostic, however optionality exists when source-specific views are needed. Refer to Data layers and stages for further context and definitions applicable to this section. Naming Convention: [domain][__env] . mart . [object_type][product_name/descriptor](__source)(__transformation) Where: - [domain][__env] - Domain and environment (e.g., corporate__prod , health__dev ) - mart - Gold layer schema (product marts) - [object_type] - Object type prefix (e.g., stg_ , fact_ , dim_ , obt_ ) - [product_name/descriptor] - Product or business-aligned name - (__source) - Optional source system identifier (only when source-specific view is needed) - (__transformation) - Optional transformation step (for staging only) Naming Guidelines: - All entity names which align to facts should be named in plural - All entity names which align to dimensions should be named in singular - Product marts are business/requirement-aligned, not technical constructs (Gold)/Product Mart Staging Models : Staging models serve as intermediary models that transform base marts (EDW) into requirement-aligned product marts. According to dbt best practices, there is a distinction between Staging and Intermediate models. Under this blueprint the use of Intermediate models is optional. Reference These models exist to stage gold product marts with business-specific transformations: Common Transformations: - Pivoting - Aggregation - Joining across multiple base marts - Business rule application - Desensitization - Complex calculations Examples: e.g: corporate__prod.mart.stg_downtime_by_region__01_pivoted e.g: corporate__prod.mart.stg_downtime_by_region__02_desensitised e.g: corporate__prod.mart.stg_late_payments__01_aggregated e.g: corporate__prod.mart.stg_late_payments__02_joined e.g: corporate__prod.mart.stg_fte_calculations__01_pivoted e.g: corporate__prod.mart.stg_fte_calculations__02_aggregated e.g: health__prod.mart.stg_patient_outcomes__01_joined e.g: health__prod.mart.stg_patient_outcomes__02_desensitised (Gold)/Product Marts (Final Products) : Final business-aligned data products ready for consumption by analytics, reporting, and BI tools. Examples: Fact tables (aggregated/business-focused): e.g: corporate__prod.mart.fact_downtime_by_region e.g: corporate__prod.mart.fact_late_payments e.g: corporate__prod.mart.fact_regional_account_payments e.g: health__prod.mart.fact_patient_outcomes Dimension tables (product-specific): e.g: corporate__prod.mart.dim_region e.g: corporate__prod.mart.dim_payment_category One Big Tables (OBT) - Denormalized aggregates: e.g: corporate__prod.mart.obt_fte_calculations e.g: corporate__prod.mart.obt_financial_summary e.g: corporate__prod.mart.obt_executive_dashboard Source-specific products (when needed): e.g: corporate__prod.mart.fact_account_payments__old_finance_system e.g: corporate__prod.mart.fact_account_payments__new_finance_system e.g: corporate__prod.mart.obt_employee_metrics__sap e.g: corporate__prod.mart.obt_employee_metrics__workday","title":"Gold (Data according to requirements)"},{"location":"standards_and_conventions/#delta-sharing","text":"Share names: {domain_name} {optional:subdomain_name} {optional:purpose} {schema_name or description} {object_name or description}__{share_purpose and or target_audience} e.g: intuitas_corporate__finance__reporting__account_payments__payments","title":"Delta Sharing"},{"location":"standards_and_conventions/#azure-data-factory","text":"Linked service names: ls_{database_name}(if not in database_name:{ organisation_name} {domain_name}) e.g: ls_financedb_intuitas_corporate Dataset names: ds_{database_name}_{object_name} Pipeline names: pl_{description: e.g copy_{source_name} to {destination_name}} Trigger names: tr_{pipeline_name}_{optional:start_time / frequency}","title":"Azure Data Factory"},{"location":"standards_and_conventions/#streaming","text":"Cluster name: {domain_name}__cluster__{optional:environment} Topic names: {domain_name}__{object/entity?}__{optional:source_system}___{optional:source_channel}__{optional:environment} Consumer group names: {domain_name}__{unique_group_name}__{optional:environment}","title":"Streaming"},{"location":"standards_and_conventions/#dbt","text":"The following standards and conventions relate to dbt projects.","title":"dbt"},{"location":"standards_and_conventions/#documentation-and-model-metadata","text":"Within each respective model folder (as needed) md: _{path to model folder using _ separators}__docs.md e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__docs.md model yml: _{path to model folder using _ separators}__models.yml e.g: models/silver/ambo_sim__kafka__local/_silver__ambo_sim__kafka__local__models.yml","title":"Documentation and model metadata"},{"location":"standards_and_conventions/#sources","text":"Folder: models/sources/{bronze/silver/gold} yml: {schema}__sources.yml (one for each source schema) e.g: ods__ambo_sim__kafka__sources.yml","title":"Sources"},{"location":"standards_and_conventions/#model-and-folder-names","text":"dbt model names are verbose (inclusive of zone and domain) to ensure global uniqueness and better traceability to folders. Actual object names should be aliased to match object naming standards.","title":"Model and Folder Names"},{"location":"standards_and_conventions/#bronze","text":"Bronze objects are likely to be referenced in sources/bronze or as seeds Folder: models/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: sources/bronze/{optional: domain name}{optional: __subdomain name(s)}/ Folder: seeds/{optional: domain name}{optional: __subdomain name(s)}/","title":"Bronze"},{"location":"standards_and_conventions/#silver","text":"Staging Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/stg/{source_system}__{source_channel}/ Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver} __stg{__entity /_object_description} __{ordinal}_{transformation description} __{source_system} __{source_channel} *e.g:* - *silver\\new_finance_system__adf\\stg\\corporate__silver__stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - or *silver\\new_finance_system__adf\\stg\\stg__accounts__01_renamed_and_typed__new_finance_system__adf.sql* - materialises to: *corporate__dev.edw.stg_account__new_finance_system__adf__01_renamed* Staging Non-source-specific (entity centric): Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} stg{__optional:dim_/fact_}{__entity /_object_description} __{ordinal}_{transformation description} e.g: corporate__dev.edw.stg_account__01_deduped e.g: corporate__dev.edw.stg_account__02_validated *e.g:* - *silver\\mart\\accounts\\stg\\corporate__silver__stg__accounts__01_deduped.sql* - or *silver\\mart\\accounts\\stg\\stg__accounts__01_deduped.sql* - materialises to: *e.g: corporate__dev.edw.stg_account__01_deduped* Mart Source-specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim_/fact_}{__entity /_object_description}__{source_system}__{source_channel} Mart Non-source specific: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} mart{__optional:dim_/fact_}{__unified entity /_object_description} Reference Data: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/ref/{entity} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__silver__} ref{__optional:dim_/fact_} {__reference data set name} {optional:__{source_system}__{source_channel}} Raw Vault: Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/rv Models: edw_rv__{vault object named as per data vault standards} Schema naming convention:** Folder: models/silver/{optional: domain name}{optional: __subdomain name(s)}/edw/bv Models: edw_bv__{vault object named as per data vault standards}","title":"Silver"},{"location":"standards_and_conventions/#gold","text":"Staging:** Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description}/stg Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__stg{__entity / product description} __{ordinal}_{transformation description} {optional: __{source_system} __{source_channel}} Dimensions: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__dim_{__entity / product description} (optional: __{source_system} __{source_channel}) Facts: Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__fact_{__entity / product description} (optional: __{source_system} __{source_channel}) Denormalized (One Big Table): Folder: models/gold/{optional: domain name}{optional: __subdomain name(s)}/mart/{entity / product description} Models: {optional: domain name} {optional: __subdomain name(s)} {optional:__gold__} mart__{entity / product description} {optional: __{source_system} __{source_channel}}","title":"Gold"},{"location":"standards_and_conventions/#example-dbt-model-structure","text":"The model structure below reflects a single catalog for domain+environment and schema separation for layers and stages: {{domain/enterprise} _project_name} \u251c\u2500\u2500 README.md \u251c\u2500\u2500 analyses \u251c\u2500\u2500 seeds \u2502 \u2514\u2500\u2500 ref_entity_data_file.csv \u251c\u2500\u2500 dbt_project.yml \u251c\u2500\u2500 macros \u2502 \u2514\u2500\u2500 custom_macro.sql \u2502 \u251c\u2500\u2500 utilities \u2502 \u2514\u2500\u2500 all_dates.sql \u251c\u2500\u2500 models/bronze \u2502 /{optional: domain and subdomains} \u2502 \u2514\u2500\u2500 _bronze.md \u251c\u2500\u2500 models/silver/{optional: domain and subdomains} \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _silver.md \u2502 \u251c\u2500\u2500 mart (entity centric objects) \u2502 | \u2514\u2500\u2500 account \u2502 | | \u2514\u2500\u2500 mart__dim_account.sql \u2502 | | \u2514\u2500\u2500 stg \u2502 | | \u2514\u2500\u2500 stg__dim_account__01_dedupe.sql \u2502 | | \u2514\u2500\u2500 stg__dim_account__02_filter.sql \u2502 | \u2514\u2500\u2500 date \u2502 | \u2514\u2500\u2500 mart__dim_date.sql \u2502 \u2514\u2500\u2500 ref \u2502 \u251c\u2500\u2500 _reference_data__models.yml \u2502 \u251c\u2500\u2500 _reference_data__sources.yml \u2502 \u2514\u2500\u2500 ref_{entity}.sql \u2502 \u251c\u2500\u2500 stg (source centric staging objects) \u2502 | \u2514\u2500\u2500 source_system_1 \u2502 | \u251c\u2500\u2500 _source_system_1__docs.md \u2502 | \u251c\u2500\u2500 _source_system_1__models.yml \u2502 | \u251c\u2500\u2500 stg__object__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__(new object)__source_system_1.sql \u2502 | \u251c\u2500\u2500 stg__object_desensitised__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg \u2502 | \u251c\u2500\u2500 stg__object__01_step__source_system_1.sql \u2502 | \u2514\u2500\u2500 stg__object__02_step__source_system_1.sql \u2502 \u251c\u2500\u2500 sources \u2502 \u2514\u2500\u2500 {optional: domain} \u2502 \u2514\u2500\u2500 {optional: bronze/silver/gold} \u2502 \u2514\u2500\u2500 _source_system_1__sources.yml \u251c\u2500\u2500 models/gold \u2502 /{optional: domain and subdomains} \u2502 \u251c\u2500\u2500 _gold.md \u2502 \u2514\u2500\u2500 domain_name e.g: finance \u2502 \u2514\u2500\u2500 mart \u2502 \u251c\u2500\u2500 _finance__models.yml \u2502 \u251c\u2500\u2500 orders.sql \u2502 \u2514\u2500\u2500 payments.sql \u2502 \u2514\u2500\u2500 stg \u2502 \u2514\u2500\u2500 stg_payments_pivoted_to_orders.sql \u251c\u2500\u2500 packages.yml \u251c\u2500\u2500 snapshots \u2514\u2500\u2500 tests \u2514\u2500\u2500 assert_positive_value_for_total_amount.sql","title":"Example dbt model structure:"},{"location":"standards_and_conventions/#dbt_projectyml","text":"The yml structure below reflects a single catalog for domain+environment and schema separation for layers and stages: models: health_lakehouse__engineering__dbt: +persist_docs: #enables injection of metadata into unity catalog relation: true columns: true bronze: ods: +schema: ods pds: +schema: pds silver: ref: +description: \"Reference data\" +schema: ref +materialized: table edw: +description: \"Enterprise data warehouse base marts\" +schema: edw +materialized: table staging: +materialized: view gold: +materialized: view # default for speed mart: +description: \"Product marts\" +schema: mart +materialized: table","title":"dbt_project.yml"},{"location":"standards_and_conventions/#cicd","text":"The following standards and conventions relate to Continuous Improvement and Continuous Delivery constructs.","title":"CI/CD"},{"location":"standards_and_conventions/#repository-naming","text":"All lowercase with hyphens as separators Format: {org}-{domain}-{purpose}-{optional:descriptor} Examples: - intuitas-corporate-dbt - intuitas-corporate-ingestion-framework - intuitas-corporate-cicd-templates - intuitas-corporate-infrastructure","title":"Repository naming"},{"location":"standards_and_conventions/#branch-naming","text":"All lowercase with hyphens as separators Naming convention: {type}-{optional:ticket-id}-{description} Types: - feature: New functionality - bugfix: Bug fixes - hotfix: Critical fixes for production - release: Release branches - docs: Documentation updates only - refactor: Code improvements with no functional changes - test: Test-related changes Examples: - feature-eng123-add-new-data-source - bugfix-eng456-fix-null-values - hotfix-prod-outage-fix - release-v2.1.0 - docs-update-readme - refactor-optimize-transforms - test-add-integration-tests","title":"Branch naming"},{"location":"standards_and_conventions/#branch-lifecycle","text":"","title":"Branch lifecycle"},{"location":"standards_and_conventions/#simple-branch-lifecycle","text":"main/master: Primary branch branch: Short-lived branches development branch, merged or rebased to main/master","title":"Simple branch lifecycle:"},{"location":"standards_and_conventions/#comprehensive-team-branch-lifecycle","text":"Main/master: Primary branch Development: Active development branch Feature/bugfix: Short-lived branches merged to development Release: Created from development, merged to main/master Hotfix: Created from main/master for urgent fixes","title":"Comprehensive team branch lifecycle:"},{"location":"standards_and_conventions/#databricks-asset-bundles","text":"Databricks asset bundles are encouraged for all Databricks projects. project/bundle name: {domain_name}__databricks (for general databricks projects) {domain_name}__dbt (for dbt databricks bundles) Bundle tags: Key: environment: {environment} Key: manager: {team_name} and or {email_address} Key: managing_domain: {domain_name}` e.g: if delegating to engineering domain Key: owner: {owner_name} Key: owning_domain: {domain_name} Key: dab: {bundle_name} Key: project: {project_name} e.g: yml tags: environment: ${bundle.target} project: health-lakehouse dab: health_lakehouse__engineering__databricks owning_domain: intuitas_engineering owner: engineering-admin@intuitas.com manager: engineering-engineer@intuitas.com managing_domain: intuitas_engineering Resources: Folder level 1: {meaningful sub-project name} Folder level 2: notebooks workflows Databricks.yml For both dev and prod: root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} Example databricks.yml # This is a Databricks asset bundle definition for health_lakehouse__engineering. # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation. bundle: name: health_lakehouse__engineering__databricks variables: default_cluster_id: value: \"-----\" include: - resources/*.yml - resources/**/*.yml - resources/**/**/*.yml targets: dev: mode: development default: true workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} prod: mode: production workspace: host: https://------.15.azuredatabricks.net root_path: /Workspace/Users/engineering-engineer@intuitas.com/.bundle/${bundle.name}/${bundle.target} permissions: - user_name: engineering-engineer@intuitas.com level: CAN_MANAGE run_as: user_name: engineering-engineer@intuitas.com","title":"Databricks Asset Bundles"},{"location":"standards_and_conventions/#security","text":"Security standards and conventioned provided here provide a starter set, however existing organisational and applicable industry standards should take precedence. Consult with your cybersecurity advisor.","title":"Security"},{"location":"standards_and_conventions/#entra","text":"Under development. (Contact us to know more). Most organisations will already have an established set of groups and conventions. Where there are gaps, the following can still be considered. Recommended areas to align to organisational governance and cyber requirements: Naming conventions for admin, service, and user groups Role-based access alignment (least privilege, separation of duties) Alignment to domains - Cross-domain vs. domain-specific group patterns Entra Group Names : Pattern: grp-<org>-<domain>-<plat>-<scope>-<role>-<env>[-<region>][-ext-<partner>][-idx] Lowercase, hyphen-separated; no spaces. Keep to \u2264 120 chars total. No PII in names. Use Security groups (not M365) for RBAC; enable PIM where appropriate e.g. Admins. role : owner \u2014 full control of the named scope admin \u2014 administrative (non-ownership) rights contrib \u2014 create/modify within scope editor \u2014 modify data/artifacts, not permissions reader \u2014 read-only steward \u2014 governance/metadata rights custodian \u2014 key/secret/storage control operator \u2014 run/ops rights (pipelines, jobs) viewer \u2014 read dashboards/reports plat: dbx (Databricks), uc (Unity Catalog), pbi (Power BI), adf (Data Factory), dlk (Data Lake), sql (Azure SQL), kva (Key Vault), syn (Synapse) scope (or object): Databricks Workspace: ws- Unity Catalog: uc-meta (metastore), uc-cat- , uc-sch- . , uc-obj- . . Power BI: pbi-ws- Data Lake: dlk-path-/datalake/ / Examples : GRP-INTUITAS-CLIN-DBX-WS-Analytics-ADMIN-PRD GRP-INTUITAS-CLIN-UC-UC-CAT-Claims-OWNER-PRD GRP-INTUITAS-CLIN-UC-UC-SCH-Claims.Curated-READER-UAT GRP-INTUITAS-FIN-PBI-PBI-WS-ExecDash-VIEWER-PRD GRP-INTUITAS-ENT-KVA-KVA-Keys-CUSTODIAN-PRD GRP-INTUITAS-CLIN-DLK-DLK-PATH-/curated/claims/READER-PRD-AUE GRP-INTUITAS-CLIN-DBX-WS-PartnerLake-READER-PRD-EXT-ACME","title":"Entra"},{"location":"standards_and_conventions/#policies","text":"Under development. (Contact us to know more). Recommended areas to align to non-functional requirements: Data retention (duration, archival, legal hold) Key retention and rotation cycles Backup and recovery standards Incident response and escalation procedures Access review and recertification","title":"Policies"},{"location":"standards_and_conventions/#frameworks","text":"Under development. (Contact us to know more). Recommended areas to align to industry and cyber compliance: Engineering standards (e.g., code repositories, CI/CD security, IaC policies) Security frameworks (e.g., NIST, ISO 27001, CIS Benchmarks, Zero Trust) Compliance mappings (HIPAA, GDPR, SOC2, local regulatory obligations)","title":"Frameworks"}]}